{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99413c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:19.120580Z",
     "iopub.status.busy": "2025-03-24T15:56:19.120247Z",
     "iopub.status.idle": "2025-03-24T15:56:22.226817Z",
     "shell.execute_reply": "2025-03-24T15:56:22.226068Z"
    },
    "id": "5db35d05",
    "papermill": {
     "duration": 3.113563,
     "end_time": "2025-03-24T15:56:22.228345",
     "exception": false,
     "start_time": "2025-03-24T15:56:19.114782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3920b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:22.238198Z",
     "iopub.status.busy": "2025-03-24T15:56:22.237865Z",
     "iopub.status.idle": "2025-03-24T15:56:22.244255Z",
     "shell.execute_reply": "2025-03-24T15:56:22.243685Z"
    },
    "id": "cMV0vTRfAN0I",
    "papermill": {
     "duration": 0.012477,
     "end_time": "2025-03-24T15:56:22.245584",
     "exception": false,
     "start_time": "2025-03-24T15:56:22.233107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811deaf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:22.253875Z",
     "iopub.status.busy": "2025-03-24T15:56:22.253664Z",
     "iopub.status.idle": "2025-03-24T15:56:23.079126Z",
     "shell.execute_reply": "2025-03-24T15:56:23.078390Z"
    },
    "id": "c4bc7518",
    "papermill": {
     "duration": 0.831308,
     "end_time": "2025-03-24T15:56:23.080744",
     "exception": false,
     "start_time": "2025-03-24T15:56:22.249436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv('/kaggle/input/machine-translation-ioai/train.csv').values\n",
    "test_dataset = pd.read_csv('/kaggle/input/machine-translation-ioai/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e752ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.089851Z",
     "iopub.status.busy": "2025-03-24T15:56:23.089599Z",
     "iopub.status.idle": "2025-03-24T15:56:23.096298Z",
     "shell.execute_reply": "2025-03-24T15:56:23.095505Z"
    },
    "id": "76a5c735",
    "outputId": "2083b5f5-3fba-4ee3-ecc4-102ff4aaeea9",
    "papermill": {
     "duration": 0.012599,
     "end_time": "2025-03-24T15:56:23.097643",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.085044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = max(map(lambda x: len(x[0]), train_dataset)) + 1\n",
    "\n",
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b47216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.106631Z",
     "iopub.status.busy": "2025-03-24T15:56:23.106353Z",
     "iopub.status.idle": "2025-03-24T15:56:23.111254Z",
     "shell.execute_reply": "2025-03-24T15:56:23.110477Z"
    },
    "id": "556ae6af",
    "papermill": {
     "duration": 0.010826,
     "end_time": "2025-03-24T15:56:23.112562",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.101736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\n",
    "            'SOS': 0,\n",
    "            'EOS': 1\n",
    "        }\n",
    "        self.index2word = {\n",
    "            0: 'SOS',\n",
    "            1: 'EOS'\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def n_words(self) -> int:\n",
    "        return len(self.index2word)\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in list(sentence):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e90270ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.121165Z",
     "iopub.status.busy": "2025-03-24T15:56:23.120967Z",
     "iopub.status.idle": "2025-03-24T15:56:23.131142Z",
     "shell.execute_reply": "2025-03-24T15:56:23.130508Z"
    },
    "id": "74410a56",
    "outputId": "a84b06f1-37ea-4152-da4d-6346b5d36ee3",
    "papermill": {
     "duration": 0.015758,
     "end_time": "2025-03-24T15:56:23.132323",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.116565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human 82\n",
      "iso 13\n"
     ]
    }
   ],
   "source": [
    "input_lang = Lang('human')\n",
    "output_lang = Lang('iso')\n",
    "\n",
    "for pair in train_dataset:\n",
    "    input_lang.add_sentence(pair[0])\n",
    "    output_lang.add_sentence(pair[1])\n",
    "\n",
    "print(input_lang.name, input_lang.n_words)\n",
    "print(output_lang.name, output_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b426baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.140756Z",
     "iopub.status.busy": "2025-03-24T15:56:23.140546Z",
     "iopub.status.idle": "2025-03-24T15:56:23.207538Z",
     "shell.execute_reply": "2025-03-24T15:56:23.206810Z"
    },
    "id": "b5e4f21a",
    "papermill": {
     "duration": 0.072545,
     "end_time": "2025-03-24T15:56:23.208833",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.136288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f91b01f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.217524Z",
     "iopub.status.busy": "2025-03-24T15:56:23.217261Z",
     "iopub.status.idle": "2025-03-24T15:56:23.221996Z",
     "shell.execute_reply": "2025-03-24T15:56:23.221349Z"
    },
    "id": "8fe0f376",
    "papermill": {
     "duration": 0.010312,
     "end_time": "2025-03-24T15:56:23.223181",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.212869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266ccc5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.231729Z",
     "iopub.status.busy": "2025-03-24T15:56:23.231521Z",
     "iopub.status.idle": "2025-03-24T15:56:23.237781Z",
     "shell.execute_reply": "2025-03-24T15:56:23.237118Z"
    },
    "id": "42832f9b",
    "papermill": {
     "duration": 0.01172,
     "end_time": "2025-03-24T15:56:23.238874",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.227154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.map_encoder = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map_combined = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        # self.bn_combined = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(p=.1)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outs):\n",
    "        embed = self.embedding(x).view(-1, 1, self.hidden_size)\n",
    "        embed = self.dropout(embed)\n",
    "\n",
    "        attn_w = self.map_encoder(encoder_outs)\n",
    "        attn_w = F.softmax(hidden.view(1,self.hidden_size) @ attn_w.view(-1, self.hidden_size).T,dim=-1)\n",
    "\n",
    "        attn_val = attn_w @ encoder_outs \n",
    "        \n",
    "        combined = torch.cat([embed, attn_val.unsqueeze(0)], dim=-1)\n",
    "        combined = self.map_combined(combined)\n",
    "        # combined = self.bn_combined(combined)\n",
    "        output = self.relu(combined)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86401d50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.247363Z",
     "iopub.status.busy": "2025-03-24T15:56:23.247168Z",
     "iopub.status.idle": "2025-03-24T15:56:23.251258Z",
     "shell.execute_reply": "2025-03-24T15:56:23.250635Z"
    },
    "id": "d3308116",
    "papermill": {
     "duration": 0.009579,
     "end_time": "2025-03-24T15:56:23.252470",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.242891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentence2idx(lang, sentence):\n",
    "    return [lang.word2index[word] for word in list(sentence)]\n",
    "\n",
    "\n",
    "def sentence2tensor(lang, sentence):\n",
    "    indexes = sentence2idx(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def pair2tensor(x):\n",
    "    input_tensor = sentence2tensor(input_lang, x[0])\n",
    "    target_tensor = sentence2tensor(output_lang, x[1])\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a9a637c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.260906Z",
     "iopub.status.busy": "2025-03-24T15:56:23.260687Z",
     "iopub.status.idle": "2025-03-24T15:56:23.266287Z",
     "shell.execute_reply": "2025-03-24T15:56:23.265539Z"
    },
    "id": "2ff20461",
    "papermill": {
     "duration": 0.011297,
     "end_time": "2025-03-24T15:56:23.267593",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.256296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_single(\n",
    "        input_tensor, target_tensor,\n",
    "        encoder, decoder,\n",
    "        encoder_optimizer, decoder_optimizer,\n",
    "        criterion\n",
    "):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "    # enocder_outs = torch.zeros((input_tensor.shape[0], MAX_LENGTH, encoder.hidden_size), device=device)\n",
    "    enocder_outs = torch.zeros((MAX_LENGTH, encoder.hidden_size), device=device)\n",
    "\n",
    "    for ei, elem in enumerate(input_tensor):\n",
    "        encoder_output, encoder_hidden = encoder(elem, encoder_hidden)\n",
    "        # enocder_outs[:, ei] = encoder_output[:,0]\n",
    "        enocder_outs[ei] = encoder_output[0,0]\n",
    "\n",
    "    # decoder_input = torch.tensor([[SOS_token] for i in range(input_tensor.shape[0])], device=device)\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    use_teacher_forcing = False\n",
    "    # use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for elem in target_tensor:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, enocder_outs)\n",
    "            loss += criterion(decoder_output, elem)\n",
    "            decoder_input = elem\n",
    "    else:\n",
    "        for elem in target_tensor:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, enocder_outs)\n",
    "            _, topi = decoder_output.data.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            loss += criterion(decoder_output, elem)\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / len(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9352d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.276331Z",
     "iopub.status.busy": "2025-03-24T15:56:23.276115Z",
     "iopub.status.idle": "2025-03-24T15:56:23.282305Z",
     "shell.execute_reply": "2025-03-24T15:56:23.281557Z"
    },
    "id": "99270fd7",
    "papermill": {
     "duration": 0.011757,
     "end_time": "2025-03-24T15:56:23.283454",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.271697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, n_epochs=5, print_every=100):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    encoder_optimizer = AdamW(encoder.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "    decoder_optimizer = AdamW(decoder.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "    encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, gamma=0.65, step_size=2)\n",
    "    decoder_scheduler = torch.optim.lr_scheduler.StepLR(decoder_optimizer, gamma=0.65, step_size=2)\n",
    "\n",
    "    training_pairs = [\n",
    "        pair2tensor(x) for x in train_dataset\n",
    "    ]\n",
    "\n",
    "    # dataloader = DataLoader(training_pairs, shuffle=True, batch_size=32)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    best_loss = 10e9\n",
    "    best_encoder = -1\n",
    "    best_decoder = -1\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print_loss_total = 0\n",
    "\n",
    "        print(f'Epoch [{epoch + 1:02d}/{n_epochs:02d}]')\n",
    "\n",
    "        # for i, batch in enumerate(dataloader):\n",
    "        for i, batch in enumerate(training_pairs):\n",
    "\n",
    "            input_tensor = batch[0]\n",
    "            target_tensor = batch[1]\n",
    "\n",
    "            # print(input_tensor.shape)\n",
    "            loss = train_single(\n",
    "                input_tensor, target_tensor,\n",
    "                encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer,\n",
    "                criterion\n",
    "            )\n",
    "            print_loss_total += loss\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "\n",
    "                if print_loss_avg < best_loss:\n",
    "                    best_loss = print_loss_avg\n",
    "                    best_encoder = encoder\n",
    "                    best_decoder = decoder\n",
    "                \n",
    "                print_loss_total = 0\n",
    "                print(f'Training ({i / len(training_pairs) * 100:.1f}%) loss: {print_loss_avg:.4f}')\n",
    "\n",
    "    encoder_scheduler.step()\n",
    "    decoder_scheduler.step()\n",
    "\n",
    "    return best_encoder, best_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d42d0e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T15:56:23.291964Z",
     "iopub.status.busy": "2025-03-24T15:56:23.291740Z",
     "iopub.status.idle": "2025-03-24T16:09:55.278464Z",
     "shell.execute_reply": "2025-03-24T16:09:55.277524Z"
    },
    "id": "0c3f5df8",
    "outputId": "a2a19cc9-bea8-453d-abc9-38c57e7dc5a7",
    "papermill": {
     "duration": 811.992736,
     "end_time": "2025-03-24T16:09:55.280121",
     "exception": false,
     "start_time": "2025-03-24T15:56:23.287385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20]\n",
      "Training (9.0%) loss: 1.7694\n",
      "Training (18.2%) loss: 1.2040\n",
      "Training (27.3%) loss: 0.7353\n",
      "Training (36.4%) loss: 0.6459\n",
      "Training (45.6%) loss: 0.5833\n",
      "Training (54.7%) loss: 0.5987\n",
      "Training (63.8%) loss: 0.5666\n",
      "Training (73.0%) loss: 0.5324\n",
      "Training (82.1%) loss: 0.4801\n",
      "Training (91.2%) loss: 0.4519\n",
      "Epoch [02/20]\n",
      "Training (9.0%) loss: 0.3958\n",
      "Training (18.2%) loss: 0.3808\n",
      "Training (27.3%) loss: 0.3570\n",
      "Training (36.4%) loss: 0.3371\n",
      "Training (45.6%) loss: 0.4728\n",
      "Training (54.7%) loss: 0.3311\n",
      "Training (63.8%) loss: 0.2639\n",
      "Training (73.0%) loss: 0.2195\n",
      "Training (82.1%) loss: 0.2134\n",
      "Training (91.2%) loss: 0.1754\n",
      "Epoch [03/20]\n",
      "Training (9.0%) loss: 0.1486\n",
      "Training (18.2%) loss: 0.1269\n",
      "Training (27.3%) loss: 0.1173\n",
      "Training (36.4%) loss: 0.1039\n",
      "Training (45.6%) loss: 0.0827\n",
      "Training (54.7%) loss: 0.0776\n",
      "Training (63.8%) loss: 0.0692\n",
      "Training (73.0%) loss: 0.0604\n",
      "Training (82.1%) loss: 0.0750\n",
      "Training (91.2%) loss: 0.0551\n",
      "Epoch [04/20]\n",
      "Training (9.0%) loss: 0.0656\n",
      "Training (18.2%) loss: 0.0591\n",
      "Training (27.3%) loss: 0.0943\n",
      "Training (36.4%) loss: 0.0462\n",
      "Training (45.6%) loss: 0.0511\n",
      "Training (54.7%) loss: 0.0412\n",
      "Training (63.8%) loss: 0.0342\n",
      "Training (73.0%) loss: 0.0371\n",
      "Training (82.1%) loss: 0.0223\n",
      "Training (91.2%) loss: 0.0849\n",
      "Epoch [05/20]\n",
      "Training (9.0%) loss: 0.0315\n",
      "Training (18.2%) loss: 0.0239\n",
      "Training (27.3%) loss: 0.0450\n",
      "Training (36.4%) loss: 0.0232\n",
      "Training (45.6%) loss: 0.0271\n",
      "Training (54.7%) loss: 0.0199\n",
      "Training (63.8%) loss: 0.0273\n",
      "Training (73.0%) loss: 0.0629\n",
      "Training (82.1%) loss: 0.0465\n",
      "Training (91.2%) loss: 0.0168\n",
      "Epoch [06/20]\n",
      "Training (9.0%) loss: 0.0280\n",
      "Training (18.2%) loss: 0.0251\n",
      "Training (27.3%) loss: 0.0251\n",
      "Training (36.4%) loss: 0.0257\n",
      "Training (45.6%) loss: 0.0186\n",
      "Training (54.7%) loss: 0.0087\n",
      "Training (63.8%) loss: 0.0056\n",
      "Training (73.0%) loss: 0.0095\n",
      "Training (82.1%) loss: 0.0300\n",
      "Training (91.2%) loss: 0.0498\n",
      "Epoch [07/20]\n",
      "Training (9.0%) loss: 0.0139\n",
      "Training (18.2%) loss: 0.0194\n",
      "Training (27.3%) loss: 0.0075\n",
      "Training (36.4%) loss: 0.0112\n",
      "Training (45.6%) loss: 0.0047\n",
      "Training (54.7%) loss: 0.0106\n",
      "Training (63.8%) loss: 0.0056\n",
      "Training (73.0%) loss: 0.0045\n",
      "Training (82.1%) loss: 0.0021\n",
      "Training (91.2%) loss: 0.0017\n",
      "Epoch [08/20]\n",
      "Training (9.0%) loss: 0.0032\n",
      "Training (18.2%) loss: 0.0031\n",
      "Training (27.3%) loss: 0.0106\n",
      "Training (36.4%) loss: 0.0057\n",
      "Training (45.6%) loss: 0.0120\n",
      "Training (54.7%) loss: 0.0319\n",
      "Training (63.8%) loss: 0.1464\n",
      "Training (73.0%) loss: 0.0447\n",
      "Training (82.1%) loss: 0.0255\n",
      "Training (91.2%) loss: 0.0219\n",
      "Epoch [09/20]\n",
      "Training (9.0%) loss: 0.0156\n",
      "Training (18.2%) loss: 0.0205\n",
      "Training (27.3%) loss: 0.0090\n",
      "Training (36.4%) loss: 0.0028\n",
      "Training (45.6%) loss: 0.0094\n",
      "Training (54.7%) loss: 0.0109\n",
      "Training (63.8%) loss: 0.0117\n",
      "Training (73.0%) loss: 0.0041\n",
      "Training (82.1%) loss: 0.0017\n",
      "Training (91.2%) loss: 0.0012\n",
      "Epoch [10/20]\n",
      "Training (9.0%) loss: 0.0023\n",
      "Training (18.2%) loss: 0.0073\n",
      "Training (27.3%) loss: 0.0016\n",
      "Training (36.4%) loss: 0.0011\n",
      "Training (45.6%) loss: 0.0011\n",
      "Training (54.7%) loss: 0.0022\n",
      "Training (63.8%) loss: 0.0085\n",
      "Training (73.0%) loss: 0.0058\n",
      "Training (82.1%) loss: 0.0073\n",
      "Training (91.2%) loss: 0.0022\n",
      "Epoch [11/20]\n",
      "Training (9.0%) loss: 0.0013\n",
      "Training (18.2%) loss: 0.0007\n",
      "Training (27.3%) loss: 0.0005\n",
      "Training (36.4%) loss: 0.0004\n",
      "Training (45.6%) loss: 0.0005\n",
      "Training (54.7%) loss: 0.0040\n",
      "Training (63.8%) loss: 0.0025\n",
      "Training (73.0%) loss: 0.0074\n",
      "Training (82.1%) loss: 0.0015\n",
      "Training (91.2%) loss: 0.0014\n",
      "Epoch [12/20]\n",
      "Training (9.0%) loss: 0.0150\n",
      "Training (18.2%) loss: 0.0600\n",
      "Training (27.3%) loss: 0.2368\n",
      "Training (36.4%) loss: 0.0237\n",
      "Training (45.6%) loss: 0.0105\n",
      "Training (54.7%) loss: 0.0076\n",
      "Training (63.8%) loss: 0.0101\n",
      "Training (73.0%) loss: 0.0171\n",
      "Training (82.1%) loss: 0.0138\n",
      "Training (91.2%) loss: 0.0084\n",
      "Epoch [13/20]\n",
      "Training (9.0%) loss: 0.0133\n",
      "Training (18.2%) loss: 0.0067\n",
      "Training (27.3%) loss: 0.0031\n",
      "Training (36.4%) loss: 0.0014\n",
      "Training (45.6%) loss: 0.0015\n",
      "Training (54.7%) loss: 0.0028\n",
      "Training (63.8%) loss: 0.0012\n",
      "Training (73.0%) loss: 0.0008\n",
      "Training (82.1%) loss: 0.0008\n",
      "Training (91.2%) loss: 0.0007\n",
      "Epoch [14/20]\n",
      "Training (9.0%) loss: 0.0010\n",
      "Training (18.2%) loss: 0.0056\n",
      "Training (27.3%) loss: 0.0085\n",
      "Training (36.4%) loss: 0.0005\n",
      "Training (45.6%) loss: 0.0010\n",
      "Training (54.7%) loss: 0.0026\n",
      "Training (63.8%) loss: 0.0004\n",
      "Training (73.0%) loss: 0.0029\n",
      "Training (82.1%) loss: 0.0105\n",
      "Training (91.2%) loss: 0.0007\n",
      "Epoch [15/20]\n",
      "Training (9.0%) loss: 0.0004\n",
      "Training (18.2%) loss: 0.0003\n",
      "Training (27.3%) loss: 0.0003\n",
      "Training (36.4%) loss: 0.0001\n",
      "Training (45.6%) loss: 0.0002\n",
      "Training (54.7%) loss: 0.0008\n",
      "Training (63.8%) loss: 0.0001\n",
      "Training (73.0%) loss: 0.0005\n",
      "Training (82.1%) loss: 0.0003\n",
      "Training (91.2%) loss: 0.0001\n",
      "Epoch [16/20]\n",
      "Training (9.0%) loss: 0.0001\n",
      "Training (18.2%) loss: 0.0001\n",
      "Training (27.3%) loss: 0.0001\n",
      "Training (36.4%) loss: 0.0001\n",
      "Training (45.6%) loss: 0.0006\n",
      "Training (54.7%) loss: 0.0007\n",
      "Training (63.8%) loss: 0.0001\n",
      "Training (73.0%) loss: 0.0002\n",
      "Training (82.1%) loss: 0.0001\n",
      "Training (91.2%) loss: 0.0001\n",
      "Epoch [17/20]\n",
      "Training (9.0%) loss: 0.0001\n",
      "Training (18.2%) loss: 0.0000\n",
      "Training (27.3%) loss: 0.0000\n",
      "Training (36.4%) loss: 0.0000\n",
      "Training (45.6%) loss: 0.0002\n",
      "Training (54.7%) loss: 0.0000\n",
      "Training (63.8%) loss: 0.0000\n",
      "Training (73.0%) loss: 0.0000\n",
      "Training (82.1%) loss: 0.0000\n",
      "Training (91.2%) loss: 0.0000\n",
      "Epoch [18/20]\n",
      "Training (9.0%) loss: 0.0000\n",
      "Training (18.2%) loss: 0.0000\n",
      "Training (27.3%) loss: 0.0000\n",
      "Training (36.4%) loss: 0.0000\n",
      "Training (45.6%) loss: 0.0000\n",
      "Training (54.7%) loss: 0.0000\n",
      "Training (63.8%) loss: 0.0000\n",
      "Training (73.0%) loss: 0.0000\n",
      "Training (82.1%) loss: 0.0000\n",
      "Training (91.2%) loss: 0.0000\n",
      "Epoch [19/20]\n",
      "Training (9.0%) loss: 0.0000\n",
      "Training (18.2%) loss: 0.0000\n",
      "Training (27.3%) loss: 0.0000\n",
      "Training (36.4%) loss: 0.0000\n",
      "Training (45.6%) loss: 0.0000\n",
      "Training (54.7%) loss: 0.0000\n",
      "Training (63.8%) loss: 0.0000\n",
      "Training (73.0%) loss: 0.0000\n",
      "Training (82.1%) loss: 0.0000\n",
      "Training (91.2%) loss: 0.0000\n",
      "Epoch [20/20]\n",
      "Training (9.0%) loss: 0.0000\n",
      "Training (18.2%) loss: 0.0000\n",
      "Training (27.3%) loss: 0.0000\n",
      "Training (36.4%) loss: 0.0000\n",
      "Training (45.6%) loss: 0.0000\n",
      "Training (54.7%) loss: 0.0000\n",
      "Training (63.8%) loss: 0.0000\n",
      "Training (73.0%) loss: 0.0000\n",
      "Training (82.1%) loss: 0.0000\n",
      "Training (91.2%) loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Encoder(input_lang.n_words, 1024).to(device)\n",
    "decoder_model = Decoder(1024, output_lang.n_words).to(device)\n",
    "\n",
    "encoder_model, decoder_model = train(encoder_model, decoder_model, n_epochs=20)\n",
    "\n",
    "#lr 3e-4, 512, 10 -> 0.71\n",
    "#lr 3e-4, 768, 20 -> 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8dda802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T16:09:55.308675Z",
     "iopub.status.busy": "2025-03-24T16:09:55.308251Z",
     "iopub.status.idle": "2025-03-24T16:09:55.314255Z",
     "shell.execute_reply": "2025-03-24T16:09:55.313589Z"
    },
    "id": "8e2c44d0",
    "papermill": {
     "duration": 0.022005,
     "end_time": "2025-03-24T16:09:55.315584",
     "exception": false,
     "start_time": "2025-03-24T16:09:55.293579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    input_tensor = sentence2tensor(input_lang, sentence)\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "    enocder_outs = torch.zeros((MAX_LENGTH, encoder.hidden_size), device=device)\n",
    "\n",
    "    for ei, elem in enumerate(input_tensor):\n",
    "        encoder_output, encoder_hidden = encoder(elem, encoder_hidden)\n",
    "        enocder_outs[ei] = encoder_output[0,0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, enocder_outs)\n",
    "        _, topi = decoder_output.data.topk(1)\n",
    "        decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "        if topi.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def predict_(encoder, decoder, dataset):\n",
    "    result = []\n",
    "\n",
    "    for _ in dataset:\n",
    "        result.append(evaluate(encoder, decoder, _)[:10])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c577d788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T16:09:55.341859Z",
     "iopub.status.busy": "2025-03-24T16:09:55.341629Z",
     "iopub.status.idle": "2025-03-24T16:09:55.346285Z",
     "shell.execute_reply": "2025-03-24T16:09:55.345648Z"
    },
    "papermill": {
     "duration": 0.019153,
     "end_time": "2025-03-24T16:09:55.347552",
     "exception": false,
     "start_time": "2025-03-24T16:09:55.328399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "\n",
    "    acc = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        pair = random.choice(train_dataset)\n",
    "        print(pair[0])\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ''.join(output_words)\n",
    "        print('<', output_sentence.replace('EOS', ''))\n",
    "        print(output_sentence.replace('EOS', '') == pair[1])\n",
    "        print('')\n",
    "        acc += int(output_sentence.replace('EOS', '') == pair[1])\n",
    "    print('Accuracy:', acc / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9988341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T16:09:55.373912Z",
     "iopub.status.busy": "2025-03-24T16:09:55.373702Z",
     "iopub.status.idle": "2025-03-24T16:09:56.541238Z",
     "shell.execute_reply": "2025-03-24T16:09:56.539962Z"
    },
    "papermill": {
     "duration": 1.182298,
     "end_time": "2025-03-24T16:09:56.542671",
     "exception": false,
     "start_time": "2025-03-24T16:09:55.360373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sipchil 10 2077\n",
      "> sipchil 10 2077\n",
      "= 17-10-2077\n",
      "< 17-10-2077\n",
      "True\n",
      "\n",
      "двадцать шестого августа 2007\n",
      "> двадцать шестого августа 2007\n",
      "= 26-08-2007\n",
      "< 26-08-2007\n",
      "True\n",
      "\n",
      "седмог 05 2049\n",
      "> седмог 05 2049\n",
      "= 07-05-2049\n",
      "< 07-05-2049\n",
      "True\n",
      "\n",
      "siebzehnter märz 2007\n",
      "> siebzehnter märz 2007\n",
      "= 17-03-2007\n",
      "< 17-03-2007\n",
      "True\n",
      "\n",
      "18.12.49\n",
      "> 18.12.49\n",
      "= 18-12-2049\n",
      "< 18-12-2049\n",
      "True\n",
      "\n",
      "двадесет шестог априла 2007\n",
      "> двадесет шестог априла 2007\n",
      "= 26-04-2007\n",
      "< 26-04-2007\n",
      "True\n",
      "\n",
      "25 февраля 2049\n",
      "> 25 февраля 2049\n",
      "= 25-02-2049\n",
      "< 25-02-2049\n",
      "True\n",
      "\n",
      "der neunzehnte april 2007\n",
      "> der neunzehnte april 2007\n",
      "= 19-04-2007\n",
      "< 19-04-2007\n",
      "True\n",
      "\n",
      "восьмого  02 2007\n",
      "> восьмого  02 2007\n",
      "= 08-02-2007\n",
      "< 08-02-2007\n",
      "True\n",
      "\n",
      "17 ноябрдә 2049\n",
      "> 17 ноябрдә 2049\n",
      "= 17-11-2049\n",
      "< 17-11-2049\n",
      "True\n",
      "\n",
      "12 semptembre 2077\n",
      "> 12 semptembre 2077\n",
      "= 12-09-2077\n",
      "< 12-09-2077\n",
      "True\n",
      "\n",
      "15 parwol 2007\n",
      "> 15 parwol 2007\n",
      "= 15-08-2007\n",
      "< 15-08-2007\n",
      "True\n",
      "\n",
      "le quatre février 2049\n",
      "> le quatre février 2049\n",
      "= 04-02-2049\n",
      "< 04-02-2049\n",
      "True\n",
      "\n",
      "җиденче июлдә 2007\n",
      "> җиденче июлдә 2007\n",
      "= 07-07-2007\n",
      "< 07-07-2007\n",
      "True\n",
      "\n",
      "21/12/77\n",
      "> 21/12/77\n",
      "= 21-12-2077\n",
      "< 21-12-2077\n",
      "True\n",
      "\n",
      "третье сентября 2007\n",
      "> третье сентября 2007\n",
      "= 03-09-2007\n",
      "< 03-09-2007\n",
      "True\n",
      "\n",
      "09 сентябрдә 2049\n",
      "> 09 сентябрдә 2049\n",
      "= 09-09-2049\n",
      "< 09-09-2049\n",
      "True\n",
      "\n",
      "den femte 02 2007\n",
      "> den femte 02 2007\n",
      "= 05-02-2007\n",
      "< 05-02-2007\n",
      "True\n",
      "\n",
      "le vingt et un novembre 2049\n",
      "> le vingt et un novembre 2049\n",
      "= 21-11-2049\n",
      "< 21-11-2049\n",
      "True\n",
      "\n",
      "am zwanzigsten 10 2007\n",
      "> am zwanzigsten 10 2007\n",
      "= 20-10-2007\n",
      "< 20-10-2007\n",
      "True\n",
      "\n",
      "унберенче 03 2077\n",
      "> унберенче 03 2077\n",
      "= 11-03-2077\n",
      "< 11-03-2077\n",
      "True\n",
      "\n",
      "четврти 01 2007\n",
      "> четврти 01 2007\n",
      "= 04-01-2007\n",
      "< 04-01-2007\n",
      "True\n",
      "\n",
      "03.12.2007\n",
      "> 03.12.2007\n",
      "= 03-12-2007\n",
      "< 03-12-2007\n",
      "True\n",
      "\n",
      "двадесет девети 03 2049\n",
      "> двадесет девети 03 2049\n",
      "= 29-03-2049\n",
      "< 29-03-2049\n",
      "True\n",
      "\n",
      "девятого 03 2049\n",
      "> девятого 03 2049\n",
      "= 09-03-2049\n",
      "< 09-03-2049\n",
      "True\n",
      "\n",
      "осми 12 2049\n",
      "> осми 12 2049\n",
      "= 08-12-2049\n",
      "< 08-12-2049\n",
      "True\n",
      "\n",
      "le neuf 05 2077\n",
      "> le neuf 05 2077\n",
      "= 09-05-2077\n",
      "< 09-05-2077\n",
      "True\n",
      "\n",
      "den sjuttonde 06 2077\n",
      "> den sjuttonde 06 2077\n",
      "= 17-06-2077\n",
      "< 17-06-2077\n",
      "True\n",
      "\n",
      "пятнадцатое 09 2007\n",
      "> пятнадцатое 09 2007\n",
      "= 15-09-2007\n",
      "< 15-09-2007\n",
      "True\n",
      "\n",
      "восьмого  02 2007\n",
      "> восьмого  02 2007\n",
      "= 08-02-2007\n",
      "< 08-02-2007\n",
      "True\n",
      "\n",
      "sa 01 2077\n",
      "> sa 01 2077\n",
      "= 04-01-2077\n",
      "< 04-01-2077\n",
      "True\n",
      "\n",
      "den fjärde 02 2007\n",
      "> den fjärde 02 2007\n",
      "= 04-02-2007\n",
      "< 04-02-2007\n",
      "True\n",
      "\n",
      "19 декабря 2007\n",
      "> 19 декабря 2007\n",
      "= 19-12-2007\n",
      "< 19-12-2007\n",
      "True\n",
      "\n",
      "03 јула 2007\n",
      "> 03 јула 2007\n",
      "= 03-07-2007\n",
      "< 03-07-2007\n",
      "True\n",
      "\n",
      "le douze 05 2049\n",
      "> le douze 05 2049\n",
      "= 12-05-2049\n",
      "< 12-05-2049\n",
      "True\n",
      "\n",
      "16 janvier 2007\n",
      "> 16 janvier 2007\n",
      "= 16-01-2007\n",
      "< 16-01-2007\n",
      "True\n",
      "\n",
      "am ein und dreizigsten 03 2007\n",
      "> am ein und dreizigsten 03 2007\n",
      "= 31-03-2007\n",
      "< 31-03-2007\n",
      "True\n",
      "\n",
      "sipsa'e 02 2049\n",
      "> sipsa'e 02 2049\n",
      "= 14-02-2049\n",
      "< 14-02-2049\n",
      "True\n",
      "\n",
      "унҗиденче 09 2049\n",
      "> унҗиденче 09 2049\n",
      "= 17-09-2049\n",
      "< 17-09-2049\n",
      "True\n",
      "\n",
      "18 juillet 2077\n",
      "> 18 juillet 2077\n",
      "= 18-07-2077\n",
      "< 18-07-2077\n",
      "True\n",
      "\n",
      "седми маја 2077\n",
      "> седми маја 2077\n",
      "= 07-05-2077\n",
      "< 07-05-2077\n",
      "True\n",
      "\n",
      "27 октября 2077\n",
      "> 27 октября 2077\n",
      "= 27-10-2077\n",
      "< 27-10-2077\n",
      "True\n",
      "\n",
      "четрнаестог 10 2049\n",
      "> четрнаестог 10 2049\n",
      "= 14-10-2049\n",
      "< 14-10-2049\n",
      "True\n",
      "\n",
      "sipgu irwol 2077\n",
      "> sipgu irwol 2077\n",
      "= 19-01-2077\n",
      "< 19-01-2077\n",
      "True\n",
      "\n",
      "25 sa'wol 2007\n",
      "> 25 sa'wol 2007\n",
      "= 25-04-2007\n",
      "< 25-04-2007\n",
      "True\n",
      "\n",
      "der sieben und zwanzigster november 2049\n",
      "> der sieben und zwanzigster november 2049\n",
      "= 27-11-2049\n",
      "< 27-11-2049\n",
      "True\n",
      "\n",
      "31 octobre 2077\n",
      "> 31 octobre 2077\n",
      "= 31-10-2077\n",
      "< 31-10-2077\n",
      "True\n",
      "\n",
      "13.06.07\n",
      "> 13.06.07\n",
      "= 13-06-2007\n",
      "< 13-06-2007\n",
      "True\n",
      "\n",
      "унынчы 04 2077\n",
      "> унынчы 04 2077\n",
      "= 10-04-2077\n",
      "< 10-04-2077\n",
      "True\n",
      "\n",
      "den tjugosjunde 02 2049\n",
      "> den tjugosjunde 02 2049\n",
      "= 27-02-2049\n",
      "< 27-02-2049\n",
      "True\n",
      "\n",
      "трећог 10 2049\n",
      "> трећог 10 2049\n",
      "= 03-10-2049\n",
      "< 03-10-2049\n",
      "True\n",
      "\n",
      "23.05.07\n",
      "> 23.05.07\n",
      "= 23-05-2007\n",
      "< 23-05-2007\n",
      "True\n",
      "\n",
      "isip 05 2049\n",
      "> isip 05 2049\n",
      "= 20-05-2049\n",
      "< 20-05-2049\n",
      "True\n",
      "\n",
      "am fünften januar 2049\n",
      "> am fünften januar 2049\n",
      "= 05-01-2049\n",
      "< 05-01-2049\n",
      "True\n",
      "\n",
      "24 септембра 2007\n",
      "> 24 септембра 2007\n",
      "= 24-09-2007\n",
      "< 24-09-2007\n",
      "True\n",
      "\n",
      "der siebzehnte 08 2077\n",
      "> der siebzehnte 08 2077\n",
      "= 17-08-2077\n",
      "< 17-08-2077\n",
      "True\n",
      "\n",
      "тридцатое марта 2049\n",
      "> тридцатое марта 2049\n",
      "= 30-03-2049\n",
      "< 30-03-2049\n",
      "True\n",
      "\n",
      "седьмое 11 2049\n",
      "> седьмое 11 2049\n",
      "= 07-11-2049\n",
      "< 07-11-2049\n",
      "True\n",
      "\n",
      "isipo 01 2049\n",
      "> isipo 01 2049\n",
      "= 25-01-2049\n",
      "< 25-01-2049\n",
      "True\n",
      "\n",
      "vierzehnter 05 2077\n",
      "> vierzehnter 05 2077\n",
      "= 14-05-2077\n",
      "< 14-05-2077\n",
      "True\n",
      "\n",
      "le vingt-neuf janvier 2049\n",
      "> le vingt-neuf janvier 2049\n",
      "= 29-01-2049\n",
      "< 29-01-2049\n",
      "True\n",
      "\n",
      "двадцать седьмого сентября 2049\n",
      "> двадцать седьмого сентября 2049\n",
      "= 27-09-2049\n",
      "< 27-09-2049\n",
      "True\n",
      "\n",
      "sipchil 10 2077\n",
      "> sipchil 10 2077\n",
      "= 17-10-2077\n",
      "< 17-10-2077\n",
      "True\n",
      "\n",
      "одиннаести 09 2049\n",
      "> одиннаести 09 2049\n",
      "= 11-09-2049\n",
      "< 11-09-2049\n",
      "True\n",
      "\n",
      "am zwanzigsten januar 2049\n",
      "> am zwanzigsten januar 2049\n",
      "= 20-01-2049\n",
      "< 20-01-2049\n",
      "True\n",
      "\n",
      "десетог јуна 2049\n",
      "> десетог јуна 2049\n",
      "= 10-06-2049\n",
      "< 10-06-2049\n",
      "True\n",
      "\n",
      "12.12.2049\n",
      "> 12.12.2049\n",
      "= 12-12-2049\n",
      "< 12-12-2049\n",
      "True\n",
      "\n",
      "den tjugoåttonde maj 2049\n",
      "> den tjugoåttonde maj 2049\n",
      "= 28-05-2049\n",
      "< 28-05-2049\n",
      "True\n",
      "\n",
      "am dritten dezember 2077\n",
      "> am dritten dezember 2077\n",
      "= 03-12-2077\n",
      "< 03-12-2077\n",
      "True\n",
      "\n",
      "am neunzehnten juni 2007\n",
      "> am neunzehnten juni 2007\n",
      "= 19-06-2007\n",
      "< 19-06-2007\n",
      "True\n",
      "\n",
      "шестог децембра 2007\n",
      "> шестог децембра 2007\n",
      "= 06-12-2007\n",
      "< 06-12-2007\n",
      "True\n",
      "\n",
      "егерме бишенче мартта 2049\n",
      "> егерме бишенче мартта 2049\n",
      "= 25-03-2049\n",
      "< 25-03-2049\n",
      "True\n",
      "\n",
      "isipe 12 2077\n",
      "> isipe 12 2077\n",
      "= 20-12-2077\n",
      "< 20-12-2077\n",
      "True\n",
      "\n",
      "23 декабря 2049\n",
      "> 23 декабря 2049\n",
      "= 23-12-2049\n",
      "< 23-12-2049\n",
      "True\n",
      "\n",
      "четврти јуна 2007\n",
      "> четврти јуна 2007\n",
      "= 04-06-2007\n",
      "< 04-06-2007\n",
      "True\n",
      "\n",
      "15/06/2077\n",
      "> 15/06/2077\n",
      "= 15-06-2077\n",
      "< 15-06-2077\n",
      "True\n",
      "\n",
      "пятое 09 2007\n",
      "> пятое 09 2007\n",
      "= 05-09-2007\n",
      "< 05-09-2007\n",
      "True\n",
      "\n",
      "den tjugosjunde 02 2049\n",
      "> den tjugosjunde 02 2049\n",
      "= 27-02-2049\n",
      "< 27-02-2049\n",
      "True\n",
      "\n",
      "samsipile chirwol 2077\n",
      "> samsipile chirwol 2077\n",
      "= 31-07-2077\n",
      "< 31-07-2077\n",
      "True\n",
      "\n",
      "25 февралда 2077\n",
      "> 25 февралда 2077\n",
      "= 25-02-2077\n",
      "< 25-02-2077\n",
      "True\n",
      "\n",
      "le dix-huit juillet 2049\n",
      "> le dix-huit juillet 2049\n",
      "= 18-07-2049\n",
      "< 18-07-2049\n",
      "True\n",
      "\n",
      "петнаести 12 2007\n",
      "> петнаести 12 2007\n",
      "= 15-12-2007\n",
      "< 15-12-2007\n",
      "True\n",
      "\n",
      "isipyuk 01 2007\n",
      "> isipyuk 01 2007\n",
      "= 26-01-2007\n",
      "< 26-01-2007\n",
      "True\n",
      "\n",
      "седамнаести марта 2049\n",
      "> седамнаести марта 2049\n",
      "= 17-03-2049\n",
      "< 17-03-2049\n",
      "True\n",
      "\n",
      "sipile 10 2049\n",
      "> sipile 10 2049\n",
      "= 11-10-2049\n",
      "< 11-10-2049\n",
      "True\n",
      "\n",
      "04 märz 2049\n",
      "> 04 märz 2049\n",
      "= 04-03-2049\n",
      "< 04-03-2049\n",
      "True\n",
      "\n",
      "23 маја 2049\n",
      "> 23 маја 2049\n",
      "= 23-05-2049\n",
      "< 23-05-2049\n",
      "True\n",
      "\n",
      "семнадцатого 02 2049\n",
      "> семнадцатого 02 2049\n",
      "= 17-02-2049\n",
      "< 17-02-2049\n",
      "True\n",
      "\n",
      "13 mars 2077\n",
      "> 13 mars 2077\n",
      "= 13-03-2077\n",
      "< 13-03-2077\n",
      "True\n",
      "\n",
      "den trettioförsta december 2049\n",
      "> den trettioförsta december 2049\n",
      "= 31-12-2049\n",
      "< 31-12-2049\n",
      "True\n",
      "\n",
      "26/07/49\n",
      "> 26/07/49\n",
      "= 26-07-2049\n",
      "< 26-07-2049\n",
      "True\n",
      "\n",
      "07 märz 2049\n",
      "> 07 märz 2049\n",
      "= 07-03-2049\n",
      "< 07-03-2049\n",
      "True\n",
      "\n",
      "13 dezember 2049\n",
      "> 13 dezember 2049\n",
      "= 13-12-2049\n",
      "< 13-12-2049\n",
      "True\n",
      "\n",
      "den tionde 04 2049\n",
      "> den tionde 04 2049\n",
      "= 10-04-2049\n",
      "< 10-04-2049\n",
      "True\n",
      "\n",
      "шеснаестог априла 2007\n",
      "> шеснаестог априла 2007\n",
      "= 16-04-2007\n",
      "< 16-04-2007\n",
      "True\n",
      "\n",
      "den trettonde 10 2007\n",
      "> den trettonde 10 2007\n",
      "= 13-10-2007\n",
      "< 13-10-2007\n",
      "True\n",
      "\n",
      "11 июня 2007\n",
      "> 11 июня 2007\n",
      "= 11-06-2007\n",
      "< 11-06-2007\n",
      "True\n",
      "\n",
      "двадцать первое 06 2077\n",
      "> двадцать первое 06 2077\n",
      "= 21-06-2077\n",
      "< 21-06-2077\n",
      "True\n",
      "\n",
      "уникенче 07 2077\n",
      "> уникенче 07 2077\n",
      "= 12-07-2077\n",
      "< 12-07-2077\n",
      "True\n",
      "\n",
      "le neuf 04 2077\n",
      "> le neuf 04 2077\n",
      "= 09-04-2077\n",
      "< 09-04-2077\n",
      "True\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder_model, decoder_model, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44d937fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T16:09:56.571953Z",
     "iopub.status.busy": "2025-03-24T16:09:56.571694Z",
     "iopub.status.idle": "2025-03-24T16:09:56.588584Z",
     "shell.execute_reply": "2025-03-24T16:09:56.587931Z"
    },
    "id": "C2euTKsiW7O9",
    "papermill": {
     "duration": 0.032756,
     "end_time": "2025-03-24T16:09:56.589711",
     "exception": false,
     "start_time": "2025-03-24T16:09:56.556955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('/kaggle/input/machine-translation-ioai/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "135a2e24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T16:09:56.618947Z",
     "iopub.status.busy": "2025-03-24T16:09:56.618697Z",
     "iopub.status.idle": "2025-03-24T16:10:45.477748Z",
     "shell.execute_reply": "2025-03-24T16:10:45.476885Z"
    },
    "id": "b5e06197",
    "papermill": {
     "duration": 48.876416,
     "end_time": "2025-03-24T16:10:45.479780",
     "exception": false,
     "start_time": "2025-03-24T16:09:56.603364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prediction = predict_(encoder_model, decoder_model, test_dataset['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a250009d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T16:10:45.509352Z",
     "iopub.status.busy": "2025-03-24T16:10:45.509015Z",
     "iopub.status.idle": "2025-03-24T16:10:45.515237Z",
     "shell.execute_reply": "2025-03-24T16:10:45.514489Z"
    },
    "id": "936f704c",
    "papermill": {
     "duration": 0.022251,
     "end_time": "2025-03-24T16:10:45.516601",
     "exception": false,
     "start_time": "2025-03-24T16:10:45.494350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prediction = [''.join(x) for x in test_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72eea181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T16:10:45.544805Z",
     "iopub.status.busy": "2025-03-24T16:10:45.544564Z",
     "iopub.status.idle": "2025-03-24T16:10:45.549070Z",
     "shell.execute_reply": "2025-03-24T16:10:45.548310Z"
    },
    "id": "BtI9Xj947bUL",
    "papermill": {
     "duration": 0.020169,
     "end_time": "2025-03-24T16:10:45.550482",
     "exception": false,
     "start_time": "2025-03-24T16:10:45.530313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset['label'] = test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2a9f828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T16:10:45.578924Z",
     "iopub.status.busy": "2025-03-24T16:10:45.578630Z",
     "iopub.status.idle": "2025-03-24T16:10:45.605082Z",
     "shell.execute_reply": "2025-03-24T16:10:45.604329Z"
    },
    "id": "7467be43",
    "papermill": {
     "duration": 0.041894,
     "end_time": "2025-03-24T16:10:45.606329",
     "exception": false,
     "start_time": "2025-03-24T16:10:45.564435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset[['id', 'label']].to_csv('submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77905f43",
   "metadata": {
    "papermill": {
     "duration": 0.014046,
     "end_time": "2025-03-24T16:10:45.635774",
     "exception": false,
     "start_time": "2025-03-24T16:10:45.621728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11495692,
     "sourceId": 96694,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 870.566361,
   "end_time": "2025-03-24T16:10:47.070750",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-24T15:56:16.504389",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
