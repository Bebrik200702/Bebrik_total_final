{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["Ih1zxm42QpG6","T8AEKDffD8k-","7dC7D3bMJz76"]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaL4","dataSources":[{"sourceId":91496,"databundleVersionId":11483707,"isSourceIdPinned":false,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Tokenization can have a significant impact on the quality of models. In this task, you will need to reduce the tokenizer to reduce the size of the model while maintaining quality. You will solve the problem of sentiment analysis. In the first part of the notebook, we implement a baseline, which measures the quality of the original model. In the second part, you need to reduce the tokenizer by removing at least 50% of tokens from it, reinitialize the embedding layer of BERT model and retrain the model. The classification quality should decrease by no more than 2% in terms of the F-measure, compared to the original model.","metadata":{"id":"n_vKWLtlBXKw"}},{"cell_type":"markdown","source":"### Baseline","metadata":{"id":"Ih1zxm42QpG6"}},{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate tokenizers","metadata":{"id":"wLd9mr9GkuvC","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:52:07.608140Z","iopub.execute_input":"2025-04-02T21:52:07.608436Z","iopub.status.idle":"2025-04-02T21:52:12.069459Z","shell.execute_reply.started":"2025-04-02T21:52:07.608412Z","shell.execute_reply":"2025-04-02T21:52:12.068646Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.21.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    BertTokenizerFast\n)\nfrom datasets import load_dataset\nimport evaluate\nimport os\nfrom tokenizers import BertWordPieceTokenizer","metadata":{"id":"dFqNUTS0mIFu","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:52:13.100722Z","iopub.execute_input":"2025-04-02T21:52:13.100979Z","iopub.status.idle":"2025-04-02T21:52:13.285415Z","shell.execute_reply.started":"2025-04-02T21:52:13.100956Z","shell.execute_reply":"2025-04-02T21:52:13.284798Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 1. Загрузка данных\ndataset = load_dataset(\"imdb\")\ntrain_data = dataset[\"train\"].shuffle(seed=42).select(range(5000))\ntest_data = dataset[\"test\"].shuffle(seed=42).select(range(1000))","metadata":{"id":"xqLZl3gtmKdF","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:52:15.196042Z","iopub.execute_input":"2025-04-02T21:52:15.196283Z","iopub.status.idle":"2025-04-02T21:52:20.993019Z","shell.execute_reply.started":"2025-04-02T21:52:15.196262Z","shell.execute_reply":"2025-04-02T21:52:20.992337Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40e09e489ec04031bc0739cbb57c7559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27b0a85ae7e54d508e789bf30221f74a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c814e9d44cae407186357d8e371ec0ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d61fe31245b481a9200eca7444648f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32922e84415b44ee94f00a90433f4c03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffcda95eecf04c2bb36d65420b3ee99b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d38867adcd4d0c894eb1ec2b9b711a"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# 2. Исходный токенизатор и модель\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ntrain_dataset = train_data.map(tokenize_function, batched=True)\ntest_dataset = test_data.map(tokenize_function, batched=True)","metadata":{"id":"_JliwRvGmPj_","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T22:26:35.169498Z","iopub.execute_input":"2025-04-02T22:26:35.169841Z","iopub.status.idle":"2025-04-02T22:26:35.381274Z","shell.execute_reply.started":"2025-04-02T22:26:35.169815Z","shell.execute_reply":"2025-04-02T22:26:35.380594Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"# 3. Обучение исходной модели\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=2\n)\n\nmodel.cuda()\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    report_to=['tensorboard'],\n    bf16=True,\n)\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"id":"5KGMcvSAmSjC","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:52:23.685931Z","iopub.execute_input":"2025-04-02T21:52:23.686171Z","iopub.status.idle":"2025-04-02T21:55:28.394923Z","shell.execute_reply.started":"2025-04-02T21:52:23.686151Z","shell.execute_reply":"2025-04-02T21:55:28.394248Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a272d54b4d1c47f18756348d4658ee30"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cb6dea3f2cc435c95013f58fe9d1618"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [471/471 02:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.339915</td>\n      <td>0.863000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.292434</td>\n      <td>0.881000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.296731</td>\n      <td>0.895000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=471, training_loss=0.2479311544171311, metrics={'train_runtime': 180.9323, 'train_samples_per_second': 82.904, 'train_steps_per_second': 2.603, 'total_flos': 1973332915200000.0, 'train_loss': 0.2479311544171311, 'epoch': 3.0})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# 4. Оценка исходной модели\noriginal_accuracy = trainer.evaluate()[\"eval_accuracy\"]\nprint(f\"Исходная точность: {original_accuracy:.4f}\")","metadata":{"id":"4Hjohn5nmXIh","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:55:31.541893Z","iopub.execute_input":"2025-04-02T21:55:31.542178Z","iopub.status.idle":"2025-04-02T21:55:35.828281Z","shell.execute_reply.started":"2025-04-02T21:55:31.542153Z","shell.execute_reply":"2025-04-02T21:55:35.827468Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [32/32 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Исходная точность: 0.8950\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Task 1\n\nRemove at least 50% of tokens from original tokenizer. Reinitialize new tokenizer. Give to new tokenizer 'new_tokenizer' name.","metadata":{"id":"T8AEKDffD8k-"}},{"cell_type":"code","source":"from collections import Counter\n\ndef get_top_k_tokens(tokenizer, texts, k: int = 10000) -> list:\n    \"\"\"\n    Извлекает топ-`k` наиболее часто встречающихся токенов из списка текстов с использованием заданного токенизатора.\n\n    1) Проходим по списку текстов, токенизирует каждый текст с помощью предоставленного токенизатора.\n    2) Подсчитываем количество появлений каждого токена и возвращает список из `k` наиболее частотных токенов.\n\n    Входные параметры:\n    - tokenizer (Tokenizer): Объект токенизатора.\n    - texts (list): Список текстов для обучения.\n    - k (int): Количество самых частотных токенов, которые необходимо вернуть.\n\n    Возвращает:\n    - list: Список из `k` наиболее часто встречающихся токенов.\n    \"\"\"\n    token_counts = Counter()\n    batch_size = 1000\n\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n        encodings = tokenizer.batch_encode_plus(batch_texts, padding=False, truncation=False)\n        # encodings = np.array(encodings['input_ids'])\n        for tokens in encodings['input_ids']:\n            token_counts.update(tokens)\n\n    # top_k_tokens = [token for token, count in token_counts.most_common(k)]\n\n    return token_counts","metadata":{"id":"sFiqdDrKXPdq","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:55:35.829416Z","iopub.execute_input":"2025-04-02T21:55:35.829734Z","iopub.status.idle":"2025-04-02T21:55:35.835540Z","shell.execute_reply.started":"2025-04-02T21:55:35.829705Z","shell.execute_reply":"2025-04-02T21:55:35.834807Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"texts = train_data['text'] + test_data['text']","metadata":{"id":"3JxtBSXWXRbB","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:55:35.836867Z","iopub.execute_input":"2025-04-02T21:55:35.837151Z","iopub.status.idle":"2025-04-02T21:55:35.890753Z","shell.execute_reply.started":"2025-04-02T21:55:35.837124Z","shell.execute_reply":"2025-04-02T21:55:35.890173Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"top_tokens = get_top_k_tokens(tokenizer, texts)","metadata":{"id":"UiUgiebTY1vD","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T21:55:35.891394Z","iopub.execute_input":"2025-04-02T21:55:35.891585Z","iopub.status.idle":"2025-04-02T21:55:36.695904Z","shell.execute_reply.started":"2025-04-02T21:55:35.891565Z","shell.execute_reply":"2025-04-02T21:55:36.695197Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (936 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"top_tokens_ids = {k : v for k,v in sorted(top_tokens.items(), key= lambda x: x[1])}\ntop_tokens_keys = {tokenizer.decode([k]) : v for k,v in sorted(top_tokens.items(), key= lambda x: x[1])}\nval_median = np.median(list(top_tokens_keys.values()))\ntop_tokens_to_delete = {tokenizer.decode([k]) : v for k,v in sorted(top_tokens.items(), key= lambda x: x[1]) if v <= 20}\ntop_tokens_to_stay = {tokenizer.decode([k]) : v for k,v in sorted(top_tokens.items(), key= lambda x: x[1]) if v > 20}","metadata":{"id":"d7An2u7aaO5Q","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T22:54:17.255369Z","iopub.execute_input":"2025-04-02T22:54:17.255696Z","iopub.status.idle":"2025-04-02T22:54:17.877252Z","shell.execute_reply.started":"2025-04-02T22:54:17.255667Z","shell.execute_reply":"2025-04-02T22:54:17.876550Z"}},"outputs":[],"execution_count":160},{"cell_type":"code","source":"#TODO Your code is here\nfrom copy import deepcopy\nfrom tqdm import tqdm\nimport json\nfrom tokenizers import models\n\nnew_tokenizer = deepcopy(tokenizer)\nmodel_state = json.loads(new_tokenizer.backend_tokenizer.model.__getstate__())\nfor word in top_tokens_to_delete:\n    del model_state[\"vocab\"][word]\n\nprint(len(model_state[\"vocab\"]))\nnew_tok_id = range(len(model_state[\"vocab\"]))\nnew_model_vocab = {}\nmapping = {}\nfor new_tok_id, (tok, tok_id) in zip(new_tok_id, model_state[\"vocab\"].items()):\n    new_model_vocab[tok] = new_tok_id\n    mapping[tok_id] = new_tok_id\nmodel_state[\"vocab\"] = new_model_vocab\n    \nmodel_class = getattr(models, model_state.pop(\"type\"))\nnew_tokenizer.backend_tokenizer.model = model_class(**model_state)\n\n\nprint(f\"\\nРазмеры словарей:\")\nprint(f\"Исходный: {len(tokenizer.vocab)}\")\nprint(f\"Новый: {len(new_tokenizer.vocab)}\")\nprint(f\"Удалено токенов: {len(tokenizer.vocab) - len(new_tokenizer.vocab)}\")\nprint(f\"Удалено токенов в %: {(len(tokenizer.vocab) - len(new_tokenizer.vocab)) * 100 / len(tokenizer.vocab)}\")","metadata":{"id":"gh_QNg-NG5AU","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T22:54:17.878165Z","iopub.execute_input":"2025-04-02T22:54:17.878381Z","iopub.status.idle":"2025-04-02T22:54:17.999786Z","shell.execute_reply.started":"2025-04-02T22:54:17.878362Z","shell.execute_reply":"2025-04-02T22:54:17.999101Z"}},"outputs":[{"name":"stdout","text":"14759\n\nРазмеры словарей:\nИсходный: 30522\nНовый: 14759\nУдалено токенов: 15763\nУдалено токенов в %: 51.644715287333725\n","output_type":"stream"}],"execution_count":161},{"cell_type":"code","source":"vocab_inv = {v:k for k,v in new_tokenizer.vocab.items()}\nvocab_inv[new_tokenizer(['celebrate'])['input_ids'][0][3]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T22:54:21.314857Z","iopub.execute_input":"2025-04-02T22:54:21.315148Z","iopub.status.idle":"2025-04-02T22:54:21.328484Z","shell.execute_reply.started":"2025-04-02T22:54:21.315124Z","shell.execute_reply":"2025-04-02T22:54:21.327833Z"}},"outputs":[{"execution_count":162,"output_type":"execute_result","data":{"text/plain":"'##brate'"},"metadata":{}}],"execution_count":162},{"cell_type":"markdown","source":"### Task 2:\n\nInitialize new Embedding layer of BERT model according to new tokenizer.\nRetrain model on classification task. The classification quality should decrease by no more than 2% in terms of the F-measure, compared to the original model.","metadata":{"id":"7dC7D3bMJz76"}},{"cell_type":"code","source":"#TODO Your code is here\n\n\n# 7. Переинициализация модели с новым словарем\nnew_model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=2,\n    ignore_mismatched_sizes=True  # Важно для изменения размера эмбеддингов!\n)\n\nnew_embeds = [0] * len(new_tokenizer.vocab)\nfor old_id, new_id in mapping.items():\n    new_embeds[new_id] = new_model.get_input_embeddings().weight.data[old_id]\nnew_embeds = torch.stack(new_embeds)\n\nnew_model.resize_token_embeddings(len(new_tokenizer.vocab))\n\ntarget_model_add_tokens_weight_data = new_model.get_input_embeddings().weight.data.clone()\n\nnew_model.get_input_embeddings().weight.data = new_embeds","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9UMjDlpkmesO","outputId":"faaabfe9-2003-40b6-fb13-b960557744f1","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T22:54:23.215103Z","iopub.execute_input":"2025-04-02T22:54:23.215404Z","iopub.status.idle":"2025-04-02T22:54:24.844338Z","shell.execute_reply.started":"2025-04-02T22:54:23.215378Z","shell.execute_reply":"2025-04-02T22:54:24.843604Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":163},{"cell_type":"code","source":"# 8. Обучение с новым токенизатором\ndef new_tokenize_function(examples):\n    return new_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\nnew_train_dataset = train_data.map(new_tokenize_function, batched=True)\nnew_test_dataset = test_data.map(new_tokenize_function, batched=True)\n\nnew_trainer = Trainer(\n    model=new_model,\n    args=training_args,\n    train_dataset=new_train_dataset,\n    eval_dataset=new_test_dataset,\n    compute_metrics=compute_metrics,\n)\n\nnew_trainer.train()\nnew_accuracy = new_trainer.evaluate()[\"eval_accuracy\"]","metadata":{"id":"vsnK9FrsoJ7T","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T22:54:24.845227Z","iopub.execute_input":"2025-04-02T22:54:24.845458Z","iopub.status.idle":"2025-04-02T22:57:22.954661Z","shell.execute_reply.started":"2025-04-02T22:54:24.845437Z","shell.execute_reply":"2025-04-02T22:57:22.953974Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c06de8cf53444d92939bea3ecee8bc08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f252a43770a24c0c9f0774f4a4b7d8a6"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [471/471 02:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.342475</td>\n      <td>0.858000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.294698</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.288918</td>\n      <td>0.897000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [32/32 00:03]\n    </div>\n    "},"metadata":{}}],"execution_count":164},{"cell_type":"code","source":"# 9. Результаты\nprint(\"\\nРезультаты сравнения:\")\nprint(f\"Исходная точность: {original_accuracy:.4f}\")\nprint(f\"Точность с новым словарем: {new_accuracy:.4f}\")\nprint(f\"Разница: {original_accuracy - new_accuracy:.4f}\")\nprint(f\"Экономия памяти: {(len(tokenizer.vocab) - len(new_tokenizer.vocab))/len(tokenizer.vocab):.2%}\")","metadata":{"id":"ge9nohQomgzB","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T22:57:36.842407Z","iopub.execute_input":"2025-04-02T22:57:36.842739Z","iopub.status.idle":"2025-04-02T22:57:36.874961Z","shell.execute_reply.started":"2025-04-02T22:57:36.842712Z","shell.execute_reply":"2025-04-02T22:57:36.874302Z"}},"outputs":[{"name":"stdout","text":"\nРезультаты сравнения:\nИсходная точность: 0.8950\nТочность с новым словарем: 0.8970\nРазница: -0.0020\nЭкономия памяти: 51.64%\n","output_type":"stream"}],"execution_count":166},{"cell_type":"code","source":"","metadata":{"id":"95Fea846zBf0","trusted":true},"outputs":[],"execution_count":null}]}