{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4dPSnSw59x9"
      },
      "source": [
        "# Morphological Inflection in Navajo\n",
        "\n",
        "## Problem Description\n",
        "\n",
        "Morphological inflection is a task in computational linguistics wherein the correct form of a word has to be generated from a lemma (base form) and a target morphosyntactic specification, e.g.\n",
        "\n",
        "\n",
        "```\n",
        "alzhish  +  V;IND;PFV;NOM(2,SG) -> íínílzhiizh\n",
        "```\n",
        "\n",
        "\n",
        "where *alzhish* is the Navajo word for 'dance', and *íínílzhiizh* is the second [2] person , singular [SG], nominative [NOM] form of the verb [V] in the indicative [IND] mood and perfect [PFV] aspect. You can find out more about the annotation schema used for the morphosyntactic specification of the target forms [here](https://unimorph.github.io/doc/unimorph-schema.pdf).\n",
        "\n",
        "This task finds application in the construction of resources for language acquisition, in statistical machine translation, and in the computational study of language.\n",
        "\n",
        "The difficulty of the task for a given language depends on the size and complexity of the inflectional paradigm of the language. A large paradigm, which distinguishes between many morphosyntactic properties (number, gender, case, aspect, etc.) and many values for these properties, requires extensive machine learning from sizeable data. While inflectional paragims tend to be highly regular, they can vary in complexity, with many factors guiding which exact morpheme should attach to a given root to mark a specific morphosyntactic property. And irregular forms can often be found too, which cannot be predicted and have to be memorized instead.\n",
        "\n",
        "Here, we ask you to train a machine learning model to perform morphological inflection in Navajo.\n",
        "\n",
        "## Your Task\n",
        "\n",
        "The code below is a near reimplementation of the approach to moprhological inflection presented in [Wu et al.](https://aclanthology.org/2021.eacl-main.163.pdf). The reimplementation uses high-level API from the *transformers* Python library. High-level APIs are convenient as they save us a lot of code-writing, but they also obscure certain aspects of the implementation.\n",
        "\n",
        "The approach of Wu and colleagues achieves a score of 52.1% for Navajo (as reported [here]()), while the score of the model below lingers about 3 percentage points behind. Study the approach of Wu and colleagues as described in their article to find out what is missing from the model below and make the necessary changes to reproduce their result.   \n",
        "\n",
        "Notice that due to the stochasticity of the model initialization and training, slight deviations from the expected score are admissible (within 0.5 percentage points).\n",
        "\n",
        "### Deliverables:\n",
        "* code\n",
        "* model accuracy on the test set\n",
        "* a report of up to 300 words describing the changes made to the model architecture, data processing, training procedure etc.\n",
        "    * If you do not succeed in reproducing the expected score, comment on what the reason may be.\n",
        "\n",
        "\n",
        "## Technical Specifications\n",
        "\n",
        "* All team solutions should be submitted as a modified and compiled copy of this base notebook.\n",
        "* Do not change cells starting with the `###DO NOT CHANGE THIS CELL###` comment.\n",
        "* You can use any platform to carry out the development of your model, but for the final submission you have to integrate your solution back into this notebook, adding code to install all dependencies and making sure that your model takes no longer than 8 hours to train.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yui_SWs9mXx-",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec5ec0c-0c34-4171-cfb5-3d7ecf559a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 evaluate-0.4.3 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers\n",
        "! pip install -U evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R_dknIv5LIsz",
        "outputId": "b12de30a-0d0f-409e-fff0-6a03c3f8121a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 10000 training data samples and 1000 dev samples.\n",
            "Raw data sample: adiłhash\tV;IND;PFV;NOM(3,GRPL)\tdaʼdiłhash\n"
          ]
        }
      ],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "\"\"\"\n",
        "Download the train and dev data\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_data(path):\n",
        "        response = requests.get(path)\n",
        "        return response.text.strip().split('\\n')\n",
        "\n",
        "train_data_path = 'https://raw.githubusercontent.com/sigmorphon/2023InflectionST/main/part1/data/nav.trn'\n",
        "dev_data_path = 'https://raw.githubusercontent.com/sigmorphon/2023InflectionST/main/part1/data/nav.dev'\n",
        "\n",
        "data = {}\n",
        "data['train'] = download_data(train_data_path)\n",
        "data['dev'] = download_data(dev_data_path)\n",
        "\n",
        "print('Downloaded {} training data samples and {} dev samples.'.format(len(data['train']), len(data['dev'])))\n",
        "print('Raw data sample:', data['train'][54])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0HpSXXVLIs0"
      },
      "source": [
        "# Task 1\n",
        "Modify the code below to achieve a full reimplementation of the approach of Wu and colleagues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1tJreU_6Y2Q",
        "outputId": "2d745fa4-1972-4e93-df8c-acfbf42bbedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed data sample: ('adiłhash <V><IND><PFV><NOM><3><GRPL>', 'daʼdiłhash')\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "DATA PREPROCESSING\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import regex\n",
        "\n",
        "def parse_tag(tag):\n",
        "    tag = re.sub(r\"\\)|\\(|,|;\", ' ', tag).split()\n",
        "    return ''.join(['<{}>'.format(t) for t in tag])\n",
        "\n",
        "def preprocess_data(raw_data):\n",
        "        preprocessed_data = []\n",
        "        for line in raw_data:\n",
        "          lemma, tag, target = line.split('\\t')\n",
        "          preprocessed_data.append(('{} {}'.format(lemma, parse_tag(tag)),target))\n",
        "        return preprocessed_data\n",
        "\n",
        "data['train'] = preprocess_data(data['train'])\n",
        "data['dev'] = preprocess_data(data['dev'])\n",
        "\n",
        "print('Preprocessed data sample:', data['train'][54])\n",
        "\n",
        "chars = set(list(''.join([''.join([d[0].split()[0], d[1]]) for d in data['train']])))\n",
        "char2id = { char: i for i, char in enumerate(chars)}\n",
        "tags = list(set(sum([regex.findall(r\"<[A-Za-z0-9]*>\",d[0]) for d in data['train']], [])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XO-qTiG5LIs0",
        "outputId": "c6be87bb-7f46-4ed7-9944-2fc503d753fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization example: ['a', 'd', 'i', 'ł', 'h', 'a', 's', 'h', ' ', '<V>', '<IND>', '<PFV>', '<NOM>', '<3>', '<GRPL>']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "TOKENIZER\n",
        "Input and output word forms are processed one character at a time, while morphosyntactic features are treated as special atomic tokens.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict, List\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers.tokenization_utils import AddedToken\n",
        "\n",
        "import warnings\n",
        "\n",
        "class CustomTokenizer(PreTrainedTokenizer):\n",
        "\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Dict[str, int],\n",
        "        bos_token=\"<s>\",\n",
        "        eos_token=\"</s>\",\n",
        "        unk_token=\"<unk>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        # Add extra_ids to the special token list\n",
        "\n",
        "        self.__token_ids = vocab\n",
        "        self.__id_tokens: Dict[int, str] = {value: key for key, value in vocab.items()}\n",
        "\n",
        "        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n",
        "        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n",
        "        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n",
        "        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n",
        "        self._added_tokens_decoder = {0: pad_token, 1: bos_token, 2: eos_token, 3: unk_token}\n",
        "        self.offset = len(self._added_tokens_decoder)\n",
        "\n",
        "        super().__init__(\n",
        "            bos_token=bos_token,\n",
        "            eos_token=eos_token,\n",
        "            unk_token=unk_token,\n",
        "            pad_token=pad_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self.__token_ids)\n",
        "\n",
        "    def get_vocab(self) -> Dict[str, int]:\n",
        "        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size + self.offset)}\n",
        "        vocab.update(self.added_tokens_encoder)\n",
        "        return vocab\n",
        "\n",
        "    def _add_eos(self, token_ids: List[int]) -> List[int]:\n",
        "        return token_ids + [self.eos_token_id]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        eos = [self.eos_token_id]\n",
        "\n",
        "        if token_ids_1 is None:\n",
        "            return len(token_ids_0 + eos) * [0]\n",
        "        return len(token_ids_0 + eos + token_ids_1 + eos) * [0]\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        token_ids_0 = self._add_eos(token_ids_0)\n",
        "        if token_ids_1 is None:\n",
        "            return token_ids_0\n",
        "        else:\n",
        "            token_ids_1 = self._add_eos(token_ids_1)\n",
        "            return token_ids_0 + token_ids_1\n",
        "\n",
        "    def _tokenize(self, text: str, **kwargs):\n",
        "        return list(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        return self.__token_ids[token]+self.offset if token in self.__token_ids else self.unk_token_id\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        return self.__id_tokens[index-self.offset] if index-self.offset in self.__id_tokens else self.unk_token\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "    def save_vocabulary(self, save_directory: str,\n",
        "                        filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
        "        if filename_prefix is None:\n",
        "            filename_prefix = ''\n",
        "        vocab_path = Path(save_directory, filename_prefix + 'vocab.json')\n",
        "        json.dump(self.__token_ids, open(vocab_path, 'w'))\n",
        "        return str(vocab_path),\n",
        "\n",
        "tokenizer = CustomTokenizer(char2id, additional_special_tokens=tags, max_len=100)\n",
        "print('Tokenization example:', tokenizer.tokenize(data['train'][54][0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class BartLearnedPositionalEmbedding(nn.Embedding):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        self.offset = 2\n",
        "        super().__init__(num_embeddings + self.offset, embedding_dim)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n",
        "        bsz, seq_len = input_ids.shape[:2]\n",
        "        device = input_ids.device\n",
        "\n",
        "        mask_24 = (input_ids == 20)\n",
        "        idx_24 = torch.where(mask_24.any(dim=1), mask_24.float().argmax(dim=1), torch.full((bsz,), seq_len-1, device=device, dtype=torch.long))\n",
        "        positions = torch.arange(1, seq_len+1, dtype=torch.long, device=device).unsqueeze(0).expand(bsz, -1)\n",
        "        mask_after_24 = torch.arange(seq_len, device=device).unsqueeze(0).expand(bsz, -1) > idx_24.unsqueeze(1)\n",
        "\n",
        "        positions = positions.masked_fill(mask_after_24, 0)\n",
        "\n",
        "        return super().forward(positions + self.offset)"
      ],
      "metadata": {
        "id": "WLIqPLqTomwu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5kmN6C1iAxlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a18d0b-38bf-4fec-f53b-f6cfaa88ec8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config of the encoder: <class 'transformers.models.bert.modeling_bert.BertModel'> is overwritten by shared encoder config: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.2,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.2,\n",
            "  \"hidden_size\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1024,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.bert.modeling_bert.BertLMHeadModel'> is overwritten by shared decoder config: BertConfig {\n",
            "  \"add_cross_attention\": true,\n",
            "  \"attention_probs_dropout_prob\": 0.2,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.2,\n",
            "  \"hidden_size\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1024,\n",
            "  \"is_decoder\": true,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "MODEL\n",
        "The model is based on the t5 model architecture: a transformer-based model with an encoder\n",
        "and a decoder, trained to take an input sequence (the lemma and the target tag)\n",
        "and to generate an output sequence (the target form).\n",
        "\"\"\"\n",
        "\n",
        "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
        "\n",
        "config_enc = BertConfig(\n",
        "    hidden_size=256,\n",
        "    num_hidden_layers=4,\n",
        "    num_attention_heads=4,\n",
        "    intermediate_size=1024,\n",
        "    position_embedding_type='absolute',\n",
        "    hidden_dropout_prob=0.2,\n",
        "    attention_probs_dropout_prob=0.2,\n",
        "    vocab_size=len(tokenizer)\n",
        ")\n",
        "\n",
        "config_dec = BertConfig(\n",
        "    hidden_size=256,\n",
        "    num_hidden_layers=4,\n",
        "    num_attention_heads=4,\n",
        "    intermediate_size=1024,\n",
        "    position_embedding_type='absolute',\n",
        "    hidden_dropout_prob=0.2,\n",
        "    attention_probs_dropout_prob=0.2,\n",
        "    vocab_size=len(tokenizer),\n",
        "    is_decoder=True\n",
        ")\n",
        "\n",
        "\n",
        "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_enc,config_dec)\n",
        "\n",
        "model = EncoderDecoderModel(config)\n",
        "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "model.generation_config.max_new_tokens = 32\n",
        "model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.bos_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIhbWtfTqeNA",
        "outputId": "de6a030d-3531-4aa3-8979-d0f86041705c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.model.encoder.embed_positions = BartLearnedPositionalEmbedding(52, 256)\n",
        "#model.model.decoder.embed_positions = BartLearnedPositionalEmbedding(52, 256)"
      ],
      "metadata": {
        "id": "ZgABPUqGqFTi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CWQTI68gLIs1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "UTILITIES\n",
        "\"\"\"\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        input, target = self.data[idx]\n",
        "        return {\"input_ids\": self.tokenizer(input, padding='longest',\n",
        "                                            truncation=True,\n",
        "                                            add_special_tokens=True)['input_ids'],\n",
        "                \"labels\": self.tokenizer(target, padding='longest',\n",
        "                                         truncation=True,\n",
        "                                         add_special_tokens=True)['input_ids']}\n",
        "\n",
        "def postprocess_data(token_ids):\n",
        "    token_ids = np.where(token_ids != -100, token_ids, tokenizer.pad_token_id)\n",
        "    return tokenizer.batch_decode(token_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PixE9lVPLIs1"
      },
      "outputs": [],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "\"\"\"\n",
        "MAIN METRIC\n",
        "\"\"\"\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"exact_match\")\n",
        "    preds, labels = eval_preds\n",
        "    decoded_preds = postprocess_data(preds)\n",
        "    decoded_labels = postprocess_data(labels)\n",
        "\n",
        "    # During development, you can uncomment the lines to see what predictions your model makes\n",
        "    ks = random.choices(list(range(len(decoded_preds))), k=15)\n",
        "    print('Predicted:', [decoded_preds[k] for k in ks])\n",
        "    print('Targets:', [decoded_labels[k] for k in ks])\n",
        "    print('___________________________________________________________________')\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0n-bgxSH8HP",
        "outputId": "421e3512-ee2b-4781-d83d-3ce48bededca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d66b8fc4d75f>:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14070' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14070/20000 1:05:32 < 27:37, 3.58 it/s, Epoch 562.76/800]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Exact Match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.559600</td>\n",
              "      <td>1.370481</td>\n",
              "      <td>0.128000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.149300</td>\n",
              "      <td>1.067726</td>\n",
              "      <td>0.404000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.880900</td>\n",
              "      <td>1.058576</td>\n",
              "      <td>0.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.795200</td>\n",
              "      <td>1.069462</td>\n",
              "      <td>0.451000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.762200</td>\n",
              "      <td>1.114788</td>\n",
              "      <td>0.468000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.746800</td>\n",
              "      <td>1.135823</td>\n",
              "      <td>0.491000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.738200</td>\n",
              "      <td>1.148750</td>\n",
              "      <td>0.482000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.734000</td>\n",
              "      <td>1.147503</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.732000</td>\n",
              "      <td>1.154965</td>\n",
              "      <td>0.474000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.730800</td>\n",
              "      <td>1.150636</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.727800</td>\n",
              "      <td>1.173902</td>\n",
              "      <td>0.478000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.724700</td>\n",
              "      <td>1.189216</td>\n",
              "      <td>0.483000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.721500</td>\n",
              "      <td>1.188940</td>\n",
              "      <td>0.512000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.720100</td>\n",
              "      <td>1.175067</td>\n",
              "      <td>0.522000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.718400</td>\n",
              "      <td>1.188277</td>\n",
              "      <td>0.507000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>0.716900</td>\n",
              "      <td>1.150661</td>\n",
              "      <td>0.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>0.715900</td>\n",
              "      <td>1.200727</td>\n",
              "      <td>0.509000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>0.714800</td>\n",
              "      <td>1.166500</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>0.714200</td>\n",
              "      <td>1.169065</td>\n",
              "      <td>0.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.713500</td>\n",
              "      <td>1.181807</td>\n",
              "      <td>0.529000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>0.712900</td>\n",
              "      <td>1.200919</td>\n",
              "      <td>0.511000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>1.192538</td>\n",
              "      <td>0.526000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>0.712000</td>\n",
              "      <td>1.184558</td>\n",
              "      <td>0.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>0.711900</td>\n",
              "      <td>1.144623</td>\n",
              "      <td>0.596000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.711400</td>\n",
              "      <td>1.232928</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>0.711300</td>\n",
              "      <td>1.214309</td>\n",
              "      <td>0.498000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>0.710800</td>\n",
              "      <td>1.195856</td>\n",
              "      <td>0.537000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>0.710900</td>\n",
              "      <td>1.191606</td>\n",
              "      <td>0.528000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11600</td>\n",
              "      <td>0.710200</td>\n",
              "      <td>1.200503</td>\n",
              "      <td>0.523000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.710200</td>\n",
              "      <td>1.171302</td>\n",
              "      <td>0.545000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12400</td>\n",
              "      <td>0.709900</td>\n",
              "      <td>1.169542</td>\n",
              "      <td>0.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12800</td>\n",
              "      <td>0.710000</td>\n",
              "      <td>1.209713</td>\n",
              "      <td>0.519000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13200</td>\n",
              "      <td>0.709600</td>\n",
              "      <td>1.240407</td>\n",
              "      <td>0.515000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13600</td>\n",
              "      <td>0.709500</td>\n",
              "      <td>1.188264</td>\n",
              "      <td>0.517000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.709500</td>\n",
              "      <td>1.192299</td>\n",
              "      <td>0.536000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['jiltłʼéééh', 'diiichʼééh', 'ajiłʼá', 'adadoołjid', 'dajitʼeesh', 'dadooshdoł', 'jiʼeeł', 'hashkaał', 'ndaałjoł', 'danichʼééh', 'adajiiłjid', 'jichaaʼ', 'diichʼééh', 'adadiilmáás', 'yisʼáá']\n",
            "Targets: ['jooltłʼá', 'diichʼééh', 'ajííłʼaʼ', 'adeidoołjił', 'dajitʼeesh', 'yisdádadoołʼoł', 'jiʼeeł', 'haashkaał', 'nidaałjooł', 'diichʼééh', 'adajishjid', 'jíícha', 'diishchʼééh', 'adadiiʼmas', 'yishʼá']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['nighaas', 'adadoołjił', 'yishdleeł', 'woołhash', 'daʼalzhsh', 'dashiilghash', 'yínítłʼił', 'nisooʼaa', 'hojilghaał', 'yínítłoh', 'nidasiitʼa', 'hajiłkaał', 'hadasoołkaal', 'yiiltłʼoʼ', 'woołdéél']\n",
            "Targets: ['nighaas', 'adadoołjił', 'yishdeeł', 'shoołhash', 'daʼalizh', 'dashiilghash', 'yítłʼił', 'niʼsoozo', 'jiilghaał', 'yítłoh', 'nidaʼsiidzo', 'hajiłkaał', 'hadasoołkaal', 'yiiltʼoh', 'woołdéél']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yiishhash', 'yishtʼeesh', 'dajídzį́į́z', 'dajítłʼóół', 'nijiniih', 'yiigaaz', 'yiyiiłtsóód', 'yildin', 'daałchʼil', 'jííkʼaad', 'yiitłʼó', 'dazhdeichʼą́ʼ', 'azhdoołjił', 'nidaashchʼiʼ', 'hayííłgeezh']\n",
            "Targets: ['shéłhash', 'yishtʼeesh', 'dajizdzį́į́z', 'dajítłʼóóh', 'njiniih', 'yiigaz', 'yiyiiłtsóód', 'yildin', 'daałchʼil', 'jííkʼą́', 'yiitłʼó', 'dazhdiichʼeeʼ', 'azhdoołjił', 'nidaashchiʼ', 'hayííłgizh']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['oołtʼááh', 'hadiilkał', 'nidaałchiʼ', 'daashtłʼá', 'ndiidlééł', 'ndazhdoolééł', 'adajiłjiid', 'ííłʼaad', 'yiyííʼeez', 'yiilʼaad', 'jooldéél', 'dahonooʼá', 'yishtʼeesh', 'hoolghaal', 'nisiiʼniiʼ']\n",
            "Targets: ['oołtʼááh', 'hadiilkał', 'nidaałchiʼ', 'daastłʼá', 'ndiidleeł', 'ndazhdooleeł', 'adajiłjiid', 'ííłʼaʼ', 'yíʼeez', 'yiilʼá', 'jooldéél', 'dahinohná', 'yishtʼeesh', 'hoolghal', 'nisiiʼniiʼ']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['jółtʼaʼ', 'jooltłʼá', 'yíníghaaz', 'yiitsʼiid', 'yiyiistsood', 'deiilgąąsh', 'iiltʼááh', 'diichʼééh', 'nijishtłizh', 'daʼsoołʼaad', 'ajiłjiid', 'yisas', 'diilchʼił', 'nidajishtłizh', 'yoolʼaad']\n",
            "Targets: ['jííłtaʼ', 'jooltłʼá', 'yíníghaz', 'yiitsʼííd', 'yiyiiłtsood', 'deiilgąsh', 'iiltʼááh', 'diichʼééh', 'njizhtłizh', 'adasoołʼaʼ', 'ajiłjiid', 'yisas', 'diilchʼił', 'ndajizhtłizh', 'yilʼá']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['nizhdoonih', 'jooʼoł', 'nisétłʼah', 'adasiilʼaad', 'adadoołtʼah', 'jííghaaz', 'ajiłʼá', 'dazhdookʼaał', 'yíníʼéél', 'nidaahtin', 'adadiiljił', 'yínółtʼaʼ', 'deeshchaał', 'haiłkaał', 'jidiichʼą́ą́ʼ']\n",
            "Targets: ['nizhdoonih', 'jooʼoł', 'nishtłʼah', 'adasiilʼaʼ', 'adadoołtʼah', 'jííghaz', 'ajiłʼá', 'dazhdookʼą́ą́ł', 'yíníʼéél', 'nidaahtin', 'adadiiljił', 'yínóółtaʼ', 'deeshchah', 'haiłkaał', 'jidiichʼeeʼ']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['hwiilneʼ', 'yiltłʼééh', 'diilgąsh', 'naniłjooł', 'hadaałgéésh', 'nisooniiʼ', 'deínółtʼaʼ', 'yisdzį́į́s', 'yootłʼóół', 'yishchʼil', 'ajilzhih', 'yínítłʼóód', 'jidoosdił', 'dajoodzį́į́s', 'ííłʼaad']\n",
            "Targets: ['hwiilneʼ', 'yiltłʼééh', 'diilgąsh', 'naniłjooł', 'hadaałgéésh', 'nisooniiʼ', 'dawóołtaʼ', 'yídzį́į́z', 'yootłʼóół', 'yishchʼil', 'ajilizh', 'sínítłʼǫ́', 'yisdázhdoołʼoł', 'dajizdzį́į́z', 'ííłʼaʼ']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['jooltłʼá', 'hołneʼ', 'nidajishjool', 'dajíneez', 'yiitłoh', 'jííłchʼil', 'ajiłʼá', 'daoohchah', 'hozhníná', 'yiʼoł', 'yíníłchʼil', 'nizhdoolchʼih', 'dajishtłʼá', 'dajooldéél', 'halghaał']\n",
            "Targets: ['jooltłʼá', 'hołneʼ', 'nidajishjool', 'dajíneez', 'yiitłoh', 'jishchʼil', 'ajiłʼá', 'daoocha', 'jiiná', 'yiʼoł', 'shíníłchʼil', 'nizhdoolchiił', 'dajistłʼá', 'dajooldéél', 'hilghaał']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['jiigis', 'deiigaas', 'daoołgąąsh', 'daoohdzį́į́z', 'daʼiilzhih', 'nidaʼiidzo', 'oolziiʼ', 'yiitʼeez', 'doołchʼił', 'dajiiłtsóód', 'nidashiitłizh', 'dazhdijoojol', 'doołchʼił', 'deísaas', 'haiilkaał']\n",
            "Targets: ['jiigis', 'deiigaas', 'daoołgąsh', 'dasoodzį́į́z', 'daʼiidlizh', 'nidaʼiidzo', 'azhlizh', 'yiitʼeez', 'doołchʼił', 'dajiiłtsóód', 'ndashiitłizh', 'dazhdijool', 'doołchʼił', 'deísáás', 'haiilkaał']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['níshínízhaaʼ', 'hadajiłgéésh', 'dajiigis', 'ségis', 'hoolneʼ', 'názhaah', 'jighaas', 'iilʼaad', 'yiyííłchʼil', 'nideiłjooł', 'ajiłjiid', 'deiigaas', 'yishtʼeesh', 'adadoohmas', 'nishínítłish']\n",
            "Targets: ['níshínízhah', 'hadajiłgéésh', 'dajiigis', 'ségis', 'hoolneʼ', 'názhah', 'jighaas', 'iilʼaʼ', 'yishchʼil', 'nideiłjooł', 'ajiłjiid', 'deiigaas', 'yishtʼeesh', 'adadoohmas', 'nishínítłizh']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yíníiltʼaʼ', 'yishʼaad', 'ndeeshjoł', 'yisdzį́į́s', 'halneʼ', 'iidoołjił', 'nisézo', 'yiildeeł', 'iishtʼááh', 'adadoołjił', 'hajiłgéésh', 'dashiiltłʼá', 'adeiiltʼááh', 'yiitʼoł', 'yiilchʼil']\n",
            "Targets: ['wóoltaʼ', 'yishʼá', 'ndeeshjoł', 'yisdzį́į́s', 'hoolneʼ', 'iidoołjił', 'niʼsézo', 'yiildeeł', 'iishtʼááh', 'adadoołjił', 'hajiłgéésh', 'dasiiltłʼá', 'adeiiltʼááh', 'yiiʼoł', 'yiilchʼil']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['hajiłgéésh', 'ííłtʼaʼ', 'yísʼéél', 'adashoołjid', 'hadasiiʼniiʼ', 'yiyííʼeez', 'dasiikʼaʼ', 'náhshaah', 'jiiłgą́ą́sh', 'shéłhash', 'adasoomááz', 'hadasoołkáál', 'nishoołjool', 'nideiłjooł', 'jítłʼah']\n",
            "Targets: ['hajiłgéésh', 'ííłtʼaʼ', 'yisdááłʼéél', 'adashoołjid', 'dahwiiʼniih', 'yíʼeez', 'deiikʼą́', 'náhshah', 'jiiłgąsh', 'shéłhash', 'adasoomááz', 'hadasoołkaal', 'nishoołjool', 'nideiłjooł', 'jinitłʼah']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yishtʼeesh', 'dazhdíísdééł', 'yíníshtʼaʼ', 'hayííłkáál', 'neitin', 'nidasoozo', 'daałchʼil', 'iidoołtʼah', 'adasoomááz', 'nohneez', 'nidaałjooł', 'naołchiʼ', 'iidoołtʼah', 'yiʼéés', 'nizhdoołjoł']\n",
            "Targets: ['yishtʼeesh', 'yisdádajisʼéél', 'yíłtaʼ', 'hayííłkaal', 'neitin', 'nidaʼsoozo', 'daałchʼil', 'iidoołtʼah', 'adasoomááz', 'nohneez', 'nidaałjooł', 'naołchiʼ', 'iidoołtʼah', 'yiʼéés', 'nizhdoołjoł']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['daałchʼil', 'jighaas', 'dishjol', 'jółtaʼ', 'yiiłtsood', 'wohtłʼóół', 'jiʼéés', 'yiishtłʼoh', 'hojoolghal', 'níshínízhaaʼ', 'ííłjid', 'neitin', 'jidijol', 'njiniih', 'deínółtaʼ']\n",
            "Targets: ['daałchʼil', 'jighaas', 'dinishjool', 'jółtaʼ', 'yiiłtsood', 'wohtłʼóół', 'jiʼéés', 'yiishtʼoh', 'jiisghal', 'níshínízhah', 'ííłjid', 'neitin', 'jidijool', 'njiniih', 'dawóołtaʼ']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['iilʼaad', 'jiiłgąsh', 'jíneez', 'nideiłjooł', 'yítłoh', 'yíníiltʼaʼ', 'wooghaazh', 'dasootłʼóóz', 'hojilghał', 'ndadoołchʼih', 'wohtłʼoł', 'dadiikʼaał', 'daahhaas', 'yiyíítʼeezh', 'dazhdijool']\n",
            "Targets: ['iilʼaʼ', 'jiiłgąsh', 'jíneez', 'nideiłjooł', 'yítłoh', 'wóoltaʼ', 'wooghaz', 'dasootłʼǫ́', 'jiilghaał', 'ndadoołchiił', 'wohtłʼóół', 'dadiikʼą́ą́ł', 'daahhaas', 'yizhtʼéézh', 'dazhdijool']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['dazhdookʼaał', 'shíníchąąd', 'dayiistłʼoh', 'hwiilghal', 'yootłoh', 'hayííłgeezh', 'iimáás', 'yildeeł', 'jíísdéél', 'iiłtʼááh', 'hojilneeʼ', 'yiʼeeł', 'adadoołtʼah', 'yildeeł', 'yisdin']\n",
            "Targets: ['dazhdookʼą́ą́ł', 'yínícha', 'dayiiłtʼoh', 'hiilghal', 'yootłoh', 'hayííłgizh', 'iimáás', 'yildeeł', 'yisdájííłʼéél', 'iiłtʼááh', 'náhojilnih', 'yiʼeeł', 'adadoołtʼah', 'yildeeł', 'yildin']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['nizhdootin', 'deidookʼaał', 'yidzį́į́s', 'deizʼeez', 'adeiiljiid', 'siitłʼǫ́', 'ííłtʼaʼ', 'nizhdoonih', 'yiyíísdéél', 'nizhdoolchʼił', 'nijizhtłizh', 'deiigis', 'hadasiilgeezh', 'nishtłʼah', 'yółtʼaʼ']\n",
            "Targets: ['nizhdootį́į́ł', 'deidookʼą́ą́ł', 'yidzį́į́s', 'daazʼeez', 'adeiiljiid', 'siitłʼǫ́', 'ííłtʼaʼ', 'nizhdoonih', 'yisdáyííłʼéél', 'nizhdoolchiił', 'njizhtłizh', 'deiigis', 'hadashiilgizh', 'nishtłʼah', 'yóltaʼ']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['deínółtʼaʼ', 'haiilgeezh', 'ííłjid', 'iilʼaad', 'yíníʼeez', 'yidoołchʼił', 'yidoołchʼił', 'neidooleeł', 'hwiilghal', 'jiiłtsóód', 'deiitłʼó', 'dasiildin', 'dayółtaʼ', 'dayoolʼaad', 'alzhih']\n",
            "Targets: ['dawóołtaʼ', 'haiilgizh', 'ííłjid', 'iilʼaʼ', 'yíníʼeez', 'yidoołchʼił', 'yidoołchʼił', 'neidooleeł', 'hiilghal', 'jiiłtsóód', 'deiitłʼó', 'deiildin', 'dayííłtaʼ', 'daalʼá', 'alizh']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yidoołhash', 'nizhdoolchih', 'deísdeeł', 'jííʼeez', 'háániiʼ', 'nínáshʼneeh', 'hazhdoołkał', 'yíghaz', 'ayííłʼaad', 'yíníldéél', 'sétłʼǫ́', 'adoomas', 'woołtłʼá', 'ajiłʼá', 'názhaaʼ']\n",
            "Targets: ['yidoołhash', 'nizhdoolchiił', 'yisdádeiłʼeeł', 'jííʼeez', 'hashniih', 'náháshnih', 'hazhdoołkał', 'yíghaz', 'ayííłʼaʼ', 'yíníldéél', 'sétłʼǫ́', 'adoomas', 'woołtłʼá', 'ajiłʼá', 'názhah']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yiitʼoł', 'deiiltłoʼ', 'hajiłkaał', 'nideiłjooł', 'naashchiʼ', 'daʼołzhih', 'yizgis', 'yiltłʼá', 'adajiłʼá', 'yítʼéézh', 'dasiigaoz', 'nijiłjooł', 'daʼjilzhih', 'hadasoołgeezh', 'yiitʼeeł']\n",
            "Targets: ['yiiʼoł', 'deiiltʼoh', 'hajiłkaał', 'nideiłjooł', 'naashchiʼ', 'daʼohłizh', 'yizgis', 'yiltłʼá', 'adajiłʼá', 'shétʼéézh', 'deiigaz', 'nijiłjooł', 'daʼjilizh', 'hadashoołgizh', 'yiiʼeeł']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['jííʼéél', 'daaghaas', 'dasiiltsood', 'dííkʼaał', 'honołghaał', 'hozhdoolneeł', 'nishiitłizh', 'dayííʼéél', 'dazhdiyoołgąsh', 'yishchʼil', 'dayííʼéél', 'ajiłjiid', 'iidoołtʼah', 'hadeeshkał', 'yiiłgąsh']\n",
            "Targets: ['jííʼéél', 'deighaas', 'deiiltsood', 'dííkʼą́ą́ł', 'hołghaał', 'hozhdoolnih', 'nishiitłizh', 'daazʼéél', 'dazhdoołgąsh', 'yishchʼil', 'daazʼéél', 'ajiłjiid', 'iidoołtʼah', 'hadeeshkał', 'yiiłgąsh']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yishʼeeł', 'nisínítin', 'dajisdin', 'hojilghaał', 'yiitʼeez', 'yiłchʼil', 'yíshtʼaʼ', 'deishhash', 'jidoołchʼił', 'dadiichʼééh', 'dajitʼeesh', 'naʼazo', 'yíʼeez', 'yiiłhash', 'yiłchʼil']\n",
            "Targets: ['yishʼeeł', 'nisínítą́', 'dajildin', 'jiilghaał', 'yiitʼeez', 'yiłchʼil', 'yíníshtaʼ', 'deishhash', 'jidoołchʼił', 'dadiichʼééh', 'dajitʼeesh', 'naʼazo', 'yíʼeez', 'yiiłhash', 'yiłchʼil']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['dayiiłhash', 'ííłʼaʼ', 'yiiłgą́ą́zh', 'dahwiilneʼ', 'yiyííłchʼil', 'deiilchʼil', 'adííłjił', 'dadoołhash', 'noohtłʼah', 'naatłíísh', 'yidoołgąsh', 'azhdoołjił', 'ajoolzhąąʼ', 'adaazmááz', 'naahʼaaz']\n",
            "Targets: ['deishhash', 'ííłʼaʼ', 'shéłgąsh', 'dahwiilneʼ', 'yishchʼil', 'deiilchʼil', 'adííłjił', 'dadoołhash', 'nohtłʼah', 'naatłíísh', 'yidoołgąsh', 'azhdoołjił', 'ajizhlizh', 'adaazmááz', 'naʼohso']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yiłchʼil', 'yishtłʼóół', 'nanilchiʼ', 'dadiiltłʼééł', 'nidaahtin', 'ńjíłtʼaʼ', 'dajoodzį́į́z', 'jííʼeez', 'hanáshʼneeh', 'deítłʼeeh', 'hozhneezá', 'jootłoh', 'iilʼaad', 'dahwiilneʼ', 'nidajishjool']\n",
            "Targets: ['yiłchʼil', 'yishtłʼóół', 'nanilchiʼ', 'dadiiltłʼééł', 'nidaahtin', 'ńjółtah', 'dajizdzį́į́z', 'jííʼeez', 'náháshnih', 'deítłʼóóh', 'jiiná', 'jootłoh', 'iilʼaʼ', 'dahwiilneʼ', 'nidajishjool']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yiitʼeesh', 'naazhtłizh', 'yooldéél', 'doohchaał', 'nidazhneezniiʼ', 'yikʼai', 'iishʼá', 'nitłʼó', 'iilzhih', 'dajiʼéés', 'joodzį́į́z', 'sootłʼǫ́', 'shétʼéézh', 'woosdleel', 'ndiidleeł']\n",
            "Targets: ['yiitʼeesh', 'naazhtłizh', 'yooldéél', 'doohchah', 'ndajizniiʼ', 'yiyííkʼą́', 'iishʼá', 'nitłʼó', 'iidlizh', 'dajiʼéés', 'jíídzį́į́z', 'sootłʼǫ́', 'shétʼéézh', 'yisdáoołʼéél', 'ndiidleeł']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['hadajiłgéésh', 'yisdeeł', 'nidaazo', 'naazhtłizh', 'yiishtłʼoh', 'sootłʼǫ́', 'neiitłíísh', 'nishétłizh', 'nidajiztin', 'neitin', 'ndeeshchih', 'hozhdoolneeł', 'neiiljooł', 'deíníilzaas', 'díísʼoł']\n",
            "Targets: ['hadajiłgéésh', 'yisdéíłʼeeł', 'nidaʼazo', 'naazhtłizh', 'yiishtʼoh', 'sootłʼǫ́', 'neiitłíísh', 'nishétłizh', 'nidajiztą́', 'neitin', 'ndeeshchiił', 'hozhdoolnih', 'neiiljooł', 'deíníilzáás', 'yisdádííłʼoł']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['jííʼéél', 'soołdin', 'neiitłíísh', 'yighaas', 'daʼiilzhih', 'nidashoołchiʼ', 'nishéłjool', 'dadiikʼaał', 'yoolʼaad', 'nitłʼah', 'ndazhdoołjoł', 'ííłjid', 'nánízhaah', 'dajííʼéél', 'dazhdiyoołgąsh']\n",
            "Targets: ['jííʼéél', 'wołdin', 'neiitłíísh', 'yighaas', 'daʼiidlizh', 'nidashoołchiʼ', 'nishéłjool', 'dadiikʼą́ą́ł', 'yilʼá', 'nitłʼah', 'ndazhdoołjoł', 'ííłjid', 'nánízhah', 'dajizʼéél', 'dazhdoołgąsh']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['hweeʼná', 'hadeiilkaał', 'deiidzį́į́s', 'yiiltłʼá', 'niʼjizo', 'yidiyoołhash', 'deiigis', 'yiyíísdéél', 'iidoołjił', 'adiiʼmas', 'dadiichʼééh', 'adiiltʼah', 'názhaah', 'daoołtłʼóóʼ', 'hadasiilgeezh']\n",
            "Targets: ['hiniiʼná', 'hadeiilkaał', 'deiidzį́į́s', 'yiiltłʼá', 'niʼjizo', 'yidoołhash', 'deiigis', 'yisdáyííłʼéél', 'iidoołjił', 'adiiʼmas', 'dadiichʼééh', 'adiiltʼah', 'názhah', 'daoołtʼoh', 'hadashiilgizh']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['ndíítį́į́ł', 'dajiltłʼééh', 'íímááz', 'yítłʼil', 'daahchaa', 'ajííłtʼaʼ', 'dajisʼaad', 'ílzhí', 'jizhtʼeezh', 'yizgis', 'haidoołkał', 'neiłjooł', 'haníná', 'yítłoh', 'yisdzį́į́s']\n",
            "Targets: ['ndíítį́į́ł', 'dajiltłʼééh', 'íímááz', 'yishtłʼił', 'daahcha', 'ajííłtʼaʼ', 'dajilʼá', 'ílizh', 'jizhtʼéézh', 'yizgis', 'haidoołkał', 'neiłjooł', 'hiná', 'yítłoh', 'yisdzį́į́s']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['dinishjol', 'iidoołtʼah', 'yiishtłʼoh', 'daniitłʼah', 'woołhash', 'diilzéél', 'dayółtʼaʼ', 'yildeeł', 'joosas', 'dajiʼeeł', 'yiildin', 'wootłʼóód', 'yizhchah', 'yiyiiłtsóód', 'yighaas']\n",
            "Targets: ['dinishjool', 'iidoołtʼah', 'yiishtʼoh', 'daniitłʼah', 'woołhash', 'yisdéiilʼéél', 'dayííłtaʼ', 'yildeeł', 'joosas', 'dajiʼeeł', 'yiildin', 'sootłʼǫ́', 'yícha', 'yiyiiłtsóód', 'yighaas']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yíníʼéél', 'shootʼéézh', 'nídaazhaah', 'iiłʼá', 'adeidoołtʼah', 'yiltłʼá', 'ségis', 'deidzį́į́s', 'daaltłʼééh', 'ajííłʼaad', 'háíníłgeezh', 'nishishchiʼ', 'ayííłʼaad', 'ndoohtłish', 'yiigaas']\n",
            "Targets: ['yíníʼéél', 'shootʼéézh', 'nídaazhah', 'iiłʼá', 'adeidoołtʼah', 'yiltłʼá', 'ségis', 'deidzį́į́s', 'daaltłʼééh', 'ajííłʼaʼ', 'háíníłgizh', 'nishishchiʼ', 'ayííłʼaʼ', 'ndoohtłish', 'yiigaas']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['hadaazniiʼ', 'adaałtʼááh', 'jíítsʼiid', 'daahtłʼó', 'haałgéésh', 'neistin', 'daahhaas', 'ndeidootin', 'adeiłjiid', 'neiiljooł', 'deíníitʼeeł', 'dazhdookʼaał', 'yiitłʼil', 'jiiłtłoh', 'ségis']\n",
            "Targets: ['dahaniih', 'adaałtʼááh', 'jitsʼííd', 'daahtłʼó', 'haałgéésh', 'neiztą́', 'daahhaas', 'ndeidootį́į́ł', 'adeiłjiid', 'neiiljooł', 'deíníiʼeeł', 'dazhdookʼą́ą́ł', 'yiitłʼił', 'jiiłtʼoh', 'ségis']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['niséniiʼ', 'ílzhih', 'neilé', 'yiisgis', 'yiyiiłtłʼoh', 'yiitłoh', 'nizhdootih', 'yiyíítʼéézh', 'yighaas', 'wołdéél', 'adadoołjił', 'adadiiljił', 'nijishjool', 'diichʼééh', 'nidaʼsiidzo']\n",
            "Targets: ['niséniiʼ', 'ílizh', 'neilé', 'yiisgis', 'yiyiiłtʼoh', 'yiitłoh', 'nizhdootį́į́ł', 'yizhtʼéézh', 'yighaas', 'woołdéél', 'adadoołjił', 'adadiiljił', 'nijishjool', 'diichʼééh', 'nidaʼsiidzo']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['yidoosdił', 'nijitin', 'dahojisghal', 'jinitłʼah', 'hozhdoolneeł', 'jíldin', 'diishchʼééh', 'ndazhdoonih', 'daahkʼá', 'dadoohchʼééh', 'diiltłʼééł', 'hadaooniiʼ', 'nítłʼah', 'dinishjol', 'nisiiʼniiʼ']\n",
            "Targets: ['yisdéidoołʼoł', 'nijitin', 'dajiisghal', 'jinitłʼah', 'hozhdoolnih', 'jildin', 'diishchʼééh', 'ndazhdoonih', 'daahkʼá', 'dadoohchʼééh', 'diiltłʼééł', 'dahohniih', 'nítłʼah', 'dinishjool', 'nisiiʼniiʼ']\n",
            "___________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:651: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: ['ajiłjiid', 'naaso', 'jiigis', 'nisiiʼnih', 'yiigis', 'dajiiłtsóód', 'adeiilʼá', 'hadajishgeezh', 'oolzhih', 'deiigaas', 'jółtʼaʼ', 'ndaahłé', 'daoołʼaaʼ', 'dayiiłgąsh', 'woołʼaaʼ']\n",
            "Targets: ['ajiłjiid', 'naʼaso', 'jiigis', 'nisiiʼniiʼ', 'yiigis', 'dajiiłtsóód', 'adeiilʼá', 'hadajishgizh', 'azhlizh', 'deiigaas', 'jółtaʼ', 'nidaahłé', 'daałʼá', 'dayiiłgąsh', 'wołʼá']\n",
            "___________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "TRAINING\n",
        "\"\"\"\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "dataset = {'train': CustomDataset(data['train'], tokenizer),\n",
        "           'dev': CustomDataset(data['dev'], tokenizer)}\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    max_steps=20000,\n",
        "    per_device_train_batch_size=400,\n",
        "    learning_rate = 0.001,\n",
        "    lr_scheduler_type='inverse_sqrt',\n",
        "    warmup_steps=4000,\n",
        "    adam_beta2=0.98,\n",
        "    label_smoothing_factor=0.1,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=400,\n",
        "    eval_delay=400,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=400,\n",
        "    report_to='none',\n",
        "    predict_with_generate=True,\n",
        "    metric_for_best_model='exact_match',\n",
        "    save_total_limit=1,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=400,\n",
        "    output_dir='baseline_0.2',\n",
        "    overwrite_output_dir=True,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['dev'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZCTkEBfH__-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e40c96-02cd-4394-856a-69cb2ef20b80"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'download_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-82315e2f3cc6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/sigmorphon/2023InflectionST/main/part1/data/nav.tst'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'download_data' is not defined"
          ]
        }
      ],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "\"\"\"\n",
        "EVALUATION\n",
        "\"\"\"\n",
        "\n",
        "test_data_path = 'https://raw.githubusercontent.com/sigmorphon/2023InflectionST/main/part1/data/nav.tst'\n",
        "test_data = download_data(test_data_path)\n",
        "test_dataset = CustomDataset(preprocess_data(test_data), tokenizer)\n",
        "result = trainer.evaluate(test_dataset)\n",
        "test_accuracy_wu = result['eval_exact_match']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Nbr53pLIs2"
      },
      "source": [
        "### Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20PvBNLOLIs2",
        "outputId": "befec8ff-6d79-4acb-e06c-f722abebb6fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.488\n"
          ]
        }
      ],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "print('Test accuracy:', test_accuracy_wu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx31P1GOLIs3"
      },
      "source": [
        "### Report\n",
        "[Describe your approach in up to 300 words here.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ9QM7u3LIs3"
      },
      "source": [
        "# Task 2\n",
        "Build a better morphological inflector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f04Vp_VtLIs3"
      },
      "outputs": [],
      "source": [
        "# your code goes here - make sure the final test accuracy is saved as test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjcrL7XHLIs3"
      },
      "source": [
        "### Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRYYuS8LLIs4"
      },
      "outputs": [],
      "source": [
        "###DO NOT CHANGE THIS CELL###\n",
        "print('Test accuracy:', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr9oE9wGLIs4"
      },
      "source": [
        "### Report\n",
        "[Describe your approach in up to 300 words here.]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
