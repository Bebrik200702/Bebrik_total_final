{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"21671848a51a401891400b8ed277cc73":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d1f5be69dfb4366b1f33bce9cd4582f","IPY_MODEL_c70e2c3d1d5e44fa95d134722a32f2c7","IPY_MODEL_b70399895f5040b8b9034ec4b0f05cf8"],"layout":"IPY_MODEL_aadde11616ab4b5381b23ec4166bc953"}},"0d1f5be69dfb4366b1f33bce9cd4582f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a55225dd650645fb8661defeac536d99","placeholder":"​","style":"IPY_MODEL_0f93517585404413873d52c2aa14d1bc","value":"model.safetensors: 100%"}},"c70e2c3d1d5e44fa95d134722a32f2c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f32d5d3c0d4044738f77dd523967d394","max":672247920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4841d2cf0964659816b2ef351beb9b7","value":672247920}},"b70399895f5040b8b9034ec4b0f05cf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a472485af7240cca9401f78cb75ac1d","placeholder":"​","style":"IPY_MODEL_c10e1a21691e4c01affe5987ac606b68","value":" 672M/672M [00:04&lt;00:00, 175MB/s]"}},"aadde11616ab4b5381b23ec4166bc953":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a55225dd650645fb8661defeac536d99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f93517585404413873d52c2aa14d1bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f32d5d3c0d4044738f77dd523967d394":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4841d2cf0964659816b2ef351beb9b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a472485af7240cca9401f78cb75ac1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c10e1a21691e4c01affe5987ac606b68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c11cd03f8b474c2ea7a46d193d43ac69":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4162b9c045cb4139894a6ca639f6ecf3","IPY_MODEL_b959a8d953c145568f175586c3607a19","IPY_MODEL_1b24362ffdbe4f5ca8feccfaa7008453"],"layout":"IPY_MODEL_bd76fccba04b4d86a2776a3cf0671714"}},"4162b9c045cb4139894a6ca639f6ecf3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57391a3802c445e18ba3f20ff6255bb6","placeholder":"​","style":"IPY_MODEL_c38538a02cdc4b71addaa31a8f49c2d5","value":"Map: 100%"}},"b959a8d953c145568f175586c3607a19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdefbe6b851f49d88b6171bb59a6772b","max":218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c52a681036654cc396dcdde4df62d4bc","value":218}},"1b24362ffdbe4f5ca8feccfaa7008453":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f66105aca2e4df392180f17802f036b","placeholder":"​","style":"IPY_MODEL_0c5c741374eb4eb18d8d623751dc410e","value":" 218/218 [00:00&lt;00:00, 1507.79 examples/s]"}},"bd76fccba04b4d86a2776a3cf0671714":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57391a3802c445e18ba3f20ff6255bb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c38538a02cdc4b71addaa31a8f49c2d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdefbe6b851f49d88b6171bb59a6772b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c52a681036654cc396dcdde4df62d4bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f66105aca2e4df392180f17802f036b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c5c741374eb4eb18d8d623751dc410e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":371821,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":307764,"modelId":328217}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Help BOBAI: Classify an unknown language","metadata":{"id":"bqAFpqJlnt77"}},{"cell_type":"code","source":"from google.colab import userdata\n\n# read_access_token = userdata.get('hf_read')\n# write_access_token = userdata.get('hf_write')\nread_access_token = \"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\"\nwrite_access_token = \"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\"","metadata":{"id":"sV85hgL0yxn0","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:16:14.535850Z","iopub.execute_input":"2025-05-04T08:16:14.536378Z","iopub.status.idle":"2025-05-04T08:16:14.562780Z","shell.execute_reply.started":"2025-05-04T08:16:14.536348Z","shell.execute_reply":"2025-05-04T08:16:14.562246Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Data","metadata":{"id":"EFTIQ9tDMqsE"}},{"cell_type":"code","source":"!pip install datasets evaluate tokenizers -q -U","metadata":{"id":"F8xoXDoIPSEG","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:16:17.555980Z","iopub.execute_input":"2025-05-04T08:16:17.556659Z","iopub.status.idle":"2025-05-04T08:16:24.398733Z","shell.execute_reply.started":"2025-05-04T08:16:17.556633Z","shell.execute_reply":"2025-05-04T08:16:24.398029Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# load the data\n\nfrom datasets import load_dataset, Dataset, DatasetDict\n\nclassification_dataset = load_dataset('InternationalOlympiadAI/NLP_problem', token=\"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\")\nraw_text = load_dataset('InternationalOlympiadAI/NLP_problem_raw', token=\"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\")","metadata":{"id":"CpgcmI2NMLyF","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:16:24.399775Z","iopub.execute_input":"2025-05-04T08:16:24.399982Z","iopub.status.idle":"2025-05-04T08:16:30.549311Z","shell.execute_reply.started":"2025-05-04T08:16:24.399963Z","shell.execute_reply":"2025-05-04T08:16:30.548739Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/397 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcd9216e613f48c08ba4c1af7c9a71d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/126k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264c9e7adb0a45cdacaa52c46e05bad2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev-00000-of-00001.parquet:   0%|          | 0.00/19.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75639749be0f41e899aea06df142bbbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1524 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45fc13daddb640d9b057cc5758b8584d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/218 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68e5d2409c3f47e6a0b1cd938d12533e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bcd04956a854f83b6018912b444f15c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/90.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62ad8f960c03472fae579b6ba648f096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/611245 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64736c3e4135484384072cefc72f7e9a"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## MLM","metadata":{"id":"ajgVXh2cQuF_"}},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,\n    AutoModelForMaskedLM,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n    BertConfig,\n    BertTokenizer,\n    PreTrainedTokenizerFast,\n    BertTokenizerFast\n)\nimport os\nimport torch\nimport torch.nn.functional as F\nimport evaluate\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordPiece\nfrom tokenizers.trainers import WordPieceTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence","metadata":{"id":"y4zEpLL6QvcA","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:16:39.119162Z","iopub.execute_input":"2025-05-04T08:16:39.119871Z","iopub.status.idle":"2025-05-04T08:17:07.568970Z","shell.execute_reply.started":"2025-05-04T08:16:39.119845Z","shell.execute_reply":"2025-05-04T08:17:07.568170Z"}},"outputs":[{"name":"stderr","text":"2025-05-04 08:16:49.221983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746346609.484290     169 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746346609.558440     169 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"tokenizer_dataset = raw_text['train']['text']\n\n#save with several strings\nwith open('corpus_spm_fixed.txt', 'w', encoding='utf-8') as f:\n    f.write('\\n'.join(tokenizer_dataset))\n\n#save in one line\n# unique_symbols = list(set(tokenizer_dataset))\n\n# f = open('corpus.txt', 'w')\n# f.write(tokenizer_dataset)\n# f.close()","metadata":{"id":"t2eB4ggyWJOE","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:07.569912Z","iopub.execute_input":"2025-05-04T08:17:07.570446Z","iopub.status.idle":"2025-05-04T08:17:10.489292Z","shell.execute_reply.started":"2025-05-04T08:17:07.570424Z","shell.execute_reply":"2025-05-04T08:17:10.488663Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n\ntokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\ntokenizer.pre_tokenizer = Whitespace()\n\ntrainer = WordPieceTrainer(\n    vocab_size=4096,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\ntokenizer.train(\n    files=[\"/kaggle/working/corpus_spm_fixed.txt\"],\n    trainer=trainer\n)\n\ntokenizer.save(\"tokenizer-wp.json\")","metadata":{"id":"agAoJasaSXyb","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:10.489910Z","iopub.execute_input":"2025-05-04T08:17:10.490101Z","iopub.status.idle":"2025-05-04T08:17:18.982250Z","shell.execute_reply.started":"2025-05-04T08:17:10.490084Z","shell.execute_reply":"2025-05-04T08:17:18.981670Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#BYTELEVEL \nif False:\n\n    os.makedirs(\"tokenizer\", exist_ok=True)\n\n    tokenizer = ByteLevelBPETokenizer()\n    \n    tokenizer.train(\n        files=[\"/kaggle/working/corpus_spm_fixed.txt\"],\n        vocab_size=4096,\n        min_frequency=2,\n        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n    )\n    \n    tokenizer.save_model(\"tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:18.983542Z","iopub.execute_input":"2025-05-04T08:17:18.983944Z","iopub.status.idle":"2025-05-04T08:17:18.987219Z","shell.execute_reply.started":"2025-05-04T08:17:18.983924Z","shell.execute_reply":"2025-05-04T08:17:18.986722Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#SENTENCEPIECE\nif False:\n    !pip install sentencepiece -q\n    import sentencepiece as spm\n    \n    spm.SentencePieceTrainer.train(\n        input='/kaggle/working/corpus_spm_fixed.txt',\n        model_prefix='spm_tokenizer',\n        vocab_size=4096,\n        model_type='bpe',\n        character_coverage=1.0,  \n        pad_id=0,\n        unk_id=1,\n        bos_id=2,\n        eos_id=3,\n        user_defined_symbols=[\"<mask>\"]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:18.987732Z","iopub.execute_input":"2025-05-04T08:17:18.987904Z","iopub.status.idle":"2025-05-04T08:17:19.001936Z","shell.execute_reply.started":"2025-05-04T08:17:18.987885Z","shell.execute_reply":"2025-05-04T08:17:19.001467Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"device = torch.device('cuda:0')\nmodel_name = \"google-bert/bert-base-multilingual-uncased\"\nmodel_cfg = BertConfig.from_pretrained(model_name)","metadata":{"id":"AsiWSqucQxJ7","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:19.002484Z","iopub.execute_input":"2025-05-04T08:17:19.002655Z","iopub.status.idle":"2025-05-04T08:17:19.121165Z","shell.execute_reply.started":"2025-05-04T08:17:19.002641Z","shell.execute_reply":"2025-05-04T08:17:19.120678Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"118a74c56d5e4be8bff1dbf7e656cbae"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"tokenizer = BertTokenizerFast(\n    tokenizer_file='/kaggle/working/tokenizer-wp.json',\n    unk_token=\"[UNK]\",\n    sep_token=\"[SEP]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    mask_token=\"[MASK]\",\n    model_max_lengths=model_cfg.max_position_embeddings\n)\ntokenizer.save_pretrained(\"./bert_tokenizer\")\ntokenizer = BertTokenizerFast.from_pretrained(\"/kaggle/working/bert_tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:19.121728Z","iopub.execute_input":"2025-05-04T08:17:19.122058Z","iopub.status.idle":"2025-05-04T08:17:19.136974Z","shell.execute_reply.started":"2025-05-04T08:17:19.122042Z","shell.execute_reply":"2025-05-04T08:17:19.136420Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model_cfg.vocab_size = tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:19.137574Z","iopub.execute_input":"2025-05-04T08:17:19.137748Z","iopub.status.idle":"2025-05-04T08:17:19.140614Z","shell.execute_reply.started":"2025-05-04T08:17:19.137735Z","shell.execute_reply":"2025-05-04T08:17:19.139993Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model_cfg.vocab_size = tokenizer.vocab_size\nmodel = AutoModelForMaskedLM.from_config(config=model_cfg)","metadata":{"id":"CBY2OhEWU0aI","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:19.141158Z","iopub.execute_input":"2025-05-04T08:17:19.141553Z","iopub.status.idle":"2025-05-04T08:17:20.625471Z","shell.execute_reply.started":"2025-05-04T08:17:19.141536Z","shell.execute_reply":"2025-05-04T08:17:20.624851Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#preparing the data\nfrom transformers import DataCollatorForLanguageModeling\n\n#tokenizing data\ndef tokenize_map(text):\n    return tokenizer(text['text'], truncation=True, return_attention_mask=True)\n\nraw_text_for_mlm = raw_text.map(tokenize_map, batched=True, remove_columns=['text'])\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True,\n    mlm_probability=0.175\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#group texts\nblock_size = 128\n\ndef group_map(batch):\n    batch = {k : sum(v, []) for k,v in batch.items()}\n    blocks_num = len(batch['input_ids']) // block_size\n    return {\n        k: torch.stack([torch.tensor(v[i*block_size:(i+1) * block_size], dtype=torch.long) for i in range(blocks_num)])\n        for k, v in batch.items()\n    }\nmlm_groups_dataset = raw_text_for_mlm.map(group_map, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlm_groups_dataset[\"train\"].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlm_groups_dataset = mlm_groups_dataset['train'].train_test_split(test_size=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./bert-mlm\",\n    overwrite_output_dir=True,\n    num_train_epochs=10,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=128,\n    save_steps=1000,\n    save_total_limit=5,\n    logging_steps=500,\n    eval_strategy=\"steps\",\n    eval_steps=250,\n    # learning_rate=7e-5,\n    # weight_decay=1e-2,\n    report_to=\"none\",\n    torch_compile=True,\n    # load_best_model_at_end =True\n    bf16=True,\n    dataloader_num_workers=4,\n    dataloader_drop_last=True\n    # lr_scheduler_type=\"cosine\",\n    # warmup_steps=800,\n)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2, betas=(0.9, 0.95))\nimport transformers\nlr_scheduler = transformers.get_cosine_schedule_with_warmup(\n    optimizer  = optimizer ,\n    num_warmup_steps = 500,\n    num_training_steps = 5640,\n    num_cycles=1.0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del model\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['TOKENIZERS_PARALLELISM'] = 'true'\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=mlm_groups_dataset['train'],\n    eval_dataset=mlm_groups_dataset['test'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    optimizers=(optimizer, lr_scheduler)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'best_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/input/mlm_ioai_2024_home_nlp/pytorch/default/1/best_model.pth\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:20.626956Z","iopub.execute_input":"2025-05-04T08:17:20.627162Z","iopub.status.idle":"2025-05-04T08:17:23.676947Z","shell.execute_reply.started":"2025-05-04T08:17:20.627147Z","shell.execute_reply":"2025-05-04T08:17:23.676186Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_169/2468296380.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/input/mlm_ioai_2024_home_nlp/pytorch/default/1/best_model.pth\"))\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"model.save_pretrained('bert-mlm-best')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:23.677699Z","iopub.execute_input":"2025-05-04T08:17:23.677913Z","iopub.status.idle":"2025-05-04T08:17:24.114943Z","shell.execute_reply.started":"2025-05-04T08:17:23.677896Z","shell.execute_reply":"2025-05-04T08:17:24.114125Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Classification","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nclass NewLanguageClassifier(nn.Module):\n    def __init__(self, encoder, num_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.num_classes = num_classes\n        enc_hid_sz = encoder.config.hidden_size\n        self.fc_head = nn.Sequential(\n            nn.Linear(enc_hid_sz, enc_hid_sz * 4),\n            nn.SiLU(),\n            nn.Linear(enc_hid_sz * 4, num_classes)\n        )\n        # self.fc_head = nn.Linear(enc_hid_sz, num_classes)\n        self.dp = nn.Dropout(p=0.1)\n        self.loss_fn = nn.CrossEntropyLoss()\n        \n    def forward(self, attention_mask, input_ids, labels=None):\n        x = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        # print(x.pooler_output)\n        x = x.last_hidden_state[:, 0, :]\n        out = self.fc_head(self.dp(x))\n        out_dict = {'logits' : out}\n        if labels is not None:\n            loss = self.loss_fn(out, labels)\n            out_dict['loss'] = loss\n        return out_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:34:14.624028Z","iopub.execute_input":"2025-05-04T09:34:14.624768Z","iopub.status.idle":"2025-05-04T09:34:14.630185Z","shell.execute_reply.started":"2025-05-04T09:34:14.624743Z","shell.execute_reply":"2025-05-04T09:34:14.629664Z"}},"outputs":[],"execution_count":248},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification,AutoModel\n\nencoder = AutoModel.from_pretrained(\n    \"/kaggle/working/bert-mlm-best\",\n    # num_labels=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:31:08.750097Z","iopub.execute_input":"2025-05-04T09:31:08.750432Z","iopub.status.idle":"2025-05-04T09:31:08.857100Z","shell.execute_reply.started":"2025-05-04T09:31:08.750408Z","shell.execute_reply":"2025-05-04T09:31:08.856384Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertModel were not initialized from the model checkpoint at /kaggle/working/bert-mlm-best and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":238},{"cell_type":"code","source":"cl_model = NewLanguageClassifier(encoder=encoder, num_classes=5).to('cuda:1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:34:16.414330Z","iopub.execute_input":"2025-05-04T09:34:16.414582Z","iopub.status.idle":"2025-05-04T09:34:16.475210Z","shell.execute_reply.started":"2025-05-04T09:34:16.414563Z","shell.execute_reply":"2025-05-04T09:34:16.474665Z"}},"outputs":[],"execution_count":249},{"cell_type":"code","source":"cl_model(\n    **{k:v.to('cuda:1') for k,v in data_collator(tokenized_data[\"dev\"].select_columns(['input_ids', 'attention_mask', 'labels'])[:218]).items()}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:34:17.719042Z","iopub.execute_input":"2025-05-04T09:34:17.719624Z","iopub.status.idle":"2025-05-04T09:34:17.816059Z","shell.execute_reply.started":"2025-05-04T09:34:17.719594Z","shell.execute_reply":"2025-05-04T09:34:17.815541Z"}},"outputs":[{"execution_count":250,"output_type":"execute_result","data":{"text/plain":"{'logits': tensor([[-0.1916,  0.0716,  0.3154,  0.2008,  0.0653],\n         [-0.1883,  0.1486, -0.1665,  0.1056,  0.0053],\n         [-0.1834,  0.2427,  0.1081,  0.1730, -0.0287],\n         ...,\n         [ 0.0245,  0.0680,  0.0048,  0.1232,  0.1937],\n         [-0.0410,  0.1978,  0.0848,  0.1287, -0.0870],\n         [ 0.1189,  0.0040, -0.1596,  0.1086,  0.2956]], device='cuda:1',\n        grad_fn=<AddmmBackward0>),\n 'loss': tensor(1.6183, device='cuda:1', grad_fn=<NllLossBackward0>)}"},"metadata":{}}],"execution_count":250},{"cell_type":"code","source":"# temp = tokenized_data['train'].select_columns(['input_ids', 'attention_mask', 'label']).rename_columns({'label' : 'labels'})\n# temp.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n# cl_model(**{k:v.unsqueeze(0).to('cuda') for k,v in temp[0].items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:21:18.018359Z","iopub.execute_input":"2025-05-04T08:21:18.018904Z","iopub.status.idle":"2025-05-04T08:21:18.024994Z","shell.execute_reply.started":"2025-05-04T08:21:18.018881Z","shell.execute_reply":"2025-05-04T08:21:18.024509Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# Baseline","metadata":{"id":"cv9MBElmMs6G"}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)\n\ntokenized_data = classification_dataset.map(preprocess_function, batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"id":"tSOWNJRKNoWM","outputId":"33155489-302d-4be9-d9a8-f5bd1be2a2fd","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:17:38.481055Z","iopub.execute_input":"2025-05-04T08:17:38.481760Z","iopub.status.idle":"2025-05-04T08:17:38.571655Z","shell.execute_reply.started":"2025-05-04T08:17:38.481737Z","shell.execute_reply":"2025-05-04T08:17:38.571068Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1524 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1741b0a3ab1945cbb9f2afa7612805bf"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/218 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f36bc85e68414393f54ac6d41e453e"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"ex = tokenized_data['train']['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:54:31.980179Z","iopub.execute_input":"2025-05-04T08:54:31.980826Z","iopub.status.idle":"2025-05-04T08:54:31.987692Z","shell.execute_reply.started":"2025-05-04T08:54:31.980803Z","shell.execute_reply":"2025-05-04T08:54:31.987098Z"}},"outputs":[],"execution_count":141},{"cell_type":"code","source":"ex[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:56:29.478013Z","iopub.execute_input":"2025-05-04T08:56:29.478537Z","iopub.status.idle":"2025-05-04T08:56:29.482109Z","shell.execute_reply.started":"2025-05-04T08:56:29.478513Z","shell.execute_reply":"2025-05-04T08:56:29.481589Z"}},"outputs":[{"execution_count":153,"output_type":"execute_result","data":{"text/plain":"'झ𑁣झच𑀪𑀢𑀟 𑀣च 𑀠न𑀞𑁦 ण𑀢 𑀟च 𑀫चझ𑁣 𑀠च𑀟 𑀲𑁦पन𑀪 च ब𑁣𑀠ढ𑁦 𑀣च ढचनत𑀫𑀢 ष ब𑀱च𑀠𑀟च 𑀢𑀟न𑀱च णच𑀫चणच'"},"metadata":{}}],"execution_count":153},{"cell_type":"code","source":"tokenizer.decode(tokenizer(ex[0])['input_ids'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:56:22.697931Z","iopub.execute_input":"2025-05-04T08:56:22.698426Z","iopub.status.idle":"2025-05-04T08:56:22.702824Z","shell.execute_reply.started":"2025-05-04T08:56:22.698401Z","shell.execute_reply":"2025-05-04T08:56:22.702308Z"}},"outputs":[{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"'झ 𑁣 झच𑀪𑀢𑀟 𑀣च 𑀠न𑀞 ##𑁦 ण𑀢 𑀟च 𑀫चझ 𑁣 𑀠च𑀟 𑀲𑁦पन𑀪 च ब 𑁣 𑀠ढ ##𑁦 𑀣च ढचनत𑀫𑀢 ष ब𑀱च𑀠𑀟च 𑀢𑀟 ##न𑀱च णच ##𑀫च ##णच'"},"metadata":{}}],"execution_count":152},{"cell_type":"code","source":"list(zip(tokenizer.decode(tokenizer(ex[0])['input_ids']), tokenizer(ex[0])['input_ids']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:55:47.492462Z","iopub.execute_input":"2025-05-04T08:55:47.493105Z","iopub.status.idle":"2025-05-04T08:55:47.497880Z","shell.execute_reply.started":"2025-05-04T08:55:47.493080Z","shell.execute_reply":"2025-05-04T08:55:47.497383Z"}},"outputs":[{"execution_count":151,"output_type":"execute_result","data":{"text/plain":"[('झ', 13),\n (' ', 69),\n ('𑁣', 1282),\n (' ', 144),\n ('झ', 1682),\n ('च', 73),\n ('𑀪', 213),\n ('𑀢', 148),\n ('𑀟', 885),\n (' ', 69),\n ('𑀣', 361),\n ('च', 2939),\n (' ', 10),\n ('𑀠', 27),\n ('न', 69),\n ('𑀞', 2091),\n (' ', 73),\n ('#', 144),\n ('#', 3778),\n ('𑁦', 35),\n (' ', 490),\n ('ण', 229),\n ('𑀢', 193),\n (' ', 152),\n ('𑀟', 214),\n ('च', 153)]"},"metadata":{}}],"execution_count":151},{"cell_type":"code","source":"cl_model(\n    **{k:v.to('cuda') for k,v in data_collator(tokenized_data[\"dev\"].select_columns(['input_ids', 'attention_mask', 'labels'])[:218]).items()}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:27:34.339408Z","iopub.execute_input":"2025-05-04T09:27:34.340054Z","iopub.status.idle":"2025-05-04T09:27:34.461759Z","shell.execute_reply.started":"2025-05-04T09:27:34.340032Z","shell.execute_reply":"2025-05-04T09:27:34.461044Z"}},"outputs":[{"name":"stdout","text":"SequenceClassifierOutput(loss=None, logits=tensor([[-0.7955,  0.0556,  4.7803, -1.4554, -1.6502],\n        [ 3.8257,  0.5513, -2.5556, -1.6525, -0.3658],\n        [-0.9515, -1.3806,  4.7565,  0.1004, -2.1349],\n        ...,\n        [-0.3437, -0.6439, -2.4224, -0.6980,  3.6845],\n        [-1.4629, -0.8148,  5.1302, -1.0329, -1.3640],\n        [-0.8302, -0.5385, -0.6906, -1.8901,  4.7112]], device='cuda:0',\n       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_169/3546122892.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cl_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m218\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_169/1049792745.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, attention_mask, input_ids, labels)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'logits'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'last_hidden_states'"],"ename":"AttributeError","evalue":"'SequenceClassifierOutput' object has no attribute 'last_hidden_states'","output_type":"error"}],"execution_count":226},{"cell_type":"code","source":"# define the evaluation metric\nimport evaluate\nimport numpy as np\n\nf1 = evaluate.load(\"f1\")\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=-1)\n    return f1.compute(predictions=predictions, references=labels, average='macro')","metadata":{"id":"nN64VrhSNuYA","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:45:37.298614Z","iopub.execute_input":"2025-05-04T08:45:37.299253Z","iopub.status.idle":"2025-05-04T08:45:37.601455Z","shell.execute_reply.started":"2025-05-04T08:45:37.299231Z","shell.execute_reply":"2025-05-04T08:45:37.600890Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"tokenized_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntokenized_data = tokenized_data.rename_columns({'label' : 'labels'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T08:35:06.476796Z","iopub.execute_input":"2025-05-04T08:35:06.477044Z","iopub.status.idle":"2025-05-04T08:35:06.483650Z","shell.execute_reply.started":"2025-05-04T08:35:06.477026Z","shell.execute_reply":"2025-05-04T08:35:06.483122Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"# define the model and the training configuration\nfrom transformers import AutoModelForSequenceClassification\n# model = AutoModelForSequenceClassification.from_pretrained(\n#     \"google-bert/bert-base-multilingual-uncased\", num_labels=5\n# )\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_classifier\",\n    overwrite_output_dir=True,\n    learning_rate=5e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=20,\n    weight_decay=1e-5,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    eval_steps=25,\n    save_total_limit=5,\n    logging_steps=25,\n    metric_for_best_model='f1',\n    load_best_model_at_end=True,\n    torch_compile=True,\n    bf16=True,\n    report_to='none',\n    lr_scheduler_type='cosine',\n    warmup_steps=40,\n)\n\ntrainer = Trainer(\n    model=cl_model.to('cuda:1'),\n    args=training_args,\n    train_dataset=tokenized_data[\"train\"].select_columns(['input_ids', 'attention_mask', 'labels']),\n    eval_dataset=tokenized_data[\"dev\"].select_columns(['input_ids', 'attention_mask', 'labels']),\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"id":"hCojWe8hOgRv","outputId":"69375388-0bb5-4b7e-bdea-6b2023703ae7","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:35:56.098508Z","iopub.execute_input":"2025-05-04T09:35:56.098787Z","iopub.status.idle":"2025-05-04T09:35:56.171461Z","shell.execute_reply.started":"2025-05-04T09:35:56.098767Z","shell.execute_reply":"2025-05-04T09:35:56.170872Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_169/910622116.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":255},{"cell_type":"code","source":"numel_1 = 0\nfor p in model.parameters():\n    numel_1 += p.numel()\nnumel_2 = 0\nfor p in cl_model.parameters():\n    numel_2 += p.numel()\nprint(numel_1 / 10e6, numel_2/10e6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:34:26.846154Z","iopub.execute_input":"2025-05-04T09:34:26.846431Z","iopub.status.idle":"2025-05-04T09:34:26.851919Z","shell.execute_reply.started":"2025-05-04T09:34:26.846413Z","shell.execute_reply":"2025-05-04T09:34:26.851283Z"}},"outputs":[{"name":"stdout","text":"16.7360261 9.1564805\n","output_type":"stream"}],"execution_count":252},{"cell_type":"code","source":"# del model\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:00:15.248262Z","iopub.execute_input":"2025-05-04T09:00:15.248538Z","iopub.status.idle":"2025-05-04T09:00:15.365738Z","shell.execute_reply.started":"2025-05-04T09:00:15.248518Z","shell.execute_reply":"2025-05-04T09:00:15.365166Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"# execute the model training\ntrainer.train()","metadata":{"id":"VTJ-w6BnosYy","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:35:59.083582Z","iopub.execute_input":"2025-05-04T09:35:59.084218Z","iopub.status.idle":"2025-05-04T09:37:17.384277Z","shell.execute_reply.started":"2025-05-04T09:35:59.084178Z","shell.execute_reply":"2025-05-04T09:37:17.383639Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [240/240 01:17, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>0.000200</td>\n      <td>1.567577</td>\n      <td>0.818256</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.000000</td>\n      <td>1.768559</td>\n      <td>0.825403</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.003600</td>\n      <td>1.931158</td>\n      <td>0.835291</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.034600</td>\n      <td>1.817553</td>\n      <td>0.799312</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.008100</td>\n      <td>1.773359</td>\n      <td>0.805768</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.010200</td>\n      <td>1.479942</td>\n      <td>0.819003</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.000400</td>\n      <td>1.419705</td>\n      <td>0.822146</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.000300</td>\n      <td>1.516334</td>\n      <td>0.829203</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.000500</td>\n      <td>1.521292</td>\n      <td>0.822577</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":256,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=240, training_loss=0.006037850585804942, metrics={'train_runtime': 77.7485, 'train_samples_per_second': 392.033, 'train_steps_per_second': 3.087, 'total_flos': 0.0, 'train_loss': 0.006037850585804942, 'epoch': 20.0})"},"metadata":{}}],"execution_count":256},{"cell_type":"code","source":"1/0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"oSYydzx9NAGU"}},{"cell_type":"code","source":"# run the trained model on a dev/test split\ndata_split = \"dev\"\neval_out = trainer.predict(tokenized_data[data_split])\npredictions = eval_out.predictions.argmax(1)\nlabels = eval_out.label_ids\ndev_f1 = f1.compute(predictions=predictions, references=labels, average='macro')","metadata":{"id":"jaa80VhiNBG_","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:18:16.859414Z","iopub.execute_input":"2025-05-04T09:18:16.860161Z","iopub.status.idle":"2025-05-04T09:18:17.120763Z","shell.execute_reply.started":"2025-05-04T09:18:16.860130Z","shell.execute_reply":"2025-05-04T09:18:17.120017Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":198},{"cell_type":"code","source":"dev_f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:18:22.422542Z","iopub.execute_input":"2025-05-04T09:18:22.423263Z","iopub.status.idle":"2025-05-04T09:18:22.426987Z","shell.execute_reply.started":"2025-05-04T09:18:22.423236Z","shell.execute_reply":"2025-05-04T09:18:22.426491Z"}},"outputs":[{"execution_count":199,"output_type":"execute_result","data":{"text/plain":"{'f1': 0.841527493476943}"},"metadata":{}}],"execution_count":199},{"cell_type":"markdown","source":"# Testing","metadata":{"id":"dwT-GexR956j"}},{"cell_type":"code","source":"# UPDATE THIS CELL ACCORDINGLY\n\n# define a funciton to load your tokenizer and model from a HF path\n# the path variables can be strings or lists of strings (for ensemble solutions)\n# def load_model(path_to_tokenizer, path_to_model, token):\n#   # Example:\n#   tokenizer = AutoTokenizer.from_pretrained(path_to_tokenizer, token=token)\n#   model = AutoModelForSequenceClassification.from_pretrained(path_to_model, token=token)\n#   model.eval()\n\n#   return tokenizer, model\n\n# define a \"predict\" function that takes the model and a list of input strings\n# and returns the outputs as a list of integer classes\ndef predict(tokenizer, model, input_texts):\n  model.eval()\n  #Example:\n  predictions = []\n  for input_text in input_texts:\n\n    input_ids = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=True)\n\n    with torch.no_grad():\n      logits = model(**{k:v.to('cuda:0') for k,v in input_ids.items()}).logits\n\n    predictions.append(logits.argmax().item())\n\n  return predictions\n\n# set variables\n# path_to_model = \"path/to/your/best/model/on/hf\" # can be a list instead\n# path_to_tokenizer = \"path/to/your/best/tokenizer/on/hf\" # can be a list instead\n# model_access_token = \"access token\" # a fine-grained token with read rights for your model repository\n","metadata":{"id":"oZkqwv229-PM","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:20:43.825983Z","iopub.execute_input":"2025-05-04T09:20:43.826549Z","iopub.status.idle":"2025-05-04T09:20:43.830919Z","shell.execute_reply.started":"2025-05-04T09:20:43.826523Z","shell.execute_reply":"2025-05-04T09:20:43.830407Z"}},"outputs":[],"execution_count":206},{"cell_type":"code","source":"# DO NOT CHANGE THIS CELL!!!\n\n# tokenizer, model = load_model(path_to_tokenizer, path_to_model, token=model_access_token)\n\ntest_data = load_dataset(\"InternationalOlympiadAI/NLP_problem_test\")['test']['text']\n\npredictions = predict(tokenizer, encoder, test_data)\n\nwith open('test_predictions.txt', 'w') as outfile:\n  outfile.write('\\n'.join([str(p) for p in predictions]))","metadata":{"id":"68SDwUjRLBYC","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:20:45.281242Z","iopub.execute_input":"2025-05-04T09:20:45.281517Z","iopub.status.idle":"2025-05-04T09:20:49.834817Z","shell.execute_reply.started":"2025-05-04T09:20:45.281498Z","shell.execute_reply":"2025-05-04T09:20:49.834241Z"}},"outputs":[],"execution_count":207},{"cell_type":"code","source":"load_dataset(\"InternationalOlympiadAI/NLP_problem_test\")['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:21:01.479344Z","iopub.execute_input":"2025-05-04T09:21:01.479736Z","iopub.status.idle":"2025-05-04T09:21:01.914700Z","shell.execute_reply.started":"2025-05-04T09:21:01.479706Z","shell.execute_reply":"2025-05-04T09:21:01.913977Z"}},"outputs":[{"execution_count":209,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 438\n})"},"metadata":{}}],"execution_count":209},{"cell_type":"code","source":"test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T09:20:54.497749Z","iopub.execute_input":"2025-05-04T09:20:54.498447Z","iopub.status.idle":"2025-05-04T09:20:54.506013Z","shell.execute_reply.started":"2025-05-04T09:20:54.498423Z","shell.execute_reply":"2025-05-04T09:20:54.505310Z"}},"outputs":[{"execution_count":208,"output_type":"execute_result","data":{"text/plain":"['𑀟च𑀘𑁦𑀪𑀢णच𑀕 𑀞चणच𑀟 ब𑀱च𑀪𑀢 𑀳न𑀟 ण𑀢 𑀞𑀱च𑀟पच𑀢 च च𑀪𑁦𑀱च ढचणच𑀟 𑀫च𑀟च 𑀞च𑀢 𑀞चणच 𑀞न𑀣न',\n 'चढ𑀢𑀟 𑀣च णच 𑀞च𑀠चपच 𑀞न 𑀳च𑀟𑀢 बच𑀠𑁦 𑀣च 𑀞च𑀠च ब𑀱च𑀠𑀟च𑀟 तढ𑀟 𑁦𑀠𑁦𑀲𑀢𑁦ल𑁦',\n '𑁦ल𑀢𑀳च ल𑁣𑀟त𑀨𑀟𑀕 𑀠चपच𑀪 𑀣च पच 𑀳च𑀠न ञचन𑀞च𑀞च𑀪 𑀪नढनपच प𑀳च𑀪𑀢𑀟 𑀠नल𑀞𑀢𑀟 झच𑀳च ढचणच𑀟 𑀲च𑀠च 𑀣च पचलचनत𑀢',\n 'णच𑀞𑀢𑀟 न𑀞𑀪च𑀢𑀟𑁦𑀕 𑀣चल𑀢ढच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀟च ढ𑀢णच𑀟 𑀳च𑀠च 𑀣च 𑀣चलच 𑀭𑀦𑀧𑀧𑀧 𑀣𑁣𑀠𑀢𑀟 𑀲𑀢पच 𑀣चबच न𑀞𑀪च𑀢𑀟𑁦𑀙',\n '𑀠चप𑀳चलच𑀪 𑀪च𑀳𑀫𑀢𑀟 𑀢𑀳च𑀳𑀳न𑀟 𑀠न𑀫चललच𑀢 च 𑀟च𑀘𑁦𑀪𑀢णच',\n '𑀫चझच𑀪 𑀠च𑀙च𑀣𑀢𑀟च𑀢 च 𑀟𑀢ब𑁦𑀪𑀕 𑀙𑀟च 𑀪च𑀳च 𑀙णच𑀙णच𑀟च न𑀞न 𑀳चढ𑁣𑀣च 𑀞च𑀪णच 𑀣𑁣𑀞च𑀙',\n 'ढ𑀢𑀣𑀢ण𑁣𑀟 𑀫च𑀣चष𑀫च𑀣च च ढचढढच𑀪 𑀞च𑀳न𑀱च𑀪 𑀞च𑀟𑁣 ढचणच𑀟 ढन𑀣𑁦 पच',\n '𑀢𑀟ख𑀢तपन𑀳 𑁣ढ𑀢𑀕 𑀞𑁣पन𑀟 च𑀠न𑀪𑀞च पच णच𑀟𑀞𑁦 𑀫न𑀞न𑀟त𑀢𑀟 𑀣चन𑀪𑀢𑀟 𑀳𑀫𑁦𑀞च𑀪च 𑀭𑀧 𑀞च𑀟 𑀣च𑀟 𑀞च𑀳न𑀱च𑀪 𑀟च𑀘𑁦𑀪𑀢णच 𑁣ढ𑀢𑀟𑀱च𑀟𑀟𑁦 𑁣𑀞𑁦𑀞𑁦',\n 'चढ𑀢𑀟 𑀣च 𑀳चढ𑁣𑀟 𑀲च𑀪च𑀳𑀫𑀢𑀟 ढन𑀪𑁣𑀣𑀢 च 𑀟च𑀘𑁦𑀪𑀢णच 𑀞𑁦 𑀟न𑀲𑀢𑀡',\n 'ढ𑀢𑀪पच𑀟𑀢णच 𑀤च पच ढच𑀢 𑀱च न𑀞𑀪च𑀢𑀟𑁦 पचललच𑀲𑀢𑀟 𑀠च𑀞च𑀠च𑀢 𑀣च 𑀞न𑀣च𑀣𑁦',\n '𑀘𑁣𑀫𑀟𑀳𑁣𑀟 ङ 𑀘𑁣𑀫𑀟𑀳𑁣𑀟𑀕 च𑀟 त𑀢 पच𑀪च𑀪 𑀞च𑀠𑀲च𑀟𑀢𑀟 ढ𑀢ल𑀢ण𑁣ण𑀢𑀟 𑀟च𑀢𑀪च',\n '𑀟चधप𑀢ध पच बच𑀪बचञ𑀢 छच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀞च𑀟 𑀲चपचनत𑀢𑀟 𑀠नपच𑀟𑁦 𑀤न𑀱च पन𑀪च𑀢',\n '𑀠नपच𑀟𑁦 𑀟च पच𑀞च 𑀳चणणच𑀣च च 𑀞च𑀟𑁣 𑀳चढ𑁣𑀣च णच𑀘𑀢𑀟 च𑀢𑀞𑀢𑀟 𑀠च𑀳न च𑀣च𑀢𑀣च𑀢पच 𑀳च𑀫न',\n '𑀠चप𑀳चलच𑀪 प𑀳च𑀣च𑀪 𑀪चणन𑀱च𑀕 णच𑀣𑀣च पच𑀳𑀫𑀢𑀟 𑀲च𑀪च𑀳𑀫𑀢𑀟 𑀞चणचणणच𑀞𑀢 णच 𑀫च𑀢𑀲च𑀪 𑀣च 𑀤च𑀟बचष𑀤च𑀟बच 𑀣च 𑀙णच𑀟 बन𑀣न𑀟 𑀫𑀢𑀘𑀢𑀪च',\n 'चलब𑁦𑀪𑀢च𑀕 𑀙𑀳चण𑁦𑀟 𑀠च𑀢 ब𑀢𑀪𑀞𑀢 णच 𑀞च𑀳च𑀟त𑁦 पच𑀠𑀞च𑀪 𑀳चण𑁦𑀟 𑀠चबच𑀟𑀢𑀙',\n '𑀠न𑀫च𑀠𑀠च𑀣न ढन𑀫च𑀪𑀢𑀕 ध𑀣ध पच त𑁦 ब𑀱च𑀠𑀟चप𑀢𑀟 चधत पच 𑀘च𑀱𑁣 झन𑀟त𑀢 𑀣च पचलचनत𑀢',\n 'च𑀟 𑀳च𑀞𑁣 झच𑀪𑀢𑀟 𑀲च𑀳𑀢𑀟𑀘𑁣𑀘𑀢𑀟 चढन𑀘चष𑀞च𑀣न𑀟च ढच𑀞𑀱च𑀢',\n 'ढ𑀢पत𑁣𑀢𑀟𑀕 चढ𑀢𑀟 𑀣च णच 𑀳च 𑀣च𑀪च𑀘च𑀪 ढचढढच𑀟 𑀞न𑀣𑀢𑀟 𑀢𑀟पच𑀟𑁦प पच𑀞𑁦 𑀲च𑀣न𑀱च',\n '𑀙ढ𑀛चढन चढ𑀢𑀟 𑀣च 𑀤च𑀢 𑀳चन𑀞च𑀪 𑀣च 𑀲च𑀪च𑀳𑀫𑀢𑀟 𑀣चलच च 𑀟च𑀘𑁦𑀪𑀢णच𑀙',\n 'ढ𑀢𑀣𑁦𑀟 णच 𑀞च𑀣𑀣च𑀠च𑀪 𑀣च 𑀳𑀫𑀢𑀪𑀢𑀟 पचललच𑀲𑀢𑀟 पचपपचल𑀢𑀟 च𑀪𑀤𑀢𑀞𑀢 च𑀠न𑀪𑀞च 𑀟च घ𑀭𑀯थप𑀟',\n 'च𑀟 𑀲च𑀪च च𑀠𑀲च𑀟𑀢 𑀣च 𑀳चढढ𑀢𑀟 पच𑀞च𑀪𑀣न𑀟 𑀟च𑀢𑀪च च 𑀟च𑀘𑁦𑀪𑀢णच',\n '𑀠चप𑀳चलच𑀪 त𑀢ष𑀪च𑀟𑀢𑀕 𑀫च𑀟ण𑁣ण𑀢 𑀠च𑀲𑀢णच 𑀫चञच𑀪𑀢 𑀣च 𑀠नपच𑀟𑁦 𑀞𑁦 ढ𑀢 𑀱च𑀘𑁦𑀟 प𑀳चललच𑀞च 𑀢णच𑀞𑁣𑀞𑀢',\n 'चढ𑀢𑀟 𑀣च छच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀞𑁦 त𑁦𑀱च 𑀞च𑀟 झच𑀪𑀢𑀟 चलढच𑀳𑀫𑀢𑀟 छच𑀟 𑀳𑀢णच𑀳च',\n '𑀳𑀫नबचढच𑀟𑀟𑀢 ब𑀖 𑀳च𑀫𑁦ल 𑀳न𑀟 𑀲च𑀪च पच𑀪𑁣 𑀞च𑀟 𑀠चप𑀳चलच𑀪 प𑀳च𑀪𑁣 च णच𑀟𑀞𑀢𑀟',\n 'प𑀪न𑀠ध णच ण𑀢 ढच𑀪च𑀤च𑀟च𑀪 लचलचपच पचपचल𑀢𑀟 च𑀪𑀤𑀢𑀞𑀢𑀟 पन𑀪𑀞𑀢णणच',\n 'च𑀳च𑀪च𑀪 𑀣च 𑀳चन𑀣𑀢णणच 𑀞𑁦 ण𑀢 𑀳चढ𑁣𑀣च 𑀪च𑀳𑀫𑀢𑀟 𑀠च𑀫च𑀘𑀘चपच 𑀣चबच 𑀞च𑀳च𑀳𑀫𑁦𑀟 𑀣न𑀟𑀢णच',\n '𑀠चप𑀳चलच𑀪 चढ𑀢𑀟त𑀢𑀕 𑀞𑁣 न𑀞𑀪च𑀢𑀟𑁦 𑀤च पच 𑀢णच त𑀢णच𑀪 𑀣च 𑀣न𑀟𑀢णच 𑀣न𑀞 𑀣च णचझ𑀢𑀟 𑀣च च𑀞𑁦 ण𑀢𑀡',\n 'चढ𑀢𑀟 𑀣च झच𑀪न𑀱च𑀪 𑀫चन𑀫च𑀱च𑀪 𑀲च𑀪च𑀳𑀫𑀢 𑀣च 𑀞च𑀳𑀫𑀢 𑀭थ𑀯𑁢र 𑀞𑁦 𑀟न𑀲𑀢 बच छच𑀟 𑀟च𑀘𑁦𑀪𑀢णच',\n '𑀱च𑀢𑀱चण𑁦𑀕 𑀢𑀳𑀱चध त𑁦 पच 𑀞च𑀳𑀫𑁦 𑀠नपन𑀠 र𑀭 च त𑁣त𑀢𑀟 𑁣𑀟𑀣𑁣𑀦 𑀠च𑀟𑀢णणचपच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀳न𑀟 𑀲च𑀪च 𑀢𑀳च 𑀳चन𑀣𑀢णणच',\n '𑀠𑁦 𑀞च𑀟𑁣 𑀤च पच ण𑀢 𑀣च पचललच𑀲𑀢𑀟 𑀟च𑀢𑀪च ढ𑀢ल𑀢णच𑀟 𑀭𑀧 𑀟च तढ𑀟𑀡',\n 'च𑀣𑁦𑀱चल𑁦 𑀱चललण च𑀣𑁦ण𑁦𑀠𑁣𑀕 ञच𑀟 𑀟च𑀘𑁦𑀪𑀢णच𑀪 𑀣च ढ𑀢𑀣𑁦𑀟 णच ढच𑀢 𑀱च 𑀠चपच𑀢𑀠च𑀞𑀢𑀟 𑀠च𑀙च𑀘𑀢𑀟 ढच𑀢प𑀢ल 𑀠चल𑀢𑀟 च𑀠न𑀪𑀞च',\n 'पच𑀞च𑀣𑀣च𑀠च 𑀞च𑀟 𑀞𑁣ब𑀢𑀟 𑀟𑀢लन𑀕 च𑀠न𑀪𑀞च 𑀤च पच 𑀤च𑀲पच𑀪𑁦 घ𑀭𑀧𑀧𑀠 𑀣चबच पचललच𑀲𑀢𑀟 𑀣च पच𑀞𑁦 ढच𑀢 𑀱च 𑀫चढच𑀳𑀫च',\n 'च𑀟𑀟𑁣ढच𑀪 𑀞𑁣𑀪𑁣𑀟च 𑀣च णच𑀞𑀢𑀟 न𑀞𑀪च𑀢𑀟𑁦 𑀳न𑀟 पच𑁥च𑀪𑁥च𑀪𑁦 𑀤च𑀠च𑀟 लच𑀲𑀢णच𑀪 𑀣न𑀟𑀢णच 𑀢𑀪𑀢𑀟𑀳च 𑀠च𑀲𑀢 𑀠न𑀟𑀢 त𑀢𑀞𑀢𑀟 𑀳𑀫𑁦𑀞𑁦𑀪च 𑀭𑀖',\n 'णच𑀣𑀣च प𑀳च𑀟च𑀟𑀢𑀟 𑀤च𑀲𑀢 𑀞𑁦 𑀟चझच𑀳च 𑀞च𑀟च𑀟च𑀟 𑀳च𑀟च𑀙𑁣𑀙𑀢 च 𑀢𑀟𑀣𑀢णच',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳 च 𑀟च𑀘𑁦𑀪𑀢णच𑀕 णच𑀱च𑀟त𑀢𑀟 𑀘𑀢𑀫𑁣𑀫𑀢 𑀙ढच 𑀤च 𑀳न 𑀢णच ढ𑀢णच𑀟 चलढच𑀳𑀫𑀢 च 𑀳𑀫𑁦𑀞च𑀪च𑀪 𑀟च𑀟 ढच𑀙',\n '𑁦ल𑁣𑀟 𑀠न𑀳𑀞𑀕 चढन ढ𑀢णच𑀪 𑀣च 𑀳न𑀞च 𑀞च𑀠चपच 𑀞न 𑀳च𑀟𑀢 बच𑀠𑁦 𑀣च चपपच𑀘𑀢𑀪𑀢𑀟 𑀣च णच 𑀲𑀢 𑀞𑁣𑀱च 𑀞नञ𑀢 च 𑀣न𑀟𑀢णच',\n '𑀘𑀢𑀪ब𑀢𑀟 पच𑀟𑀞च𑀪 𑀠च𑀢 𑀟च 𑀢𑀪च𑀟 णच ढच𑀪𑁣 ब𑀢ढ𑀪चलपच𑀪',\n 'झचलनढचल𑁦𑀟 𑀣च 𑀤च च 𑀲न𑀳𑀞च𑀟पच 𑀢𑀣च𑀟 पचपपचल𑀢𑀟 च𑀪𑀤𑀢𑀞𑀢𑀟 𑀟च𑀘𑁦𑀪𑀢णच णच त𑀢 बचढच 𑀣च पच𑁥च𑀪𑁥च𑀪𑁦𑀱च',\n 'णच𑀣𑀣च 𑀫चन𑀫च𑀱च𑀪 𑀲च𑀪च𑀳𑀫𑀢𑀟 𑀞चणच𑀟 चढ𑀢𑀟त𑀢 पच𑀞𑁦 पच𑀳𑀢𑀪𑀢 𑀞च𑀟 𑀪चणन𑀱च𑀪 𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच',\n 'ढ𑀢𑀣𑀢ण𑁣𑀕 ढच𑀫चन𑀳𑀫𑀢णच𑀪 𑀣च 𑀞𑁦 𑀳च𑀟च𑀙च𑀪 पनझच 𑀞𑁦𑀞𑁦 𑀟चध𑁦ध च 𑀞च𑀣न𑀟च',\n '𑀙𑀢𑀳𑀱चध पच 𑀞च𑀳𑀫𑁦 𑀣चणच 𑀣चबच त𑀢𑀞𑀢𑀟 𑀠च𑀙च𑀢𑀞चपच𑀟 चतप𑀢𑁣𑀟 चबच𑀢𑀟𑀳प 𑀫न𑀟ब𑁦𑀪𑀙',\n '𑀞न𑀣𑀢𑀟 𑀞𑀢𑀪𑀢𑀲प𑁣𑀟 𑀣च ढच च ढन𑀞चपच𑀪 𑀠च𑀞च𑀠च𑀳𑀫𑀢 𑀠च𑀢 णच𑀱च 𑀱च𑀘𑁦𑀟 पच𑀪च 𑀳𑀫𑀢',\n '𑀙𑀞𑀛न𑀣𑀢𑀟 𑀲𑀢प𑁣 𑀟𑁦 𑀣चल𑀢ल𑀢𑀟 प𑀳च𑀣च𑀪 𑀠𑁣प𑁣त𑀢 च 𑀟च𑀘𑁦𑀪𑀢णच𑀙',\n '𑀟𑀢ब𑁦𑀪𑀢च ण𑁣नप𑀫 𑀢𑀟ख𑁦𑀳प𑀠𑁦𑀟प 𑀲न𑀟𑀣𑀕 तढ𑀟 णच 𑀲𑀢पच𑀪 𑀣च प𑀳च𑀪𑀢𑀟 𑀳𑀫𑀢𑀪𑀢𑀟 ढन𑀫च𑀪𑀢 𑀟च 𑀪चणच 𑀠चपच𑀳च 𑀟च ढ𑀢ल𑀢णच𑀟 𑀬𑀖',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 𑀘𑀢𑀫𑁣𑀫𑀢 च च𑀠न𑀪𑀞च 𑀳न𑀟 𑀲च𑀪च ढन𑀣𑁦 𑀫च𑀪𑀞𑁣𑀞𑀢𑀟 𑀞च𑀳न𑀱च𑀟त𑀢',\n 'चढ𑀢𑀟 𑀣च 𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀞𑁦 त𑁦𑀱च 𑀞च𑀟 𑀣𑀢𑀠ढ𑀢𑀟 ढच𑀳𑀫𑀢𑀟 𑀣च 𑀞𑁦 𑀞च𑀟 𑀞च𑀳च𑀪',\n '𑀠च𑀢 𑀞न𑀣𑀢𑀟 𑀣न𑀟𑀢णच 𑀘𑁦𑀲𑀲 ढ𑁦𑀤𑁣𑀳𑀕 चढन ढ𑀢णच𑀪 बच𑀠𑁦 𑀣च चपपच𑀘𑀢𑀪𑀢𑀟 𑀣च णच 𑀲𑀢 𑀞𑁣𑀱च 𑀞न𑀣𑀢 च 𑀣न𑀟𑀢णच',\n '𑀟ढ𑀳𑀕 𑀘𑀢𑀫𑁣𑀫𑀢𑀟 𑀣च 𑀞चणच𑀟 चढ𑀢𑀟त𑀢 𑀳न𑀞च 𑀲𑀢 प𑀳च𑀣च च 𑀟च𑀘𑁦𑀪𑀢णच',\n 'ब𑀱च𑀠𑀟च𑀟 𑀘𑀢𑀫च𑀪 𑀢𑀠𑁣 णच ढ𑀢णच 𑀠च𑀟𑁣𑀠च चलढच𑀳च 𑀣𑀢णणच𑀪 𑀟च𑀢𑀪च 𑀠𑀢ल𑀢णच𑀟 𑀭𑀧',\n 'चढनढन𑀱च𑀟 𑀞च𑀪𑀲च𑀲च ब𑀱𑀢𑀱च 𑀣च 𑀳न𑀞च 𑀲च𑀪न च च𑀲𑀢𑀪𑀞च च ठ𑀧𑀭थ',\n '𑀠चपच𑀞च𑀟 𑀞च𑀪𑁦 𑀞च𑀢 𑀣चबच 𑀫च𑀣न𑀪च𑀟 चढचढ𑁦𑀟 𑀫च𑀱च च 𑀟च𑀘𑁦𑀪𑀢णच',\n 'झच𑀙𑀢𑀣च𑀪 त𑀢𑀪𑁦 𑀞न𑀣𑀢𑀟 तढ𑀟 पच 𑀪चढच 𑀞च𑀟 𑀠च𑀘चल𑀢𑀳च𑀪 𑀣चपपच𑀱च𑀟 𑀟च𑀘𑁦𑀪𑀢णच',\n 'च𑀞𑀢𑀟𑀱न𑀠𑀢 च𑀣𑁦𑀳𑀢𑀟च𑀕 𑀣चल𑀢ल𑀢𑀟 𑀣च णच 𑀳च च𑀞च 𑀱च𑀟𑀞𑁦 𑀳𑀫𑀢 𑀣चबच 𑀤च𑀪ब𑀢𑀟 त𑀢𑀟 𑀫च𑀟त𑀢 𑀣च 𑀲𑀢𑀲𑀢पच 𑀙णच𑀟 𑀟𑀢ब𑁦𑀪𑀢च',\n 'च𑀠ढचल𑀢णच𑀪 𑀪न𑀱च पच 𑀘च𑀟ण𑁣𑀱च 𑀠च𑀟𑁣𑀠च च𑀳च𑀪च𑀪 ढ𑀢ल𑀢ण𑁣ण𑀢 च ढ𑀢𑀪𑀟𑀢𑀟 𑀞𑁦ढढ𑀢',\n '𑀠नलप𑀢त𑀫𑁣𑀢त𑁦𑀕 चढ𑀢𑀟 𑀣च 𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀞𑁦 त𑁦𑀱च 𑀞च𑀟 𑀞च𑀪𑀢𑀟 𑀞न𑀣𑀢𑀟 𑀣𑀳पख 𑀣च ब𑁣पख',\n '𑀠चपच ढच 𑀳च 𑀳च𑀠न𑀟 𑀞न𑀣𑀢𑀟 𑀳𑀫𑀢बच 𑀞च𑀠च𑀪 𑀠च𑀤च𑀘𑁦𑀟𑀳न च 𑀲च𑀣𑀢𑀟 𑀣न𑀟𑀢णच',\n '𑀪𑀢𑀞𑀢त𑀢𑀟 णच𑀟𑀞𑀢𑀟 प𑀢ब𑀪चण 𑀕 णच𑀣𑀣च छच𑀟 बन𑀣न𑀟 𑀫𑀢𑀘𑀢𑀪च𑀪 णच𑀟𑀞𑀢𑀟 𑀞𑁦 ण𑀢𑀟 𑀞चलचत𑀢 𑀣च बच𑀟ण𑁦𑀟 ढ𑀢𑀳𑀫𑀢णच 𑀣𑁣𑀟 𑀳न 𑀪चणन',\n '𑀞𑀛च𑀳च𑀳𑀫𑁦𑀟 च𑀲𑀢𑀪𑀞च 𑀣च 𑀣च𑀪च𑀘च𑀪 𑀞न𑀣𑀢𑀟𑀳न पच पच𑀳𑀫𑀢 𑀳चढ𑁣𑀣च 𑀲च𑀣न𑀱च𑀪 𑀲च𑀟 𑀣𑀢𑀟 𑀢𑀟ब𑀢लच',\n '𑀢𑀞𑀢𑀪च𑀪𑀢𑀟 त𑁦𑀱च प𑀳च𑀟च𑀟𑀢𑀟 पचलचनत𑀢 णच 𑀳च 𑀠नपच𑀟𑁦 𑀟च 𑀳चणच𑀪 𑀣च णचप𑀳न𑀟𑀳न च 𑀤𑀢𑀠ढचढ𑀱𑁦 णच 𑀘च 𑀫च𑀟𑀞चल𑀢 च 𑀟च𑀘𑁦𑀪𑀢णच',\n 'प𑀳चनप𑀳चण𑀢 𑀣च 𑀪𑀢ढच च 𑀫च𑀪च𑀠पचतत𑀢णच𑀪 𑀠च𑀳च𑀟च𑀙च𑀟पच𑀪 𑀠च𑀢 पच 𑀟च𑀘𑁦𑀪𑀢णच',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 च𑀠न𑀪𑀞च 𑀤च पच ढच𑀢 𑀱च 𑀞𑁣𑀱च𑀟𑀟𑁦 𑀣च𑀟 𑀞च𑀳च पचललच𑀲𑀢𑀟 𑀣चलच 𑀭𑀦ठ𑀧𑀧',\n '𑀱च𑀫चल𑀫चलन ढ𑀢णच𑀪 𑀣च 𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀞𑁦 त𑀢𑀞𑀢 च 𑀱च𑀟𑀟च𑀟 ल𑁣𑀞चत𑀢',\n '𑀙𑀠न𑀟 ण𑀢 𑀲च𑀪𑀢𑀟षत𑀢𑀞𑀢 𑀣च 𑀳च𑀞𑁦 ढन𑀣𑁦 𑀢णच𑀞𑁣𑀞𑀢𑀟 𑀞च𑀟 पन𑀣न 𑀟च 𑀟च𑀘𑁦𑀪𑀢णच𑀙',\n '𑀟च𑀪𑁦𑀟𑀣𑀪च 𑀠𑁣𑀣𑀢𑀕 𑀲𑀢𑀪च𑀢𑀠𑀢𑀟𑀢𑀳पच𑀟 𑀢𑀟𑀣𑀢च णच 𑀘च𑀟ण𑁦 𑀣𑁣𑀞𑁣𑀞𑀢𑀟 𑀟𑁣𑀠च 𑀣च 𑀳न𑀞च 𑀘च𑀱𑁣 त𑁦षत𑁦ष𑀞नषत𑁦 ढचणच𑀟 𑀳𑀫𑁦𑀞च𑀪च बन𑀣च च𑀟च 𑀤च𑀟बचष𑀤च𑀟बच',\n 'च𑀟 𑀟𑁦𑀠𑀢 च झच𑀪च ञचब𑁦𑀱च झच𑀳च𑀳𑀫𑁦 𑀠चपचलचनपच ल𑁣𑀞चत𑀢𑀟 ढ𑀢णच𑀟 ढच𑀳𑀫𑀢',\n 'णच𑀣𑀣च 𑀢𑀟𑀣𑀢च पच पच𑀳𑁣 𑀞𑁦णच𑀪 𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀭थ𑁢 𑀤न𑀱च ब𑀢𑀣च',\n '𑀱च𑀳न 𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀟च तचततच𑀞च𑀪 𑀠च𑀙च𑀢𑀞चपच𑀟 𑀱नपच𑀪 लच𑀟पच𑀪𑀞𑀢 𑀞च𑀟 णच𑀘𑀢𑀟 च𑀢𑀞𑀢',\n 'पचललच𑀲𑀢𑀟 त𑁣ख𑀢𑀣ष𑀭थ𑀕 च𑀟च ढनञ𑁦 𑀱च झच𑀟च𑀟च𑀟 छच𑀟 𑀞च𑀳न𑀱च 𑀞च𑀠𑀲च𑀟𑀢 𑀞णचनपच 𑀣𑁣𑀟 𑀳च𑀠न𑀟 पचललच𑀲𑀢𑀟 ढन𑀫च𑀪𑀢',\n '𑀣चढच𑀪च𑀪 𑀣च 𑀳𑁣𑀘𑁣𑀘𑀢𑀟 𑀠चल𑀢 𑀳न𑀞च ण𑀢 𑀱च 𑁦त𑁣𑀱च𑀳 पच 𑀘च𑀟ण𑁦 𑀠न𑀳न पच𑀞न𑀟𑀞न𑀠𑀢',\n '𑀞च𑀳च𑀲𑀢𑀟 𑀞नञ𑀢𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀟च ठ𑀧ठ𑀭𑀕 ढन𑀫च𑀪𑀢 णच बचढचपच𑀪 𑀱च 𑀠च𑀘चल𑀢𑀳च 𑀞च𑀳च𑀲𑀢𑀟 𑀟च𑀢𑀪च प𑀢𑀪𑀢ल𑀢णच𑀟 𑀭𑀰𑀯𑀧𑀗',\n '𑀢𑀪च𑀟 पच 𑀳च𑀞𑁦 𑀙𑀞𑀱चत𑁦 𑀘𑀢𑀪ब𑀢𑀟 𑀣च 𑀞𑁦 𑀲च𑀳चष𑀞चन𑀪𑀢𑀟 𑀠च𑀢𑀙',\n 'चढन ढ𑀢णच𑀪 बच𑀠𑁦 𑀣च 𑀳चढ𑁣𑀟 𑀠च𑀢 𑀞नञ𑀢𑀟 𑀣न𑀟𑀢णच च𑀪𑀟चनलप 𑀣च णच 𑀳चन𑀞𑁦 𑁦ल𑁣𑀟 𑀠न𑀳𑀞',\n '𑀪च𑀙चण𑀢 𑀪𑀢बच𑀕 𑀲च𑀲नप𑀢𑀞च𑀪 𑀠चललच𑀞च𑀪 ब𑀢𑀣च𑀘𑁦 च 𑀟च𑀘𑁦𑀪𑀢णच',\n '𑀟च𑀘𑁦𑀪𑀢णच 𑀤च पच 𑀪च𑀳च र𑀖फ 𑀟च 𑀞न𑀣𑀢𑀟 𑀳𑀫𑀢बच 𑀳चढ𑁣𑀣च 𑀲च𑀣न𑀱च𑀪 𑀲च𑀪च𑀳𑀫𑀢𑀟 𑀠च𑀢',\n '𑀳𑀫नबचढच ढन𑀫च𑀪𑀢 णच ढचणच𑀪 𑀣च न𑀠च𑀪𑀟𑀢𑀟 ढनञ𑁦 𑀢णच𑀞𑁣𑀞𑀢𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀫नञन 𑀟च 𑀞च𑀟 पन𑀣न',\n 'णच 𑀞च𑀠चपच 𑀳𑀫नबचढच ढन𑀫च𑀪𑀢 णच ढनञ𑁦 ढ𑁣𑀣𑁣𑀘𑀢𑀟 𑀟च𑀘𑁦𑀪𑀢णच ष च𑀫𑀠च𑀣 बन𑀠𑀢',\n '𑀳चनण𑀢𑀟 णच𑀟चण𑀢𑀕 𑀙𑀪न𑁥च𑁥𑁥न𑀟 𑀠𑁣प𑁣त𑀢 𑀠च𑀳न 𑀫चञच𑀪𑀢𑀙 च𑀞𑁦 𑀳चणच𑀪 𑀱च 𑀙णच𑀟 च𑀲𑀪𑀢𑀞च',\n 'झन𑀟ब𑀢णच𑀪 𑀠च𑀟𑁣𑀠च पच झच𑀣𑀣च𑀠च𑀪 𑀣च 𑀣चलच𑀪 𑀠च𑀳च𑀪च च 𑀞च𑀣न𑀟च',\n 'ढ𑀢𑀟त𑀢𑀞𑁦𑀟 बच𑀳𑀞𑀢णच𑀕 𑀣च बच𑀳𑀞𑁦 तढ𑀟 णच 𑀪चब𑁦 𑀞नञ𑀢𑀟 𑀣च ढच𑀟𑀞न𑀟च 𑀞𑁦 तच𑀤च𑀪 𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच𑀡',\n 'णच𑀣𑀣च 𑀪चब𑁦 𑀣च𑀪च𑀘च𑀪 𑀟च𑀢𑀪च 𑀤च𑀢 𑀳𑀫च𑀲𑀢 𑀪चणन𑀱च𑀪 पचलच𑀞च च 𑀟च𑀘𑁦𑀪𑀢णच',\n '𑀞𑁣𑀪𑀢णच पच च𑀪𑁦𑀱च पच 𑀞च𑀪णचपच 𑀪च𑀣𑁦ष𑀪च𑀣𑀢𑀟 𑀪च𑀳𑀫𑀢𑀟 लच𑀲𑀢णच𑀪 𑀳𑀫नबचढच 𑀞𑀢𑀠 𑀘𑁣𑀟बषन𑀟',\n '𑀠च𑀳च𑀟च 𑀞𑀢𑀠𑀢णणच 𑀳न𑀟 𑀳च𑀞𑁣 𑀳चन𑀪चण𑁦 त𑀢𑀞𑀢𑀟 𑀘च𑀠च𑀙च',\n 'णच𑀱च𑀟 च𑀠𑀲च𑀟𑀢 𑀣च 𑀳𑀫च𑀲न𑀞च𑀟 𑀳च𑀣च 𑀤न𑀠न𑀟पच 𑀟च 𑀫च𑀟च ढचतत𑀢',\n 'त𑁣ख𑀢𑀣ष𑀭थ𑀕 𑀲𑀢ण𑁦 𑀣च च𑀠न𑀪𑀞च𑀱च 𑀗𑀧𑀧𑀦𑀧𑀧𑀧 𑀳न𑀟 𑀠नपन 𑀳च𑀟च𑀣𑀢णणच𑀪 त𑁣ख𑀢𑀣',\n 'ग𑀞च𑀪च𑀟त𑀢𑀟 चलढच𑀳𑀫𑀢 णच 𑀳च 𑀱च𑀳न 𑀠च𑀙च𑀢𑀞चपच𑀟 च𑀳𑀢ढ𑀢प𑀢 𑀟च प𑀳च𑀟च𑀪 च𑀢𑀞𑀢𑀟𑀳नभ',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच𑀕 पच𑁥𑀢𑀟 𑀫च𑀟𑀞चल𑀢𑀟 𑀣च 𑀞𑁦 𑀳च𑀠न𑀟 𑀠नपन𑀠 ढचणच𑀟 𑀳𑀫𑀢बच पच𑀳𑀫𑀢𑀟 𑀫च𑀟𑀞चल𑀢',\n '𑀪च𑀠च𑀣च𑀟𑀕 च𑀠𑀲च𑀟𑀢𑀟 च𑀤न𑀠𑀢 बच लच𑀲𑀢णच𑀪 𑀘𑀢𑀞𑀢',\n 'णच𑀪𑀢𑀟णच𑀪 𑀣च 𑀞च𑀳𑀫𑀢𑀟पच 𑀞𑁦 णच𑀱च𑀟 𑀞च𑀪ण𑁦𑀱च𑀦 𑀞न𑀠च पच𑀟च 𑀳𑁣𑀟 𑀪च𑀱च',\n '𑀠च𑀳च𑀟च 𑀳न𑀟 त𑁦 त𑀢𑀱𑁣𑀟 ढच𑀳𑀢𑀪 ढच णच 𑀫च𑀟च झच𑀪𑀲𑀢𑀟 𑀘𑀢𑀠चभ𑀢',\n 'ढन𑀫च𑀪𑀢 णच 𑀞𑁣𑀪𑀢 𑀳𑀫नबचढच𑀟 𑀫न𑀞न𑀠च𑀪 𑀢𑀟𑀳𑀫𑁣𑀪च𑀪 लच𑀲𑀢णच',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 णच𑀣𑀣च तनपच𑀪 𑀞𑁣𑀪𑁣𑀟च पच 𑀘च𑀱𑁣 झच𑀪च𑀟त𑀢𑀟 𑀢𑀳𑀞च𑀪 𑁣𑀩णब𑁦𑀟 च झच𑀳च𑀳𑀫𑁦 𑀠च𑀳न पच𑀳𑁣𑀱च',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 पच णचणच 𑀠नपन𑀠 𑀤च𑀢 𑀳च𑀟 णच𑀟च 𑀣च तनपच𑀪𑀡',\n 'च𑀟𑀟𑁣ढच𑀪 त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳 𑀤च पच 𑀳𑀫च𑀲𑀢 पचपपचल𑀢𑀟 च𑀪𑀤𑀢𑀞𑀢𑀟 च𑀲𑀢𑀪𑀞च',\n '𑀠चबच𑀟𑀢𑀟 प𑀳च𑀪𑀢𑀟 𑀢णचल𑀢 𑀟च 𑀠च𑀤च𑀕 𑀱च𑀣च𑀟𑀟𑁦 𑀫च𑀟ण𑁣ण𑀢 𑀳न𑀞च 𑀣चत𑁦 च ण𑀢 च𑀠𑀲च𑀟𑀢 𑀣च 𑀳न𑀡',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 त𑀫𑀢𑀟च 𑀤च पच पच𑀢𑀠च𑀞च 𑀱च 𑀟च𑀘𑁦𑀪𑀢णच 𑀣च 𑀞च𑀳च𑀳𑀫𑁦𑀟 च𑀲𑀢𑀪𑀞च',\n 'च𑀟 बच𑀟𑁣 𑀠चबच𑀟𑀢𑀟 त𑀢𑀱𑁣𑀟 𑀳𑁣',\n '𑀣च बच𑀳𑀞𑁦 च𑀟 बच𑀟𑁣 𑀠चबच𑀟𑀢𑀟 त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳 च 𑀟च𑀘𑁦𑀪𑀢णच𑀡',\n '𑀞𑁣 𑀞न𑀟 𑀳च𑀟 𑀟चन𑀙𑀢𑀟 चढ𑀢𑀟त𑀢𑀟 𑀣च 𑀞𑁦 𑀢𑀟बच𑀟पच 𑀪चणन𑀱च𑀪 चन𑀪𑁦𑀡',\n '𑀪च𑀟च𑀪 𑀳𑀢𑀞𑀢लच पच 𑀣न𑀟𑀢णच𑀕 𑀙च पचढढचपच𑀪 च𑀟 ण𑀢 ब𑀱च𑀘𑀢𑀟 𑀘𑀢𑀟𑀢 𑀞च𑀲𑀢𑀟 चन𑀪𑁦𑀙',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच 𑀫𑀢𑀪च 𑀣च ल𑀢𑀞𑀢पच 𑀞च𑀟 तनपच𑀪 प𑀳च𑀟च𑀟𑀢𑀟 𑀣च𑀠न𑀱च',\n '𑀞𑁣 च𑀟च 𑀢णच 𑀱च𑀪𑀞𑁦𑀱च 𑀣चबच तनपच𑀪 𑀳𑀢𑀞𑀢लच𑀡',\n 'चढ𑀢𑀟 𑀣च णच 𑀞च𑀠चपच 𑀞न 𑀳च𑀟𑀢 बच𑀠𑁦 𑀣च चप𑀢𑀳𑀫च𑀱च',\n 'च𑀠𑀲च𑀟𑀢𑀟 णचल𑁣 𑀣च 𑀠न𑀫𑀢𑀠𑀠च𑀟त𑀢𑀟𑀳च बच लच𑀲𑀢णच𑀪 𑀘𑀢𑀞𑀢𑀟 𑀣च𑀟 च𑀣च𑀠',\n '𑀫च𑀱च𑀟 𑀘𑀢𑀟𑀢 च 𑀳𑀫𑁦𑀞च𑀪न𑀟 𑀞न𑀪नत𑀢णच 𑀞च 𑀢णच 𑀘च𑀟ण𑁣 𑀣𑀢𑀠चनत𑁦𑀱च',\n '𑀠𑁦त𑁦 त𑁦 तनपच𑀪 ध𑁣ल𑀢𑁣 𑀞न𑀠च णचणच पच𑀞𑁦 णचञन𑀱च𑀡',\n 'तनपच𑀪 𑀞𑁣𑀪𑁣𑀟च 𑀕 ढ𑀢𑀣𑁦𑀟 𑀤च𑀢 ढनझचत𑀢 च𑀠न𑀪𑀞च𑀱च 𑀳न 𑀳च𑀟णच पच𑀞न𑀟𑀞न𑀠𑀢 प𑀳च𑀱𑁣𑀟 𑀞𑀱च𑀟च 𑀭𑀧𑀧',\n 'पचपपचन𑀟च𑀱च च𑀞च𑀟 तनपच𑀪 𑀳𑀢𑀞𑀢लच पच𑀪𑁦 𑀣च 𑀲च𑀪𑀲𑁦𑀳च चढ𑀣नललच𑀫𑀢 𑀢ढ𑀪च𑀫𑀢𑀠 𑀠न𑀳च',\n '𑁦ढ𑁣लच𑀕 𑀟च𑀘𑁦𑀪𑀢णच पच 𑀳𑀫𑀢𑀪णच णच𑀞𑀢 𑀣च ढचलच𑀙𑀢𑀟 𑀣च 𑀞𑁦 पन𑀟𑀞च𑀪च𑀪 च𑀲𑀢𑀪𑀞च',\n '𑀤च𑀲𑀢𑀟 𑀱नपच 𑀟च 𑀘च𑀟ण𑁣 प𑀳न𑀲च𑀟 𑀠चपच 𑀣च 𑀱न𑀪𑀢',\n '𑀪च𑀟च𑀪 𑀠चपच पच 𑀣न𑀟𑀢णच𑀕 𑀙णच𑀪च𑀟 𑀣च च𑀞𑁦 𑀫च𑀢𑀲च 𑀣च लच𑀪न𑀪च𑀪 𑀞𑀱च𑀞𑀱चल𑀱च ढच छचछच𑀟 चल𑀘च𑀟𑀟न ढच 𑀟𑁦𑀙',\n 'त𑀢𑀱𑁣𑀟 𑀞𑁣𑀣च𑀕 𑀠न 𑀭𑀧 𑀠न𑀞च 𑀪चणन त𑀢𑀞𑀢𑀟 𑀠नपन𑀠 𑀭𑁢𑀧 च 𑀟च𑀘𑁦𑀪𑀢णच',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच𑀕 चढ𑀢𑀟 𑀣च णच 𑀳च 𑀘च𑀪𑀢𑀪च𑀢 𑀞𑁦 𑀳च𑀠न𑀟 𑀠चप𑀳चलच𑀪 𑀤नढच𑀪 बच𑀳𑀫𑀢',\n '𑀣चल𑀢ढच𑀟 ब𑀫च𑀟च 𑀳न𑀟 𑀞𑁣𑀳च 𑀳न ढच𑀪 त𑀫𑀢𑀟च 𑀳चढ𑁣𑀣च त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳',\n '𑀪𑀢बचष𑀞च𑀲𑀢𑀟 त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 𑀠चपच𑀪 𑀣च 𑀞𑁦 𑀘चब𑁣𑀪च𑀟पच𑀪 𑀳च𑀠च𑀪 𑀣च 𑀠चबच𑀟𑀢𑀟 त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच𑀕 चढ𑀢𑀟 𑀣च णच 𑀳च 𑀠चपच 𑀳न𑀞च 𑀲𑀢 𑀲न𑀳𑀞च𑀟पच𑀪 त𑀢𑀱𑁣𑀟 𑀞च𑀢 𑀠च𑀢 प𑀳च𑀟च𑀟𑀢',\n '𑀠नपन𑀠 र 𑀳न𑀟 𑀠नपन ढचणच𑀟 𑀞𑀱चल𑁦𑀞𑀱चल𑁦 णच 𑀞𑀢𑀲𑁦 𑀣च 𑀳न च 𑀳𑁦𑀟बचल',\n '𑀠चलच𑀪𑀢च𑀕 णच𑀣𑀣च च𑀞𑁦 𑀲च𑀠च 𑀣च झच𑀪च𑀟त𑀢𑀟 𑀠चबच𑀟𑀢𑀟 तनपच𑀪 च 𑀘𑀢बच𑀱च',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच𑀕 च𑀟च 𑀱च𑀪𑀞𑁦𑀱च 𑀣चबच तनपच𑀪 पच𑁥𑀢𑀟 𑀫च𑀟𑀞चल𑀢𑀡',\n '𑀳चढढ𑀢𑀟 𑀫च𑀟ण𑁣ण𑀢 ढ𑀢णन 𑀠च𑀳न 𑀳चन𑀞𑀢 𑀟च 𑀪चब𑁦 𑀠चप𑀳च𑀟च𑀟त𑀢णच𑀪 झ𑀢ढच 𑀣चबच 𑀢𑀟ब𑀢लच 𑀣च च𑀠न𑀪𑀞च',\n '𑀢ललच𑀪 त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 णच𑀣𑀣च तनपच𑀪 पच𑀞𑁦 लचलचपच झ𑀱चझ𑀱चल𑀱च',\n '𑀙णच𑀣𑀣च 𑀟च प𑀳च𑀟𑀢 𑀞𑁣𑀱च 𑀫च𑀪 𑀠𑀢𑀘𑀢𑀟च ढचणच𑀟 𑀟च 𑀫च𑀢𑀫न𑀙',\n '𑀙णच𑀪 प𑀢𑀞प𑁣𑀞 𑀣𑀢𑀟 𑀣च 𑀞𑁦 पच𑀢𑀠च𑀞च 𑀱च 𑀠च𑀳न 𑀠चप𑀳चलच𑀪 झ𑀱चझ𑀱चल𑀱च प𑀳च𑀲पचत𑁦 ब𑀢𑀣च𑀘𑁦𑀟𑀳न',\n '𑀪च𑀙चण𑀢 𑀪𑀢बच𑀕 तनपच𑀪 पच𑀪𑀢𑀟 𑀲न𑀞च',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳 च 𑀟च𑀘𑁦𑀪𑀢णच𑀕 𑀳𑀫𑀢𑀟 झच𑀳च𑀪 𑀟च त𑀢𑀟 बचलचढच च णचझ𑀢 𑀣च च𑀟𑀟𑁣ढच𑀪𑀡',\n '𑀙णच𑀣𑀣च तनपच𑀪 𑀳𑀢𑀞𑀢लच पच 𑀳च 𑀟च 𑀳𑀫च𑀲𑁦 𑀳𑀫𑁦𑀞च𑀪च पच𑀞𑀱च𑀳 च 𑀞च𑀪चपन𑀟 𑀣𑀢𑀲ल𑁣𑀠च𑀙',\n '𑀞चलल𑀢 णच𑀣𑀣च च𑀳𑀢ढ𑀢प𑀢𑀟 तनपच𑀪 त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳 णच𑀞𑁦 च ल𑁦बच𑀳',\n 'चढ𑀢𑀟 𑀣च णच 𑀳च 𑀠च𑀳न बच𑀘𑁦𑀪𑁦𑀟 ढच𑀪त𑀢 𑀳न𑀞च 𑀲𑀢 𑀠च𑀳न 𑀣𑁣ब𑁣𑀟 ढच𑀪त𑀢 𑀞च𑀪𑀳च𑀳𑀫𑀢',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 𑀠च𑀳न 𑀣चन𑀞𑁦 𑀣च तनपच𑀪 𑀳न𑀟 𑀫चन𑀪च 𑀠𑀢ल𑀢णच𑀟 र च 𑀣न𑀟𑀢णच',\n 'णच𑀱च𑀢पच𑀪 𑀳𑀫च𑀟 𑀳𑀫𑀢𑀳𑀫च च प𑀳च𑀞च𑀟𑀢𑀟 गणच𑀟 𑀠चपच',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच𑀕 चलच𑀠𑁣𑀠𑀢𑀟 तनपच𑀪 प𑀳च𑀟च𑀟𑀢𑀟 𑀣च𑀠न𑀱च',\n '𑀠𑁦 णच 𑀳च च𑀞𑁦 𑀳च𑀠न𑀟 𑀞च𑀪न𑀱च𑀪 𑀠नपच𑀟𑁦𑀟 𑀣च ढच 𑀳च त𑀢𑀟 𑀟च𑀠च𑀡',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच𑀕 ढचणच𑀟च𑀢 𑀞च𑀟 𑀠चबन𑀟बन𑀟च𑀟 तनपच𑀪 𑀞च𑀟𑀳च',\n 'च𑀠𑀳𑁣𑀳𑀫𑀢𑀟 पच𑀞च𑀪𑀣न𑀟𑀞न𑀕 चढ𑀢𑀟 𑀣च 𑀞𑁦 𑀘च𑀱𑁣 𑀫च𑁥𑁣',\n 'चढ𑀢𑀟 𑀣च 𑀞𑁦 𑀫च𑀢𑀲च𑀪 𑀣च तनपच𑀪 𑀲च𑀪𑀲चञ𑀢णच 𑀣च णच𑀣𑀣च च𑀞𑁦 𑀠चबच𑀟त𑁦 पच',\n 'चढन 𑀫नञन 𑀣च 𑀠च𑀳च𑀟च 𑀳न𑀞च बच𑀟𑁣 बच𑀠𑁦 𑀣च तनपच𑀪 𑀞𑀢ञ𑀢𑀠𑁦𑀱च',\n 'णच𑀣𑀣च 𑀤च 𑀞च बच𑀟𑁣 𑀞𑁣 𑀞च𑀟च 𑀣च 𑀞च𑀟𑀳च𑀪 𑀫च𑀟𑀘𑀢',\n '𑀠च𑀳न 𑀲च𑀠च 𑀣च पच𑀪𑀢𑀟 𑀲न𑀞च 𑀳न𑀟 𑀪चबन ष 𑀱𑀫𑁣',\n 'च𑀠𑀢𑀟त𑁦𑀱च 𑀣च चललन𑀪च𑀪 𑀪𑀢बचष𑀞च𑀲𑀢 𑀟च 𑀪चबन𑀱च च 𑀣न𑀟𑀢णच',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 𑀠𑁦 णच 𑀳च तनपच𑀪 𑀞𑁣𑀪𑁣𑀟च पच 𑀠चणच𑀪 𑀣च 𑁥𑁦𑀪चण𑁦 𑀠नबचण𑁦𑀡',\n 'णच𑀣𑀣च तनपच𑀪 𑀳𑀢𑀞𑀢लच पच 𑀫च𑀟च च𑀢𑀳𑀫च चन𑀪𑁦',\n '𑀫च𑀟ण𑁣ण𑀢 𑀖 𑀟च 𑀞च𑀪𑁦 𑀞च𑀢 𑀣चबच 𑀞च𑀠न𑀱च 𑀣च त𑀢𑀱𑁣𑀟 𑀞𑁣𑀣च',\n 'च𑀟𑀟𑁣ढच𑀪 त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳 पच ण𑀢 𑀞𑀢𑀳च𑀟 𑀲च𑀪𑀞𑁣 च 𑀱च𑀘𑁦𑀟 त𑀫𑀢𑀟च',\n 'ग𑀠नपन𑀠 ञचणच त𑀢𑀞𑀢𑀟 𑀫नञन 𑀣च 𑀞𑁦 𑀣च त𑀢𑀱𑁣𑀟 𑀳नबच ढच 𑀳न 𑀳च𑀠न𑀟 पच𑀢𑀠च𑀞𑁣𑀟 𑀣च णच 𑀣चत𑁦भ',\n '𑀳𑀢𑀟च𑀣च𑀪𑀢𑀟 𑀫च𑀣च ल𑁦𑀠𑁣 𑀟च 𑀣च𑀟 प𑀳च𑀠𑀢 णच 𑀙𑀞च𑀳𑀫𑁦 𑀠नपन𑀠 ब𑁣𑀠च च 𑀞च𑀟𑁣𑀙',\n '𑀙णच𑀣𑀣च 𑀠चलच𑀪𑀢णच पच 𑀞च𑀳𑀫𑁦 𑀠𑀢𑀟 𑀙णचपच 𑀠च𑀢 𑀳𑀫𑁦𑀞च𑀪च 𑀭𑀗𑀙',\n '𑀟च𑀘𑁦𑀪𑀢णच𑀕 ल𑀢𑀞𑀢प𑁣त𑀢 𑀟च पन𑀪न𑀪न𑀱च𑀪 ढच𑀪𑀢𑀟 झच𑀳च𑀪 च𑀟च प𑀳च𑀞च 𑀣च णच𑀘𑀢𑀟 च𑀢𑀞𑀢',\n 'चलच𑀠𑁣𑀠𑀢 र 𑀟च 𑀞च𑀟𑀳च𑀪 ढच𑀞𑀢𑀟 𑀠च𑀫च𑀢𑀲च',\n 'णच𑀣𑀣च पन𑀟च𑀟𑀢 𑀠च𑀪च 𑀞णचन 𑀞𑁦 𑀳च𑀟णच 𑀠नपच𑀟𑁦 𑀳𑀫𑀢बच ढचलच𑀙𑀢',\n '𑀠नपन𑀠𑀢𑀟 𑀣च णच 𑀳𑀫च𑀲𑁦 𑀳𑀫𑁦𑀞च𑀪न 𑀖𑀧 ढच𑀢 ण𑀢 𑀱च𑀟𑀞च ढच णच 𑀪च𑀳न',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 𑀳𑀫𑀢𑀟 𑀠𑁦 𑀪च𑀳𑀫𑀢𑀟 𑀘𑀢𑀟 ञच𑀟𑀣च𑀟𑁣 𑀞𑁣 झच𑀠𑀳𑀫𑀢 𑀞𑁦 𑀟न𑀲𑀢𑀡',\n '𑀠चत𑁦ष𑀠चत𑁦𑀟 𑀞च𑀟𑁣𑀕 च𑀢𑀞𑀢𑀟 𑀣च 𑀞𑀱च𑀪च𑀪𑀪न𑀟 ल𑀢𑀞𑀢प𑁣त𑀢𑀟 चढन𑀘च 𑀤च 𑀳न ण𑀢 च 𑀞च𑀟𑁣',\n '𑀱चपच ढच𑀞न𑀱च𑀪 तनपच पच ढनललच च 𑀘𑀢𑀫च𑀪 ढ𑁦𑀟न𑁦',\n 'णच𑀣𑀣च च𑀟𑁣ढच𑀪 𑀣च पच 𑀲च𑀪न 𑀳𑀫𑁦𑀞च𑀪न 𑀬𑀧𑀧 𑀣च 𑀳न𑀞च 𑀱नत𑁦 𑀞𑁦 𑀳𑀫च𑀲च𑀪 लच𑀲𑀢णच𑀪𑀞न',\n '𑀠चपच𑀪 𑀣च पच 𑀳चनणच 𑀪चणन𑀱च 𑀳चढ𑁣𑀣च 𑀤𑀢णच𑀪च𑀪 𑀣च पच 𑀞च𑀢 𑀠चपचपपच𑀪च𑀪 𑀞नपच𑀪𑁦',\n '𑀠च𑀙च𑀢𑀞चपच𑀟 लच𑀲𑀢णच 𑀟च 𑀲न𑀳𑀞च𑀟पच𑀪 ढच𑀪च𑀤च𑀟च𑀪 𑀞𑀢𑀳च 𑀣चबच 𑀙णच𑀟 ढ𑀢𑀟𑀣𑀢बच',\n '𑀟च𑀘𑁦𑀪𑀢णच𑀕 च𑀟 बच𑀟𑁣 𑀱च𑀣च𑀟𑀣च 𑀳न𑀞च ण𑀢 𑀫नल𑀣च 𑀣च 𑀠च𑀢 त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳',\n '𑀠च𑀳च𑀟च 𑀞𑀢𑀠𑀢णणच 𑀟च 𑀫च𑀳च𑀳𑀫𑁦𑀟 𑁥च𑀪𑀞𑁦𑀱च𑀪 च𑀟𑀟𑁣ढच𑀪 𑀞𑀱चलच𑀪च 𑀣चबच 𑀳च𑀪च𑀪𑀢𑀟 𑀳च𑀠च𑀟𑀢णच',\n 'चढ𑀢𑀟त𑀢 ढ𑀢णच𑀪 𑀠च𑀳न बनढच 𑀣च 𑀞च 𑀢णच 𑀞𑀢𑀳च',\n 'णच𑀪च 𑀲𑀢ण𑁦 𑀣च 𑀣नढन ठ𑀧 𑀟𑁦 𑀳न𑀞च 𑀞च𑀠न 𑀣च तनपच𑀪 𑀫𑀢ख च 𑀟च𑀘𑁦𑀪𑀢णच च ढच𑀪चष न𑀟𑀢त𑁦𑀲',\n '𑀙𑀢𑀟च 𑀳𑀫च𑀟 𑀠चबच𑀟𑀢𑀟 त𑀢𑀱𑁣𑀟 𑀞𑁣𑀣च 𑀟च 𑀟𑀰𑀖𑀦𑀧𑀧𑀧 च 𑀞𑁣𑀱चत𑁦 𑀪च𑀟च𑀙',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 𑀱च𑀟𑀟𑁦 𑀳𑀫𑀢𑀪𑀢 𑀟च𑀘𑁦𑀪𑀢णच पच ण𑀢 𑀣𑁣𑀟 𑀣च𑀞𑀢ल𑁦 तनपच𑀪𑀡',\n 'च𑀟च 𑀘चष𑀢𑀟ष𑀘च प𑀳च𑀞च𑀟𑀢𑀟 𑀠च𑀳न ञ𑁣𑀪𑀢𑀟 बच𑀪बच𑀘𑀢णच 𑀣च 𑀟च 𑀤च𑀠च𑀟𑀢 च 𑀘𑀢बच𑀱च',\n 'णच𑀣𑀣च च𑀞च ण𑀢 𑀞न𑀳𑀞न𑀪𑁦𑀟 𑀳च𑀟च𑀪 𑀣च 𑀠नपन𑀱च𑀪 𑀠नपन𑀠 चल𑀫चल𑀢 𑀣च 𑀪च𑀟𑀳च',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳𑀕 𑀠𑁦 णच 𑀳च तनपच𑀪 पच 𑀲𑀢 𑀞च𑀳𑀫𑁦 ढच𑀞च𑀞𑁦𑀟 𑀲चपच च च𑀠न𑀪𑀞च𑀡',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच𑀕 णच𑀣𑀣च च𑀞𑁦 𑀞च𑀪𑁦 𑀞च𑀢 𑀣च णच𑀪च 𑀣चबच तनपच𑀪 𑀞णच𑀟𑀣च𑀪 ढ𑀢𑀪𑀢',\n 'ढचणच𑀟𑀢𑀟 𑀣𑀪 𑀟च𑀳𑀢𑀪न 𑀳च𑀟𑀢 ब𑀱च𑀪𑀤𑁣 𑀞च𑀟 तनपच𑀪 लच𑀳𑀳च',\n 'णच𑀣𑀣च च𑀞𑁦 प𑀳च𑀟ब𑀱च𑀠च𑀪 𑀠च𑀳न तनपच𑀪 𑀣च 𑀞𑁦 𑀘𑀢𑀪𑀞𑀢पच 𑀫चल𑀢पपच च 𑀟च𑀘𑁦𑀪𑀢णच',\n 'त𑀫𑀪𑀢𑀳प𑀢च𑀟 𑁦𑀪𑀢𑀞𑀳𑁦𑀟𑀕 चढ𑀢𑀟 𑀣च 𑀞𑁦 𑀘च𑀱𑁣 ढनबच𑀱च𑀪 𑀤नत𑀢णच',\n 'णचन𑀳𑀫𑁦 𑀠च𑀙च𑀢𑀞चपच𑀟 लच𑀲𑀢णच 𑀤च 𑀳न 𑀣च𑀢𑀟च 𑀳𑀫𑀢बच णच𑀘𑀢𑀟 च𑀢𑀞𑀢𑀡',\n 'प𑀳च𑀲पच𑀕 𑀳𑀫𑀢𑀟 𑀞𑀢त𑀢𑀟 ञ𑀢𑀟𑀞न णच 𑀲𑀢 ढच𑀟ञच𑀞𑀢 णच𑀱च𑀟 झ𑀱चण𑁣ण𑀢𑀟 तनपच𑀡',\n 'त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳 पच 𑀞च𑀠च 𑀠नपन𑀠 थठ त𑀢𑀞𑀢𑀟 𑀳च𑀙𑁣𑀙𑀢 ठर च 𑀞च𑀟𑁣',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच𑀕 चढनढन𑀱च𑀟 𑀣च 𑀞न𑀞𑁦 ढनझचपच𑀪 𑀳च𑀟𑀢 𑀞च𑀟 त𑀢𑀱𑁣𑀟 𑀳नबच',\n '𑀪च𑀱च𑀪 𑀣च 𑀞न𑀟ब𑀢ण𑁣ण𑀢 𑀞𑁦 पच𑀞च𑀱च 𑀱च𑀘𑁦𑀟 पच𑀢𑀠च𑀞च 𑀱च 𑀠च𑀳न लच𑀪न𑀪च𑀪 च𑀠𑁣𑀳च𑀟𑀢𑀟 𑀘𑀢𑀟𑀢',\n 'लच𑀲𑀢णच 𑀤𑀢𑀟च𑀪𑀢णच𑀕 𑀫चल𑀢𑀟 𑀣च 𑀠च𑀙च𑀢𑀞चपच𑀟 लच𑀲𑀢णच 𑀞𑁦 त𑀢𑀞𑀢 च च𑀟𑀟𑁣ढच𑀪 तनपच𑀪 𑀞𑁣𑀪𑁣𑀟च',\n '𑀞न𑀟 𑀢णच 𑀠चबच𑀟च𑀪 𑀞न𑀪च𑀠𑁦𑀡 𑀱चपच ल𑀢𑀞𑀢पच पच त𑁦 णच𑀟च 𑀣च 𑀞णचन 𑀞न 𑀞𑁣णच',\n '𑀙𑀱च𑀳न 𑀠च𑀤च𑀟 ढच 𑀳च 𑀫च𑀢𑀫न𑀱च 𑀞𑁣 𑀣च 𑀞न𑀱च च𑀟 बच 𑀠चपच𑀟𑀳न 𑀣च त𑀢𑀞𑀢भ',\n 'च𑀟च 𑀤न𑀤नपच त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳 𑀣𑁣𑀟 च 𑀳चन𑀞𑁦 प𑀪न𑀠ध ड 𑀳𑀫नबचढच𑀟 𑀠च𑀙च𑀢𑀞चपच𑀟 𑀱𑀫𑀢प𑁦 𑀫𑁣न𑀳𑁦',\n '𑀤च च ढन𑀣𑁦 𑀳चढ𑁣𑀟 च𑀳𑀢ढ𑀢प𑀢𑀟 त𑁣𑀪𑁣𑀟चख𑀢𑀪न𑀳 च त𑀫𑀢𑀟च',\n '𑀠नपन𑀠 𑀟च ढ𑀢णन 𑀣च णच 𑀱च𑀪𑀞𑁦 𑀣चबच तनपच𑀪 𑀫𑀢ख च 𑀣न𑀟𑀢णच',\n '𑀫𑁣पन𑀟च𑀕 𑀙णच𑀟 ध𑀣ध 𑀟च 𑀤च𑀟बचष𑀤च𑀟बच𑀪 च𑀣च𑀱च 𑀣च 𑀫न𑀞न𑀟त𑀢𑀟 𑀞𑁣पन𑀟 𑀞𑁣ल𑀢 𑀞च𑀟 𑀤चढ𑁦𑀟 𑀢𑀠𑁣',\n '𑀞𑀱चल𑁦ष𑀞𑀱चल𑁦 णच 𑀞𑀢𑀲𑁦 𑀣च 𑀘च𑀠𑀢𑀙च𑀟 𑀢𑀟𑁦त च 𑀞𑁣ब𑀢',\n 'ढन𑀫च𑀪𑀢 णच 𑀳𑀫न𑀪𑁦 𑀠च𑀳न 𑀟न𑀟च 𑀣च𑀠न𑀱च𑀪 त𑁦𑀱च ढच च बच𑀟𑀢𑀟 𑀳च च 𑀞च𑀠𑀲𑁦 ञ𑀢𑀟 प𑀢𑀟नढन',\n '𑀠च𑀠𑀠च𑀟 𑀣चन𑀪च𑀕 चढन 𑀭𑀧 𑀣च णच 𑀳𑀫च𑀢𑀣च 𑀱च ढढत𑀫चन𑀳च',\n '𑀙𑀠न𑀟च 𑀞𑁣𑀞च𑀪𑀢𑀟 𑀳चण𑁦 𑀤नत𑀢णच𑀪 𑀱𑀢𑀞𑁦 𑀣च 𑀳चन𑀪च𑀟 ब𑀱च𑀠𑀟𑁣𑀟𑀢𑀟 ध𑀣ध𑀙',\n '𑀙𑀫च𑀪 णच𑀟𑀤न चधत ढच पच त𑀢𑀞च 𑀘च𑀠𑀙𑀢णणच ढच𑀙',\n '𑀘च𑀠𑀙𑀢णणच𑀪 चधत पच 𑀱च𑀪𑁦 𑀱च णच𑀟𑀞𑀢𑀟 च𑀪𑁦𑀱च 𑀠नझच𑀠𑀢𑀟 𑀳𑀫नबचढच𑀟त𑀢𑀟 𑀘च𑀠𑀙𑀢णणच𑀪 ष 𑁦लष𑀪न𑀲च𑀢',\n '𑀪चढ𑀢𑀙न 𑀞𑀱च𑀟𑀞𑀱च𑀳𑁣𑀕 ब𑀱च𑀠𑀟चप𑀢𑀟 ढन𑀫च𑀪𑀢 ढच पच 𑀣च𑀠न 𑀣च 𑀫चल𑀢𑀟 𑀣च 𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀞𑁦 त𑀢𑀞𑀢 ढच',\n '𑀞𑁣पन𑀟 𑀞𑁣ल𑀢 पच 𑀳𑁣𑀞𑁦 𑀤चढ𑁦𑀟 ब𑀱च𑀠𑀟च𑀟 ढचण𑁦ल𑀳च 𑀟च चधत',\n 'चढ𑀢𑀟 𑀣च 𑀞𑁦 𑀲च𑀪न𑀱च च 𑀘𑀢𑀫𑁣𑀫𑀢𑀟 𑀢बढ𑁣 णच ण𑀢 𑀞च𑀠च 𑀣च णच𑀣𑀣च ढ𑁣𑀞𑁣 𑀫च𑀪च𑀠 पच 𑀲च𑀪च ष 𑀠च𑀳च𑀟च',\n 'णच𑀣𑀣च च𑀞च ण𑀢 𑀟च त𑀢 𑀤चढ𑁦𑀟 𑀲𑀢पच𑀪 𑀣च ब𑀱च𑀟𑀢 𑀢𑀟च 𑀫च𑀟𑀟न𑀟 गणच𑀟 ढ𑀢𑀟𑀣𑀢बच ड 𑀳च𑀣𑀢𑀜 च𑀟ब𑁣 चढ𑀣नललच𑀫𑀢',\n '𑀠न𑀟 ञ𑀢𑀟𑀞𑁦 𑁥च𑀪च𑀞च𑀪 ध𑀣ध च 𑀞च𑀟𑁣 ष 𑀳𑀫𑁦𑀞च𑀪चन',\n 'णच𑀞नढन 𑀣𑁣बच𑀪च𑀕 𑀣चल𑀢लच𑀟 𑀣च 𑀳न𑀞च 𑀳च 𑀟𑀢 ढच𑀪𑀢𑀟 𑀘च𑀠𑀙𑀢णणच𑀪 ध𑀣ध',\n '𑀳नढनलष𑀣चषढच𑀞च𑀟 प𑀢𑀟नढन णच 𑀫च𑀢𑀲च𑀪 𑀣च त𑁦षत𑁦ष𑀞नषत𑁦 च 𑀟च𑀘𑁦𑀪𑀢णच',\n 'प𑀳नबन𑀟𑁦 ढच पच 𑀞च𑀪𑁦 ढच 𑀞च𑀟 𑀟च𑀣𑀢𑀟 𑀠न𑀞च𑀠च𑀢 च 𑀠च𑀘चल𑀢𑀳च𑀪 𑀟च𑀘𑁦𑀪𑀢णच',\n '𑀙च𑀞𑀱च𑀢 𑀘𑀢पन𑀱च प𑀳च𑀞च𑀟𑀢𑀟च 𑀣च पच𑀠ढन𑀱चल 𑀣च 𑀳च𑀪च𑀞𑀢 𑀞च𑀟 पच𑀞च𑀪च𑀪 𑀳𑀫नबचढच𑀟 झच𑀳च𑀙',\n '𑀫𑁣पन𑀟च𑀟 ढ𑀢𑀞𑀢𑀟 त𑀢𑀞च 𑀳𑀫𑁦𑀞च𑀪च 𑀖थ 𑀣च 𑀳च𑀠न𑀟 𑀙णच𑀟त𑀢𑀟 𑀞च𑀟 𑀟च𑀘𑁦𑀪𑀢णच',\n '𑀪च𑀳𑀫च पच च𑀠𑀢𑀟त𑁦 च 𑀤𑀢णच𑀪त𑀢 पच𑀳𑀫च𑀪 𑀟न𑀞𑀢ल𑀢णच पच 𑀤चध𑁣𑀪𑀢𑀤𑀫𑀤𑀫𑀢च',\n '𑀠𑁦 𑀞𑁦 𑀲च𑀪न𑀱च च 𑀞च𑀳𑀫𑀠𑀢𑀪 𑀞न𑀠च 𑀠𑁦 णच 𑀳च च𑀞च 𑀣च𑀠न 𑀣च णच𑀟𑀞𑀢𑀟𑀡',\n '𑀪𑀢𑀞𑀢त𑀢𑀟 चधत𑀕 𑁥च𑀟बच𑀪𑁦𑀟 𑀳𑀫𑁦𑀞च𑀪चन णच ण𑀢 𑀲चपचल𑀢 𑀣च 𑀳नल𑀫न𑀟 न𑀱च𑀪 𑀘च𑀠𑀙𑀢णणच𑀪 𑀞च𑀟 𑀪𑀢𑀞𑀢त𑀢𑀟 𑀞च𑀟𑁣',\n 'चप𑀢𑀞न चढनढच𑀞च𑀪𑀕 𑀠न𑀞च𑀠𑀢𑀟 𑀳𑀫नबचढच𑀟 𑀞च𑀳च ढच णच ढन𑀞चपच𑀪 𑀠च𑀢 𑀘च𑀟ष𑀞च𑀲च 𑀱च𑀘𑁦𑀟 𑀤चढ𑁦𑀟 𑀠चपच𑀢𑀠च𑀞𑀢𑀟𑀳च',\n 'चप𑀢𑀞न चढनढच𑀞च𑀪𑀕 छच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀟𑁦 𑀞𑁦 𑀳𑁣𑀟 𑀟च प𑀳चणच पच𑀞च𑀪च च ठ𑀧ठ𑀰',\n 'छच𑀟 च𑀣च𑀱च 𑀞𑁦 णचञच 𑀘𑀢पचष𑀘𑀢पच𑀪 𑁥च𑀪च𑀞च प𑀳च𑀞च𑀟𑀢𑀟 ढन𑀫च𑀪𑀢 𑀣च प𑀢𑀟नढन ष 𑀲च𑀣च𑀪 𑀳𑀫नबचढच𑀟 झच𑀳च',\n 'चधत 𑀤च पच त𑀢 𑀤चढ𑁦𑀟 ठ𑀧ठ𑀰 𑀣न𑀞 𑀣च 𑀠चप𑀳चलच𑀪 प𑀳च𑀪𑁣 च 𑀟च𑀘𑁦𑀪𑀢णच ष चढ𑀣नललच𑀫𑀢 च𑀣च𑀠न',\n 'धचप𑀪𑀢त𑀢च 𑁦पप𑁦𑀫𑀕 𑁦𑀲तत पच 𑀞च𑀠च प𑀳𑁣𑀫न𑀱च𑀪 𑀳𑀫नबचढच𑀪 𑀠च𑀘चल𑀢𑀳च𑀪 𑀱च𑀞𑀢लच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀞च𑀟 𑀤च𑀪ब𑀢𑀟 𑀪च𑀳𑀫च𑀱च',\n 'णचन𑀳𑀫𑁦 𑀪𑀢𑀞𑀢त𑀢𑀟 त𑀢𑀞𑀢𑀟 ब𑀢𑀣च 𑀤च𑀢 झच𑀪𑁦 च 𑀘च𑀠भ𑀢णणच𑀪 चधत𑀡',\n 'ब𑀱च𑀠𑀟𑁣𑀟𑀢 च𑀪𑁦𑀱च 𑀳न𑀟 𑀲𑀢 𑀟च 𑀞न𑀣न 𑀫न𑀘𑀘च 𑀞च𑀟 पच𑀞च𑀪च𑀪 𑀳𑀫नबचढच𑀟 𑀞च𑀳च 𑀟च ठ𑀧ठ𑀰 ष 𑀠च𑀳च𑀟च',\n 'च𑀛ढ𑀢𑀟 𑀣च णच 𑀘च𑀱𑁣 𑀲चञच प𑀳च𑀞च𑀟𑀢𑀟 𑀠न𑀪पचलच बच𑀪𑁣 𑀣च च𑀣𑁣 𑀣𑁣बन𑀱च',\n '𑀠च𑀟णच𑀟 𑀙णच𑀟 𑀳𑀢णच𑀳च 𑀣च 𑀞𑁦 𑀲च𑀲चपच𑀱च च 𑀤चढ𑁦𑀟 𑀲𑀢पच𑀪 𑀣च ब𑀱च𑀟𑀢 𑀟च पच𑀞च𑀪च𑀪 𑀳𑀫नबचढच𑀟 𑀞च𑀳च च 𑀘च𑀠𑀙𑀢णणच𑀪 चधत 𑀟च ठ𑀧ठ𑀰',\n 'चधत 𑀟च पच𑀟पच𑀟त𑁦 𑁣𑀳𑀢𑀟ढच𑀘𑁣𑀦 लच𑀱च𑀟𑀦 ढ𑁦लल𑁣 𑀣च 𑀳चन𑀪च𑀟 𑀠च𑀳न 𑀟𑁦𑀠च𑀟 पच𑀞च𑀪च𑀪 𑀳𑀫नबचढच𑀟 𑀟च𑀘𑁦𑀪𑀢णच',\n '𑀞𑀱च𑀟𑀞𑀱च𑀳𑁣 ढच 𑀳𑀫𑀢 𑀣च चल𑀞𑀢ढलच ड बच𑀟𑀣न𑀘𑁦',\n '𑀪च𑀙चण𑀢 𑀪𑀢बच𑀕 णच𑀣𑀣च 𑀠चपच𑀞𑀢𑀟 च𑀢𑀞च 𑀳च𑀞च𑀠च𑀞𑁣𑀟 𑀤च𑁥𑁦 पच 𑀢𑀟पच𑀟𑁦प 𑀞𑁦 𑀘च𑀱𑁣 त𑁦त𑁦𑀞नत𑁦 च 𑀟च𑀘𑁦𑀪𑀢णच',\n 'ब𑀱च𑀠𑀟चप𑀢𑀟 𑀤च𑀠𑀲च𑀪च पच 𑀪न𑀲𑁦 𑀞च𑀲𑁣𑀲𑀢𑀟 णचञच लचढच𑀪च𑀢 𑀳𑀫𑀢𑀣च च 𑀘𑀢𑀫च𑀪',\n '𑀙𑀠चपच𑀞च𑀢 𑀫न𑀣न 𑀣च 𑀟𑀟धध 𑀤च पच 𑀣चन𑀞च 𑀣𑁣𑀟 𑀞च𑀱𑁣 प𑀳च𑀪𑁣 च 𑀘𑀢𑀫च𑀪 𑀤च𑀠𑀲च𑀪च𑀙',\n 'ग𑀫चञच ञच𑀟 पच𑀞च𑀪च 𑀣च 𑀠चपच𑀢𑀠च𑀞𑀢 𑀠न𑀳नल𑀠च𑀢 च 𑀟च𑀘𑁦𑀪𑀢णच णच 𑀳चढच 𑀱च 𑀠न𑀳नलन𑀟त𑀢भ',\n 'ढच 𑀟च 𑀳𑁣𑀟 लच𑀞चढ𑀢𑀟 𑀠चपच𑀪 𑀳𑀫नबचढच𑀟 𑀞च𑀳च ष च𑀢𑀳𑀫च ढन𑀫च𑀪𑀢',\n 'बच𑀟𑁦 𑀠च𑀟𑀢 𑀫च𑀟णच𑀕 𑀫𑀢𑀪च 𑀣च चढ𑀣नललच𑀫𑀢 च𑀣च𑀠न 𑀳चढ𑁣𑀟 𑀳𑀫नबचढच𑀟 चधत',\n '𑀤चढ𑁦𑀟 ठ𑀧ठ𑀰𑀕 𑀞𑀱च𑀟𑀞𑀱च𑀳𑁣 णच त𑁦 𑀳न𑀟च पचपपचन𑀟च𑀱च 𑀣च ध𑁦प𑁦𑀪 𑁣ढ𑀢',\n 'प𑀛चझच𑀣𑀣च𑀠च पच 𑁥च𑀪𑀞𑁦 च चधत ढचणच𑀟 𑀲𑀢पच𑀪 𑀣च छच𑀟 𑀞𑀱च𑀠𑀢प𑀢𑀟 णचझ𑀢𑀟 𑀤च𑁥𑁦𑀟 ठ𑀧ठ𑀰',\n 'णच𑀣𑀣च ढचपन𑀟 𑀪चढच 𑀞चप𑀢𑀟 𑀤चढ𑁦 𑀞𑁦 𑀘च𑀟 𑀫च𑀟𑀞चल𑀢𑀟 गणच𑀟 𑀟च𑀘𑁦𑀪𑀢णच',\n 'ढ𑁣लच प𑀢𑀟नढन𑀕 𑀣चल𑀢लच𑀟 𑀣च 𑀳न𑀞च 𑀳च 𑀳𑀢णच𑀳च𑀪 प𑀳𑁣𑀫𑁣𑀟 ब𑀱च𑀠𑀟च𑀟 लचब𑁣𑀳 पच 𑀳𑀫च ढच𑀠ढच𑀟 𑀣च पच 𑀳चन𑀪च𑀟 छच𑀟 𑀳𑀢णच𑀳च',\n '𑀞𑁣 𑀠𑁦 णच 𑀳च 𑀱च𑀳न ढच च बच𑀟𑀢𑀟 झ𑀱च𑀤𑁣𑀟 ब𑀱च𑀠𑀟चप𑀢𑀟 𑀳𑀫नबचढच ढन𑀫च𑀪𑀢𑀡',\n 'ग𑀞न 𑀞च𑀪ढ𑀢 𑀞न𑀣𑀢 𑀢𑀣च𑀟 च𑀟 ढच 𑀞न च ल𑁣𑀞चत𑀢𑀟 𑀤चढ𑁦 च𑀠𑀠च 𑀞न 𑀤चढ𑀢 तच𑀟तच𑀟पचभ',\n '𑀢𑀛𑀟𑁦त पच 𑀲𑀢पच𑀪 𑀣च 𑀘𑁦𑀪𑀢𑀟 𑀳न𑀟चण𑁦𑀟 छच𑀟 पच𑀞च𑀪च𑀪 𑀳𑀫नबचढच𑀟 झच𑀳च 𑀟च ठ𑀧ठ𑀰 𑀣च छच𑀟 𑀠च𑀘चल𑀢𑀳च𑀪 𑀣𑁣𑀞𑁣𑀞𑀢',\n '𑀤च𑀟 𑀲𑀢 ढच𑀢 𑀱च 𑀲च𑀟𑀟𑀢𑀟 𑀢ल𑀢𑀠𑀢 𑀠न𑀫𑀢𑀠𑀠च𑀟त𑀢 च 𑀘𑀢𑀫च𑀪 ण𑁣ढ𑁦 ष 𑀣च𑀟 पच𑀞च𑀪च𑀪 ध𑀣ध',\n '𑀙𑀠न𑀟 ण𑀢 𑀠न𑀪𑀟च 𑀣च ढन𑀫च𑀪𑀢 णच च𑀠𑀢𑀟त𑁦 𑀣च 𑀣𑁣𑀞च𑀪 𑀤च𑁥𑁦𑀟 ठ𑀧ठ𑀰𑀙',\n '𑀞𑁣पन पच त𑁦 𑀠न𑀫च𑀠𑀠च𑀣 चढचत𑀫च 𑀟𑁦 𑀣च𑀟 पच𑀞च𑀪च𑀪 ब𑀱च𑀠𑀟च𑀟 𑀞च𑀟𑁣 च ध𑀣ध',\n '𑀪चढ𑀢𑀙न 𑀞𑀱च𑀟𑀞𑀱च𑀳𑁣𑀕 प𑀳𑁣𑀫𑁣𑀟 ब𑀱च𑀠𑀟च𑀟 𑀞च𑀟𑁣 णच ढच𑀪 ध𑀣ध णच 𑀞𑁣𑀠च 𑀘च𑀠𑀙𑀢णणच𑀪 𑀟𑀟धध',\n '𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀟च तचततच𑀞च𑀪 𑀲𑁦𑀠𑀢 𑀲च𑀟𑀢ष𑀞चण𑁣𑀣𑁦 𑀞च𑀟 𑀞𑁣𑀠च𑀱च चधत',\n 'झन𑀳𑁣𑀳𑀫𑀢𑀟 चधत 𑀳न𑀟 𑀘च 𑀣चबच प𑀳च𑀞च𑀟𑀢𑀟𑀳न 𑀣च बच𑀟𑀣न𑀘𑁦',\n 'चढन 𑀭𑀧 𑀣च च𑀞च च𑀠ढचप𑁣 च 𑀤च𑀠च𑀟 𑀘𑀢𑀟 ढच𑀫च𑀳𑀢𑀟 ण𑀢 𑀱च 𑀞न𑀟𑀣𑀢𑀟 प𑀳च𑀪𑀢𑀟 𑀠नल𑀞𑀢𑀟 𑀟च𑀘𑁦𑀪𑀢णच बणच𑀪च𑀟 𑀲न𑀳𑀞च',\n '𑀪न𑀟𑀣न𑀟च𑀪 𑀳𑁣𑀘𑀢𑀟 𑀟च𑀘𑁦𑀪𑀢णच पच ण𑀢 𑀱चप𑀳𑀢 𑀣च 𑀤च𑀪ब𑀢𑀟 त𑀢𑀟 𑀫च𑀟त𑀢 𑀣च 𑀱च𑀳न 𑀘च𑀠𑀢भच𑀟पच 𑀞𑁦 ण𑀢 𑀞च𑀟 𑀠च𑀟णच𑀟𑀳न',\n 'ढच 𑀫चल𑀢𑀟च ढच 𑀟𑁦 𑀞न णच𑀲𑁦 𑀠𑀢𑀟 ष 𑀳च𑀟चपच चढढ𑁣',\n 'ब𑀱च𑀠𑀟𑁣𑀟𑀢𑀟 ध𑀣ध 𑀳न𑀟 𑀳𑁣𑀞𑀢 ढन𑀫च𑀪𑀢 𑀞च𑀟 𑀙𑀠चणच𑀪 𑀣च 𑀲च𑀣च𑀪 𑀳𑀫नबचढच𑀟 झच𑀳च 𑀫𑁦𑀣𑀢𑀞𑀱चपच𑀪 चधत𑀙',\n '𑀠न𑀙च𑀤न 𑀠चबच𑀘𑀢𑀕 𑀙णच𑀟 𑀳च𑀟𑀣च 𑀳न𑀟 𑀞च𑀠च प𑀳𑁣𑀫𑁣𑀟 𑀞𑀱च𑀠𑀢𑀳𑀫𑀢𑀟च𑀟 चणणन𑀞च 𑀟च 𑀞च𑀟𑁣',\n 'चढनढन𑀱च ढ𑀢णच𑀪 𑀣च ब𑀱च𑀠𑀟चप𑀢𑀟 च𑀠न𑀪𑀞च पच त𑁦 बच𑀠𑁦 𑀣च 𑀠चप𑀳चल𑁣ल𑀢𑀟 𑀟च𑀘𑁦𑀪𑀢णच',\n 'बच 𑀘𑁦𑀪𑀢𑀟 𑀙णच𑀟 पच𑀞च𑀪च𑀪 𑀳𑀫नबचढच𑀟 झच𑀳च च 𑀟च𑀘𑁦𑀪𑀢णच 𑀟च ठ𑀧ठ𑀰',\n 'ढनढच बचलच𑀣𑀢𑀠च𑀕 𑀙ढच𑀟 𑀳च𑀟 ल𑁣𑀞चत𑀢𑀟 𑀣च च𑀞च 𑀫च𑀢𑀲𑁦 𑀟𑀢 ढच𑀙',\n 'ध𑀣ध त𑁦 न𑀱च𑀪 𑀳𑀢णच𑀳च 𑀢𑀟च 𑀟च𑀟 च 𑀘च𑀠𑀙𑀢णणच𑀪 ष णच𑀞नढन 𑀠𑁣𑀫च𑀠𑀠𑁦𑀣',\n '𑀤च𑁥𑁦𑀟 च𑀟च𑀠ढ𑀪च𑀕 पच𑀪𑀢𑀫𑀢𑀟 𑀳चढ𑁣𑀟 ब𑀱च𑀠𑀟च त𑀫च𑀪ल𑁦𑀳 त𑀫न𑀞𑀱न𑀠च 𑀳𑁣लन𑀣𑁣',\n 'ढचनत𑀫𑀢𑀕 𑀞चन𑀪च 𑀟च 𑀤च𑀪ब𑀢𑀟 𑀢𑀳च णनबन𑀣च 𑀣च 𑀠𑀯च 𑀣च 𑀳चल𑀱च𑀟पच𑀪 𑀣च प𑀢𑀪𑀢ल𑀢णच𑀟 𑀣चणच',\n '𑀱च𑀟𑁦 𑀟𑁦 𑀳चढ𑁣𑀟 ब𑀱च𑀠𑀟च𑀟 ढचण𑁦ल𑀳च𑀦 𑀣𑁣नण𑁦 𑀣𑀢𑀪𑀢𑀡',\n '𑀤च 𑀠न 𑀲𑀢प𑁣 𑀣च 𑀳चढढ𑀢𑀟 𑀫च𑀟ण𑁣ण𑀢𑀟 𑀞न𑀣𑀢𑀟 𑀳𑀫𑀢बच बच 𑀞चप𑀳𑀢𑀟च ष 𑀟न𑀪च 𑀞𑀫चल𑀢ल',\n '𑀙𑀳𑀫𑀢𑀟 च𑀪𑀠च𑀳𑀫𑀢𑀟 𑀪च𑀟च𑀪 𑀣𑀢𑀠𑁣𑀞𑀪चञ𑀢णणच 𑀟च 𑀪चबन𑀱च 𑀟𑁦 च 𑀟च𑀘𑁦𑀪𑀢णच𑀡𑀙',\n 'ब𑀱च𑀠𑀟च च𑀯 च𑀯 𑀳नल𑁦𑀕 𑀞च𑀠𑀠चलच चणणन𑀞च𑀟 𑀣च 𑀟च 𑀲च𑀪च पच 𑀲𑀢 𑀠न𑀫𑀢𑀠𑀠च𑀟त𑀢 𑀳च𑀠च 𑀣च 𑀱च𑀙च𑀣𑀢 𑀟च ढ𑀢णन',\n 'च𑀫𑀠च𑀣 लच𑀱च𑀟 णच त𑁦 𑀠च𑀘चल𑀢𑀳च𑀪 𑀣चपपच𑀱च𑀟 𑀟च𑀘𑁦𑀪𑀢णच ढच 𑀙णच𑀪 च𑀠𑀳𑀫𑀢𑀟 𑀳𑀫चपच ढच त𑁦',\n '𑀤च𑁥𑁦𑀟 ब𑀱च𑀠𑀟च𑀟 𑁣𑀟𑀣𑁣𑀕 𑀳च𑀞च𑀠च𑀞𑁣𑀟 𑀤चढ𑁦𑀟 𑀞च𑀢षप𑀳चण𑁦',\n '𑀱च𑀢𑀱चण𑁦𑀕 𑀟च𑀘𑁦𑀪𑀢णच पच पन𑀪च 𑀙छच𑀟 𑀳च𑀟𑀣च 𑀳𑁣𑀠चल𑀢च 𑀣च 𑀳चत𑁦 𑀣चल𑀢ढच𑀟 𑀘च𑀠𑀢𑀙च𑀪 ब𑀪𑁦𑁦𑀟𑀲𑀢𑁦ल𑀣',\n '𑀳𑁣𑀞चधन𑀕 𑀠न𑀟च ढन𑀞चपच𑀪 च 𑀪चढच 𑀘𑀢𑀫च𑀪 𑀞च𑀣न𑀟च ब𑀢𑀣च ढ𑀢णन',\n '𑀟च𑀘𑁦𑀪𑀢णच𑀕 𑀠च𑀘चल𑀢𑀳च𑀪 𑀣𑁣𑀞𑁣𑀞𑀢𑀟 झच𑀳च𑀪 𑀟च 𑀳𑀫𑀢𑀪𑀢𑀟 𑀘च𑀟ण𑁦 ढचपन𑀟 𑀤च𑁥𑁦𑀟 छच𑀟 प𑀢𑀟झ𑁦',\n '𑀤च𑁥𑁦𑀟 ठ𑀧ठ𑀰𑀕 ढच 𑀳च𑀢 𑀠च𑀳न 𑀤च𑁥𑁦 𑀳न𑀟 प𑀳चणच 𑀘𑀢𑀪च𑀟 𑀞चणच𑀟 च𑀢𑀞𑀢 ढच 𑀱च𑀟𑀟च𑀟 𑀞च𑀪𑁣 ष 𑀢𑀟𑁦त',\n 'च𑀫𑀠𑁦𑀣 लच𑀱च𑀟 णच 𑀙𑀫च𑀪𑀤न𑀞च𑀙 𑀱च𑀳न 𑀙णच𑀟 𑀘च𑀠𑀙𑀢णणच𑀪𑀳च पच चधत',\n 'ध𑀪ध पच त𑁦 च 𑀳𑀫𑀢𑀪ण𑁦 पच𑀞𑁦 पच ण𑀢 ब𑁣बचणणच 𑀣च 𑀠च𑀟णच𑀟 𑀘च𑀠𑀙𑀢णणन𑀟 𑀟च𑀘𑁦𑀪𑀢णच च 𑀤चढ𑁦 𑀠च𑀢 𑀤न𑀱च',\n '𑀠𑁦 णच ढच𑀠ढच𑀟पच 𑀫च𑀪ब𑀢प𑀳𑀢𑀟 𑀠च𑀘चल𑀢𑀳च𑀪 च𑀠न𑀪𑀞च 𑀣च चढ𑀢𑀟 𑀣च णच 𑀳चढच 𑀲च𑀪न𑀱च च च𑀲𑀢𑀪𑀞च𑀡',\n '𑀞णचनपच𑀪 𑀠𑁣प𑁣त𑀢 𑀣च ढन𑀫च𑀪𑀢 णच ढच𑀢 𑀱च 𑀟𑀢𑀘च𑀪 पच 𑀙𑀫च𑀪𑀤नझच 𑀱च𑀳न 𑀙णच𑀟 𑀟च𑀘𑁦𑀪𑀢णच𑀙',\n '𑀤च𑁥𑁦𑀟 ठ𑀧ठ𑀰𑀕 𑀱च𑀙च𑀣𑀢𑀟 𑀞𑀱च𑀟च 𑀭𑀭𑀭 𑀢𑀟𑁦त पच ढच𑀢 𑀱च 𑀘च𑀠भ𑀢णणन 𑀳न 𑀠𑀢𑀞च 𑀳न𑀟चण𑁦𑀟 गणच𑀟 पच𑀞च𑀪च𑀪 𑀳𑀫नबचढच𑀟 𑀞च𑀳च',\n '𑀠च𑀪च𑀳च 𑀣च𑀪च𑀘च 𑀟𑁦 𑀞𑁦 णचञच त𑁦𑀱च 𑀤च𑀟 𑀘च𑀟ण𑁦 𑀱च प𑀢𑀟नढन ष 𑀞𑀱च𑀟𑀞𑀱च𑀳𑁣',\n 'ढच𑀟बच𑀪𑁦𑀟 𑀳च𑀟चपच चल𑁦𑀢𑀪𑁣 णच ढन𑀣𑁦 𑀳चढ𑁣𑀟 𑁣𑀲𑀢𑀳𑀫𑀢𑀟 चधत च 𑀞𑁦ढढ𑀢',\n '𑀠न𑀟 ढ𑀢 झच𑀙𑀢𑀣च 𑀱न𑀪𑀢𑀟 प𑀳𑀢ब𑁦 𑀠च𑀫𑀣𑀢 बन𑀳चन ड 𑀞च𑀞च𑀞𑀢𑀟 𑀠च𑀘चल𑀢𑀳च𑀪 𑀤च𑀠𑀲च𑀪च',\n '𑀱च𑀟𑀟𑁦 पच𑀳𑀢𑀪𑀢 𑀪𑀢𑀞𑀢त𑀢𑀟 चधत 𑀤च𑀢 𑀢णच ण𑀢 च 𑀞च𑀟 𑀘च𑀠𑀢णणच𑀪𑀡',\n '𑀣च𑀠ढच𑀪𑀱च पच ढच𑀪𑀞𑁦 च 𑀘च𑀠भ𑀢णणच𑀪 ध𑀣ध च 𑀘𑀢𑀫च𑀪 ण𑁣ढ𑁦',\n '𑀙𑀢𑀟च 𑀣च 𑀫न𑀘𑀘𑁣𑀘𑀢𑀟 𑀣च 𑀞𑁦 𑀟न𑀟च ढन𑀫च𑀪𑀢 𑀟च 𑀟𑁦𑀠च𑀟 𑀱च𑀙च𑀣𑀢 𑀟च न𑀞न𑀙',\n '𑀞𑁣पन𑀟 𑀞𑁣ल𑀢 पच पचढढचपच𑀪 𑀣च पच𑀞च𑀪च𑀪 चढढच 𑀞चढ𑀢𑀪 णन𑀳न𑀲',\n '𑀠𑁦 णच 𑀳च ढन𑀫च𑀪𑀢 𑀞𑁦 𑀘च𑀟 झच𑀲च 𑀱च𑀘𑁦𑀟 𑀳च 𑀫च𑀟𑀟न च झन𑀣न𑀪𑀢𑀟 𑀣𑁣𑀞च𑀪 𑀤च𑁥𑁦𑀡',\n 'च𑀢𑀳𑀫च ढन𑀫च𑀪𑀢𑀕 च𑀟 ण𑀢 𑀱च च𑀢𑀳𑀫च तच 𑀣𑁣𑀟 पच 𑀞𑀢𑀪च ढन𑀫च𑀪𑀢 𑀙𑀘च𑀟च𑀪𑀙',\n '𑁣𑀳𑀢𑀟ढच𑀘𑁣 𑀟𑁦 णच 𑀲𑀢 तच𑀟तच𑀟पच 𑀣च 𑀳𑀫नबचढच𑀟त𑀢𑀟 𑀟च𑀘𑁦𑀪𑀢णच ष चढ𑀣न𑀪𑀪च𑀫𑀠च𑀟 ढच𑀲𑀲च ण𑁣लच',\n 'ब𑁣𑀣𑀱𑀢𑀟 𑁣ढच𑀳𑁦𑀞𑀢𑀕 चधत पच 𑀫च𑀟च 𑀳𑀫𑀢 पच𑀞च𑀪च',\n '𑀢𑀟𑁦त 𑀤च पच त𑀢 बचढच 𑀣च ढचणच𑀪 𑀣च 𑀪𑀢𑀘𑀢𑀳पच𑀪 𑀠च𑀳न 𑀤चढ𑁦',\n '𑀟च𑀘𑁦𑀪𑀢णच𑀕 𑀘च𑀠𑀙𑀢णणच𑀪 चधत 𑀤च पच 𑀳च𑀳च𑀟पच 𑀪𑀢𑀞𑀢त𑀢𑀟 𑀙णच𑀙णच𑀟पच च 𑀞च𑀟𑁣',\n '𑀳𑀫𑁦𑀞च𑀪चन 𑀙णच  𑀞𑁣𑀠च𑀙 𑀘च𑀠भ𑀢णणच𑀪 ध𑀣ध',\n '𑀢ध𑁣ढ𑀕 𑀳𑁣𑀘𑁣𑀘𑀢𑀟षढच𑀞च 𑀠च𑀳न 𑀳𑁣𑀟 𑀞च𑀲च ढ𑀢च𑀲𑀪च 𑀣च 𑀞𑁦 𑀞𑀢𑀪च च 𑀞च𑀳𑀫𑁦 𑀠नपच𑀟𑁦 च 𑀲चत𑁦ढ𑁣𑁣𑀞 च 𑀟च𑀘𑁦𑀪𑀢णच',\n 'बन𑀣च𑀘𑀢𑀕 चधत त𑁦 पच ण𑀢 𑀱च पच𑀞च𑀪चपच 𑀞च𑀲च𑀪 न𑀟बनलन',\n '𑀠न𑀫च𑀠𑀠च𑀣न ढन𑀫च𑀪𑀢𑀕 𑀞𑁣 𑀠च𑀘चल𑀢𑀳च𑀪 𑀣चपपच𑀱च𑀟 𑀟च𑀘𑁦𑀪𑀢णच पच 𑀤च𑀠च 𑀙णच𑀪 च𑀠𑀳𑀫𑀢𑀟 𑀳𑀫चपच𑀟𑀳च𑀡',\n 'पच𑀳𑀢𑀪𑀢𑀟 𑀳𑁣𑀳𑀫𑀢णचल 𑀠𑀢𑀣𑀢णच च 𑀳𑀢णच𑀳च𑀪 𑀟च𑀘𑁦𑀪𑀢णच',\n '𑀠च𑀳न त𑁦𑀱च  ढचढन 𑀞नञ𑀢 च 𑀟च𑀘𑁦𑀪𑀢णच  𑀳च𑀢 च𑀟 त𑀢ण𑁣 ढच𑀳𑀫𑀢  𑀞च𑀪णच 𑀳न𑀞𑁦𑀦 𑀢𑀟 𑀘𑀢 𑀞𑀱च𑀟𑀞𑀱च𑀳𑁣',\n '𑀱च𑀟𑁦 𑀟𑁦 𑀳𑀢लच𑀪 𑀞च𑀳𑀫𑁦 झच𑀪𑀲𑀢𑀟 झच𑀟च𑀟च𑀟 𑀫न𑀞न𑀠𑁣𑀠𑀢 च 𑀟च𑀘𑁦𑀪𑀢णच𑀡',\n '𑀤च𑀟 त𑀢𑀪𑁦 पचललच𑀲𑀢𑀟 𑀠च𑀟 𑀲𑁦पन𑀪 𑀞𑁣 𑀤च च ण𑀢 𑀤च𑀟बचष𑀤च𑀟बच च  𑀟च𑀘𑁦𑀪𑀢णच ष प𑀢𑀟नढन',\n '𑀤चढ𑁦𑀟 𑀟च𑀘𑁦𑀪𑀢णच 𑀟च ठ𑀧ठ𑀰𑀕 𑀞न ढच 𑀠न लचढच𑀪च𑀟 चढनढन𑀱च𑀟 𑀣च 𑀳न𑀞च 𑀳𑀫च𑀲𑁦 𑀞न',\n '𑀞𑁣𑀲च𑀪 𑀳नल𑀫न च ढन𑀣𑁦 पच𑀞𑁦 प𑀳च𑀞च𑀟𑀢𑀟𑀠न 𑀣च ढच𑀟बच𑀪𑁦𑀟 𑀳𑀫𑁦𑀞च𑀪चन 𑀓 बच𑀟𑀣न𑀘𑁦',\n 'ढन𑀫च𑀪𑀢 णच बच𑀟च 𑀣च 𑀠च𑀢 𑀠चलच 𑀞न𑀠च णच णच𑀪𑀣च णच त𑀢 बचढच 𑀣च 𑀳𑀫नबचढच𑀟त𑀢𑀟 चधत',\n 'धच𑀪𑀢𑀳 𑀳प ब𑁦𑀪𑀠च𑀢𑀟 𑀤च पच 𑀣चन𑀞𑀢 च𑀟प𑁣𑀟𑀢𑁣 त𑁣𑀟प𑁦 णच 𑀠चण𑁦 बन𑀪ढ𑀢𑀟 ध𑁣त𑀫𑁦पप𑀢𑀟𑁣',\n '𑀠च𑀟 त𑀢पण 𑀤च पच ण𑀢 𑀞𑁣𑀞च𑀪𑀢𑀟 𑀪𑀢𑀞𑁦 𑀳𑀢लखच𑀦 𑀠च𑀳न न𑀟𑀢प𑁦𑀣 𑀳न𑀟 ढच प𑁦𑀟 𑀫चब 𑀣च𑀠च𑀪 ढचढढच𑀟 त𑁦𑀲च𑀟𑁦',\n '𑀙णच𑀟 𑀱च𑀳च𑀟 लच ल𑀢बच 𑀣च णच𑀪𑀘𑁦𑀘𑁦𑀟𑀢णच𑀪𑀳न 𑀤च पच 𑀞च𑀪𑁦 च ढच𑀟च',\n '𑀱च𑀳च𑀟 ल𑀢ख𑁦𑀪ध𑁣𑁣ल 𑀣च 𑀟च 𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 𑀣च 𑀳न𑀞च 𑀪चब𑁦 𑀞च𑀲𑀢𑀟 𑀳न 𑀞च𑀪च च त𑀫च𑀠ध𑀢𑁣𑀟𑀳 ल𑁦चबन𑁦',\n '𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 𑀧षर ढच𑀪त𑁦ल𑁣𑀟च𑀕 ढच𑀪तच पच 𑀫च𑀣च 𑀠च𑀞𑀢 न𑀞न च ढ𑁦𑀪𑀟चढ𑁦न',\n '𑀟चप𑀫च𑀟 𑀪𑁦𑀣𑀠𑁣𑀟𑀣 णच 𑀞𑁣𑀠च ढ𑁦𑀳𑀢𑀞पच𑀳 𑀣चबच 𑀳𑁣नप𑀫च𑀠धप𑁣𑀟',\n 'ढच𑀫चब𑁣𑀟 चल𑀢 𑀞च𑀱𑁣𑀘𑀢 णच ढनब𑁦 𑀣𑁣ब𑁣𑀟 𑀳𑀢𑀳त𑁣',\n 'चन𑀪𑁦ल𑀢𑁦𑀟 पत𑀫𑁣नच𑀠𑁦𑀟𑀢𑀕 𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 पच 𑀳चण𑀢 ञच𑀟 𑀱च𑀳च𑀟 𑀠𑁣𑀟चत𑁣',\n '𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 पच त𑀢 𑀱च𑀳च 𑀟च ढ𑀢णन च 𑀘𑁦𑀪𑁦 च लच ल𑀢बच ढचणच𑀟 तच𑀳च त𑁦लपच',\n '𑀫𑁦𑀟𑀪ण णच 𑀳चण𑀢 𑀫च𑀟𑀟न𑀟 𑀘च𑀪𑀢 च 𑀞न𑀟ब𑀢णच𑀪 त𑁣𑀠𑁣 𑀠च𑀢 ढनबच 𑀳𑁦𑀪𑀢𑁦 ढ',\n '𑀱𑁦𑀳प 𑀫च𑀠 पच 𑀞च𑀠𑀠चलच 𑀣चन𑀞च𑀪 𑀟चण𑁦𑀲 चबन𑁦𑀪𑀣',\n '𑀞𑁣 𑀳च𑀣𑀢𑁣 𑀠च𑀟𑁦 𑀤च𑀢 त𑀢 बचढच 𑀣च पच𑀞च ल𑁦𑀣च च ल𑀢ख𑁦𑀪ध𑁣𑁣ल च ढच𑀣𑀢𑀡',\n '𑀠𑁣𑀪𑁣तत𑁣 पच 𑀲𑀢पच𑀪 𑀣च 𑀳𑀢𑀲च𑀟𑀢णच 𑀣चबच बच𑀳च𑀪 𑀞𑁣𑀲𑀢𑀟 𑀣न𑀟𑀢णच च 𑀜चपच𑀪',\n '𑀫च𑀟𑀞चल𑀢𑀟 𑀪𑁣𑀟चल𑀣𑁣 णच 𑀪चढन𑀦 𑀠च𑀟 नप𑀣 𑀟च पचपपचन𑀟च𑀱च 𑀞च𑀟 बच𑀞ध𑁣',\n '𑀘नख𑁦𑀟पन𑀳 𑀤च पच 𑀲न𑀳𑀞च𑀟त𑀢 𑀠𑁦𑀳𑀳𑀢 𑀣च 𑀟𑁦ण𑀠च𑀪 𑀣च 𑀠ढचधध𑁦 च 𑀲च𑀪च𑀟𑀳च',\n 'च𑀟 त𑀢 पच𑀪च𑀪 ध𑀢ललच𑀪𑀳 𑀠𑀢ल𑀢णच𑀟 पच𑀪च च𑀟 𑀞न𑀠च 𑀞𑀱च𑀳𑀫𑁦 𑀠चपच 𑀠च𑀞𑀢 न𑀞न 𑀪𑀢ब𑀢𑀳',\n 'ढचणच𑀟 𑀞𑁣𑀪च𑀪 𑀳प𑁦ख𑁦𑀟 ब𑁦𑀪𑀪च𑀪𑀣 च𑀳प𑁣𑀟 ख𑀢ललच पच त𑀢 रष𑀧 च ध𑀪𑁦𑀠𑀢𑁦𑀪',\n '𑀠𑀛च𑀟 त𑀢पण पच 𑀣च𑀪𑁦 𑀞च𑀟 प𑁦ढन𑀪𑀢𑀟 ध𑀪𑁦𑀠𑀢𑁦𑀪 ल𑁦चबन𑁦',\n 'णच𑀟𑀟𑀢त𑀞 णच 𑀞𑁣𑀠च ढच𑀪तच𑀦 ढचणच𑀟 𑀣च 𑀞𑀱च𑀟प𑀢𑀪चब𑀢𑀟𑀳च णच 𑀞च𑀪𑁦 च चत 𑀠𑀢लच𑀟',\n 'ध𑀳ब 𑀟च 𑀳𑀫𑀢𑀪𑀢𑀟 𑀞च𑀳𑀫𑁦 𑀞नञ𑀢 𑀞च𑀟 𑀪च𑀳𑀫𑀲𑁣𑀪𑀣𑀦 𑀲𑁦ल𑀢𑀩 णच ण𑀢 𑀞च𑀳न𑀱च',\n '𑀞𑀛𑁣𑀪𑀢णच पच 𑀞न𑀣न 𑀤च पच 𑀘𑁦 बच𑀳च𑀪 त𑀢𑀟 𑀞𑁣𑀲𑀢𑀟 𑀣न𑀟𑀢णच 𑀣च 𑀳𑁣𑀟 𑀠च𑀢 𑀘𑀢𑀟णच',\n 'प𑀳𑁣𑀫𑁣𑀟 𑀣च𑀟 𑀞𑀱चलल𑁣𑀟 त𑀫𑁦ल𑀳𑁦च ब𑀛च𑀪ण तच𑀫𑀢लल णच ण𑀢 𑀪𑀢पचणच',\n '𑀳नच𑀪𑁦𑀤 𑀤च𑀢 𑀳च𑀞𑁦 𑀞𑁣𑀠च𑀱च 𑀞न𑀟ब𑀢णच𑀪𑀳च 𑀟चत𑀢𑁣𑀟चल 𑀱च𑀣𑀣च पन𑀟 णच𑀟च 𑀣च 𑀞न𑀪नत𑀢णच',\n '𑀣𑀛चबच 𑀱च𑀟𑀟च𑀟 𑀱𑁣𑀪ल𑀣 तनध 𑀣𑀢𑀟 𑀤च𑀟 ण𑀢 𑀠न𑀪चढन𑀳 च च𑀪ब𑁦𑀟प𑀢𑀟च',\n '𑀪𑁣ढ𑁦𑀪प ल𑁦𑀱च𑀟𑀣𑁣𑀱𑀳𑀞𑀢 णच लच𑀳𑀫𑁦 पच𑀞चल𑀠𑀢𑀟 𑀤𑀢𑀟च𑀪𑁦 च 𑀟च𑀫𑀢णच𑀪 पन𑀪च𑀢 𑀞च𑀪𑁣 ढ𑀢णन च 𑀘𑁦𑀪𑁦',\n '𑀠𑀛च𑀟 न𑀟𑀢प𑁦𑀣 𑀟च 𑀳𑀫𑀢𑀪𑀢𑀟 𑀞𑁣𑀪च𑀪 𑀪𑁣𑀟चल𑀣𑁣 𑀣चबच झन𑀟ब𑀢णच𑀪',\n '𑀠𑁦𑀳𑀳𑀢 णच लच𑀳𑀫𑁦 𑀞𑁣𑀲𑀢𑀟 𑀣न𑀟𑀢णच 𑀟च न𑀞न 𑀣च च𑀪ब𑁦𑀟प𑀢𑀟च पच 𑀣चन𑀞च',\n '𑀱च𑀟𑁦 𑀟𑁦 ब𑁦𑁣𑀪ब𑁦 𑀱𑁦च𑀫𑀡 चल𑀘𑀢𑀫न𑀟 𑀱च 𑀞नञ𑀢𑀟 𑀳चण𑁦𑀟 𑀙णच𑀟 झ𑀱चलल𑁣 𑀞𑁦 𑀳𑀫𑀢बच𑀡',\n '𑀪𑀢𑀞𑀢त𑀢𑀟 𑀞च𑀠च𑀪न𑀕 𑀠च𑀳𑁣णच झ𑀱चलल𑁣𑀟 झच𑀲च𑀪 𑀣च ढच 𑀳च 𑀳𑁣𑀟 झच𑀳च𑀪𑀳न पच ण𑀢 𑀟च𑀳च𑀪च',\n '𑀞च𑀳न𑀱च𑀪 𑀙णच𑀟 𑀞𑀱चलल𑁣𑀕 𑀠च𑀞𑁣𑀠च𑀪 𑀘𑁦𑀳न𑀳𑀦 ढ𑁦लल𑀢𑀟ब𑀫च𑀠𑀦 𑀱च𑀪𑀣षध𑀪𑁣𑀱𑀳𑁦𑀦 𑁦𑀪𑀢𑀞𑀳𑁦𑀟𑀦 बखच𑀪𑀣𑀢𑁣ल𑀦 ढ𑁦ल𑁣पप𑀢𑀦 𑀣च𑀟𑀘न𑀠च',\n '𑀙णच𑀟 𑀱च𑀳च𑀟 𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 𑀭𑀭 च𑀞च बचणणचपच पच𑀱चब𑁣ब𑀢𑀟𑀳न ढनबच पच𑀠चनलच',\n 'च𑀪𑀳𑁦𑀟चल 𑀤च पच ण𑀢 𑀱च𑀳च ढ𑀢णच𑀪 त𑀢𑀞𑀢𑀟 𑀱चपच𑀟 𑀳चपन𑀠ढच',\n '𑀞𑁣पन पच ढचणच𑀪 𑀣च ढ𑁦ल𑀢𑀟 ब𑀪𑁦𑁦𑀟𑀱𑁣𑁣𑀣 𑀱च𑀟𑀣च च𑀞𑁦 पन𑀫न𑀠च 𑀣च च𑀢𑀞चपच 𑀲णच𑀣𑁦',\n 'च𑀛𑀢𑀞𑀢𑀟 𑀞𑁣त𑀢 च ध𑀪𑁦𑀠𑀢𑁦𑀪 𑀤च𑀢 𑀢णच 𑀳𑀫च𑀲च𑀪 लच𑀲𑀢णच𑀪 𑀞𑀱च𑀞𑀱चल𑀱च 𑀢𑀟 𑀘𑀢 ध𑁣पप𑁦𑀪',\n '𑁦𑀪ल𑀢𑀟ब 𑀫चचलच𑀟𑀣𑀕 𑀠च𑀟त𑀫𑁦𑀳प𑁦𑀪 त𑀢पण पच 𑀞च𑀠𑀠चलच 𑀳चण𑁦𑀟 𑀣च𑀟 𑀱च𑀳च𑀟 𑀟𑁣𑀪𑀱चण 𑀣चबच ढ𑁣𑀪न𑀳𑀳𑀢च 𑀣𑁣𑀪प𑀠न𑀟𑀣',\n 'चपल𑁦प𑀢त𑁣 𑀭ष𑀧 𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣𑀕 चपल𑁦प𑀢त𑁣 𑀠च𑀣𑀪𑀢𑀣 पच त𑀢 बचढच 𑀣च 𑀤च𑀠च पच 𑀫न𑀣न च लच ल𑀢बच',\n '𑀠च𑀟त𑀫𑁦𑀳प𑁦𑀪 न𑀟𑀢प𑁦𑀣 पच 𑀳च𑀟च𑀪 𑀣च च𑀳च𑀪च𑀪 श𑀭𑀭𑀖𑀯𑀖𑀠 च 𑀞च𑀞च𑀪 ठ𑀧ठ𑀭षठठ',\n '𑀤𑀛च च ण𑀢 ब𑀱च𑀟𑀘𑁣𑀟 𑀞𑀱चलल𑁣𑀟 𑀣च 𑀠च𑀪च𑀣𑁣𑀟च णच त𑀢 𑀢𑀟ब𑀢लच 𑀣च 𑀫च𑀟𑀟न च 𑀭थ𑀗𑁢',\n '𑀙णच𑀟 𑀱च𑀳च𑀟 𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 𑀣च 𑀳न𑀞च 𑀘𑁦 च𑀠न𑀪𑀞च 𑀱च𑀳च𑀟 𑀳च𑀣च 𑀤न𑀠न𑀟पच',\n '𑀫𑀛𑁦𑀪𑀟च𑀟𑀣𑁦𑀤 𑀟च 𑀲च𑀪च𑀟𑀳च णच बच𑀠च ढनबच बच𑀳च𑀪 𑀞𑁣𑀲𑀢𑀟 𑀣न𑀟𑀢णच च 𑀜चपच𑀪',\n '𑀞च𑀳न𑀱च𑀪 छच𑀟 झ𑀱चलल𑁣𑀕 𑀠च𑀞𑁣𑀠च𑀪 प𑀢𑀠ढ𑁦𑀪𑀦 𑀠च𑀟𑁦𑀦 𑀟न𑀟𑁦𑀤𑀦 तनतन𑀪𑁦ललच𑀦 ध𑀫𑀢लल𑀢ध𑀳𑀦 𑀪𑀢त𑀫च𑀪ल𑀢𑀳𑁣𑀟',\n '𑀞च𑀳न𑀱च𑀪 गणच𑀟 𑀞𑀱चलल𑁣𑀕 𑀠च𑀞𑁣𑀠च𑀪𑀕 𑀟𑁦ण𑀠च𑀪𑀦 𑁦𑀪𑀢𑀞𑀳𑁦𑀟𑀦 𑀣𑁦𑀠ढ𑁦ल𑁦𑀦 प𑀪च𑁣𑀪𑁦𑀦 𑀣𑁦𑀟𑀟𑀢𑀳𑀦 च𑀳𑁦𑀟𑀳𑀢𑁣𑀦 ल𑀢𑀟बच𑀪𑀣𑀦 𑁦𑀞𑀢प𑀢𑀞𑁦',\n 'ल𑀢लल𑁦 ख𑀳 त𑀫𑁦ल𑀳𑁦च𑀕 𑀱चपच𑀞𑀢लच च𑀤ध𑀢ल𑀢तन𑁦पच 𑀣च चल𑁣𑀟𑀳𑁣 𑀳न ढनबच 𑀲च𑀲चपच𑀱च𑀪',\n 'त𑀫𑁦ल𑀳𑁦च ठषर च𑀪𑀳𑁦𑀟चल𑀕 बन𑀟𑀟𑁦𑀪𑀳 पच 𑀘𑁦 पच 𑀪चबच𑀪बच𑀘𑀢 त𑀫𑁦ल𑀳𑁦च च 𑀳पच𑀠𑀲𑁣𑀪𑀣 ढ𑀪𑀢𑀣ब𑁦',\n '𑀲𑀪च𑀟𑀞 𑀪𑀢ढ𑁦𑀪ण णच ण𑀢 𑀪𑀢पचणच 𑀣चबच 𑀳च𑀟च𑀙च𑀪 𑀞𑀱चलल𑁣𑀟 𑀞च𑀲च',\n 'त𑀫𑁦ल𑀳𑁦च ख𑀳 𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣𑀕 𑀙णच𑀟 𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 𑀣च 𑀳न𑀞च 𑀘𑁦 𑀳पच𑀠𑀲𑁣𑀪𑀣 ढ𑀪𑀢𑀣ब𑁦',\n 'ढचण𑁦𑀪𑀟 𑀠न𑀟𑀢त𑀫 पच ण𑀢 𑀱च 𑀳चल𑀳𑀤ढन𑀪ब 𑀪न𑀱च𑀟 𑀞𑀱चललचण𑁦',\n '𑀠𑀛च𑀳न ल𑀢ख𑁦𑀪ध𑁣𑁣ल 𑀟च 𑀟𑁦𑀠च𑀟 𑀱चञच𑀟𑀣च 𑀤च 𑀳न 𑀳चण𑀢 झन𑀟ब𑀢णच𑀪',\n '𑀟𑀞न𑀟𑀞न 𑀤च𑀢 𑀞𑁣𑀠च त𑀫𑁦ल𑀳𑁦च𑀦 𑀳𑁣नप𑀫बचप𑁦 𑀤च𑀢 𑀤चन𑀟च च 𑀢𑀟ब𑀢लच𑀦 च𑀤𑀤𑁦𑀣𑀢𑀟𑁦 णच ण𑀢 𑀞च𑀳न𑀱च',\n 'च𑀛ल 𑀟च𑀳𑀳𑀪 पच 𑀳चन𑀣𑀢 च𑀪चढ𑀢च पच ण𑀢 𑀱च 𑀪𑁣𑀟चल𑀣𑁣 पचण𑀢𑀟 चलढच𑀳𑀫𑀢 𑀠च𑀢 प𑀳𑁣𑀞च',\n 'तच𑀪ल𑁣 च𑀟त𑁦ल𑁣पप𑀢 𑀠च𑀢 𑀲न𑀳𑀞च𑀟पच𑀪 𑀞चलनढचल𑁦 णच 𑀘𑁦 𑀳पच𑀠𑀲𑁣𑀪𑀣 ढ𑀪𑀢𑀣ब𑁦',\n '𑀱च𑀟𑀢 𑀘च𑀠𑀢𑀙𑀢𑀟 प𑀳च𑀪𑁣 णच 𑀲च𑀣𑁣 𑀣चबच ढ𑁦𑀟𑁦 णच 𑀠नपन च 𑀲𑀢ल𑀢𑀟 𑀱च𑀳च 𑀟च लन𑀳च𑀢ल',\n 'ढ𑀪𑁦𑀟प𑀲𑁣𑀪𑀣 पच 𑀞च𑀪च णचबच ढच𑀪च𑀞च𑀪 𑀠च𑀟 न𑀟𑀢प𑁦𑀣',\n 'च𑀪𑀳𑁦𑀟चल 𑀤च पच ण𑀢 𑀱च𑀳च पच𑀪च च त𑀢𑀞𑀢𑀟 𑀱चपच𑀟 𑁣𑀞प𑁣ढच',\n 'प𑁣पप𑁦𑀟𑀫च𑀠द𑀟च ढ𑀢ढ𑀢णच𑀪 ब𑁣𑀪𑀣𑁣𑀟𑀦 𑀱चपच𑀞𑀢लच त𑀫च𑀠ढ𑁦𑀪लच𑀢𑀟 𑀤च𑀢 ढच𑀪 ल𑀢ख𑁦𑀪ध𑁣𑁣ल',\n '𑀫चचलच𑀟𑀣 𑀟च 𑀤च𑀤𑀤चढ𑀢 णच𑀟च 𑀞न𑀠च 𑀘𑀢𑀟 𑀪च𑀣च𑀣𑀢 च 𑀞च𑀲च𑀪𑀳च',\n 'प𑁣पप𑁦𑀟𑀫च𑀠 पच 𑀲च𑀪च ध𑀪𑁦𑀠𑀢𑁦𑀪 पच ढच𑀟च 𑀣च त𑀢𑀟 𑀞𑀱चललचण𑁦 𑀣च णच𑀱च',\n '𑀞ल𑁣धध णच ण𑀢 𑀳च𑀟च𑀣𑀢𑀟 𑀞𑁣𑀞च𑀪 धच𑀪𑀞𑁦𑀪 𑀞𑁣त𑀢णच𑀟 ढ𑁣न𑀪𑀟𑁦𑀠𑁣नप𑀫',\n 'प𑁦𑀟 𑀫चब 𑀤च𑀢 𑀠चण𑁦 बन𑀪ढ𑀢𑀟 𑀪𑁣𑀟चल𑀣𑁣 च 𑀱चपच𑀟 𑀘च𑀟च𑀢𑀪न',\n 'ढच𑀪त𑁦ल𑁣𑀟च पच 𑀳च𑀠न बन𑀪ढ𑀢𑀟 ढनबच त𑀫च𑀠ध𑀢𑁣𑀟𑀳 ल𑁦चबन𑁦 च ढच𑀣𑀢',\n 'तच𑀳𑁦𑀠𑀢𑀪𑁣𑀕 𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 पच त𑀢 𑀱च𑀳च 𑀭𑀖𑀧 पच𑀪𑁦 𑀣च 𑀣च𑀟 ढ𑀪च𑀤𑀢ल च त𑀢𑀞𑀢𑀟 𑀲𑀢ल𑀢',\n '𑀘𑁣𑀳𑁦 𑀠𑁣न𑀪𑀢𑀟𑀫𑁣 णच 𑀲च𑀪च 𑀣च𑀪च𑀱च च 𑀱च𑀳च𑀟 𑀲च𑀪𑀞𑁣 च 𑀳𑁦𑀪𑀢𑁦 च पच ढच𑀟च',\n '𑀞च𑀳न𑀱च𑀪 छच𑀟 झ𑀱चलल𑁣𑀕 𑀠च𑀞𑁣𑀠च𑀪 𑀠च𑀟𑁦𑀦 𑀳चलच𑀫𑀦 ढचल𑁦𑀦 𑀣𑁦 𑀘𑁣𑀟ब𑀦 𑀞𑁣न𑀟𑀣𑁦𑀦 𑀠चपप𑀫𑀢𑀘𑀳 𑀣च ढ𑁣प𑀠च𑀟',\n 'च𑀳प𑁣𑀟 ख𑀢ललच  पच 𑀟च𑀣च न𑀟च𑀢 𑁦𑀠𑁦𑀪ण 𑀳चढ𑁣𑀟 𑀞𑁣त𑀢णच𑀟पच',\n '𑀣𑀛न𑀞𑀞च𑀟 𑀙णच𑀟 𑀱च𑀳च𑀟 𑀣च च𑀞च बचणणचपच बच𑀳च𑀪 त𑀢𑀟 𑀞𑁣𑀲𑀢𑀟 𑀣न𑀟𑀢णच च 𑀜चपच𑀪',\n 'ढच𑀪त𑁦ल𑁣𑀟च 𑀤च पच 𑀞च𑀪च 𑀣च 𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 𑀣च 𑀘नख𑁦𑀟पन𑀳 च च𑀠न𑀪𑀞च',\n 'प𑀫𑁣𑀠च𑀳 𑀠नलल𑁦𑀪 च𑀞च 𑀤चढच 𑀲𑀢पचपत𑁦𑀟 𑀣च𑀟 𑀞𑀱चलल𑁣𑀟 ढचण𑁦𑀪𑀟 च ठ𑀧ठ𑀭टठठ',\n 'न𑀛𑀟𑀢प𑁦𑀣 पच त𑀢𑀪𑁦 𑀪𑁣𑀟चल𑀣𑁣 𑀣चबच त𑀢𑀞𑀢𑀟 𑀙णच𑀟 𑀱च𑀳च𑀟 𑀣च 𑀤च 𑀳न 𑀞च𑀪च 𑀣च त𑀫𑁦ल𑀳𑁦च',\n 'ल𑁦𑀱च𑀟𑀣𑁣𑀱𑀳𑀞𑀢 णच त𑀢 𑀞𑀱चलल𑁣 𑀟च 𑁢𑀧𑀭 𑀪च𑀟च𑀪 चल𑀫च𑀠𑀢𑀳',\n 'णच𑀣𑀣च 𑀙णच𑀟 च𑀘च𑀟प𑀢𑀟च 𑀳न𑀞च ण𑀢 𑀠न𑀪𑀟च𑀪 त𑀢𑀟 बच𑀳च𑀪 𑀞𑁣𑀲𑀢𑀟 𑀣न𑀟𑀢णच च 𑀞च𑀳च𑀪𑀳न',\n '𑀠𑀢𑀟𑁣 𑀪च𑀢𑁣लच𑀕 𑀱च𑀞𑀢ल𑀢𑀟 धचनल ध𑁣बढच 𑀠𑀢𑀟𑁣 𑀪च𑀢𑁣लच णच 𑀠न𑀳च𑀟पच 𑀪च𑀫𑁣पच𑀟𑀟𑀢𑀟 𑀠नपन𑀱च𑀪𑀳च',\n '𑀙𑀟𑀛च ण𑀢 𑀳च𑀙च ढच𑀟 𑀠नपन ढच𑀙 𑀢𑀟 𑀘𑀢 𑀣च𑀟 𑀱च𑀳च𑀟 च𑀪𑀳𑁦𑀟चल 𑀣च च𑀞च तच𑀞च 𑀱च 𑀱न𑀞च',\n '𑀠च𑀟𑁦 णच 𑀘𑁦 𑀘च𑀠न𑀳 𑀣𑁣𑀠𑀢𑀟 𑀞च𑀠𑀠चलच 𑀞𑁣𑀠च𑀱च ढचण𑁦𑀪𑀟',\n '𑀱चपच𑀞𑀢लच 𑀠चबन𑀢𑀪𑁦 णच ढनबच 𑀱च न𑀟𑀢प𑁦𑀣 𑀱च𑀳च 𑀣च 𑀲त 𑀳𑀫𑁦𑀪𑀢𑀲𑀲',\n 'ढचण𑁦𑀪𑀟 𑀠न𑀟𑀢त𑀫 ख𑀳 ख𑀢ललच𑀪𑁦चल𑀕 𑀱च𑀳न ढचपनपन𑀱च 𑀣च णच 𑀞च𑀠चप 𑀞न 𑀳च𑀟𑀢 𑀞च𑀟 𑀲च𑀲चपच𑀱च𑀪',\n '𑀠च𑀞𑁣𑀠च𑀪 𑁣𑀳𑀢𑀠𑀫𑁦𑀟𑀦 ढ𑁦लल𑀢𑀟ब𑀫च𑀠𑀦 𑁦𑀟𑀣𑀪𑀢त𑀞𑀦 ब𑁣𑀠𑁦𑀤𑀦 𑀤च𑀫च च 𑀞च𑀳न𑀱च𑀪 णच𑀟 𑀞𑀱चलल𑁣',\n 'च𑀟 प𑀳चणच𑀪 𑀣च 𑀪च𑀟च𑀪 𑀱च𑀳च𑀟 ढच𑀪त𑁦ल𑁣𑀟च 𑀣च 𑀳𑁦ख𑀢ललच च लच ल𑀢बच',\n '𑀞च𑀪𑀢𑀠 ढ𑁦𑀟𑀤𑁦𑀠च णच ण𑀢 𑀞च𑀟ष𑀞च𑀟ष𑀞च𑀟 𑀣च 𑀪चनल च णच𑀱च𑀟 त𑀢𑀟 𑀞𑀱चललचण𑁦',\n '𑀢ढ𑀪च𑀫𑀢𑀠𑁣ख𑀢त णच 𑀳च𑀞च 𑀫च𑀟𑀟न 𑀞च𑀟 णच𑀪𑀘𑁦𑀘𑁦𑀟𑀢णच𑀪 𑀞च𑀞च 𑀣चणच च चत 𑀠𑀢लच𑀟',\n 'पच𑀠ढचण𑁣ण𑀢 𑀣च च𑀠𑀳𑁣𑀳𑀫𑀢 𑀞च𑀟 ढ𑀪च𑀤𑀢ल च बच𑀳च𑀪 𑀞𑁣𑀲𑀢𑀟 𑀣न𑀟𑀢णच च 𑀜चपच𑀪',\n 'च𑀪𑀳𑁦𑀟चल पच 𑀳चण𑀢 𑀤𑀢𑀟त𑀫𑁦𑀟𑀞𑁣 𑀣चबच 𑀠च𑀟 त𑀢पण',\n 'च𑀪𑀳𑁦𑀟चल पच 𑀞च𑀠𑀠चलच 𑀣चन𑀞च𑀪 बचढ𑀪𑀢𑁦ल 𑀘𑁦𑀳न𑀳',\n 'त𑀫𑁦ल𑀳𑁦च पच च𑀢𑀞𑁦 𑀱च 𑀣च 𑀪𑁦चल 𑀳चझ𑁣 𑀞च𑀲𑀢𑀟 पच 𑀘𑁦 𑀳च𑀟प𑀢चब𑁣 ढ𑁦𑀪𑀟चढन',\n '𑀠च𑀟त𑀫𑁦𑀳प𑁦𑀪 न𑀟𑀢प𑁦𑀣 पच तच𑀳च ल𑀢ख𑁦𑀪ध𑁣𑁣ल 𑀣च त𑀢 रष𑀧',\n '𑀞च𑀳न𑀱च𑀪 छच𑀟 झ𑀱चलल𑁣𑀕 𑀠च𑀞𑁣𑀠च𑀪 𑀠ढचधध𑁦𑀦 𑀫चचलच𑀟𑀣𑀦 प𑁣𑀟𑁦ण𑀦 𑀠𑁣न𑀟प𑀦 𑀳𑀢लखच𑀦 𑀣चल𑁣प',\n '𑀠ढचधध𑁦 𑀟च 𑀘𑀢𑀟 ध𑀳ब पच ण𑀢 𑀠च𑀳च 𑀙𑀙𑀪𑁣𑀠𑁣𑀟 ढच𑀞च𑀙𑀙 𑀤च𑀢 ढच𑀪 𑀞न𑀟ब𑀢णच𑀪 च 𑀘च𑀟च𑀢𑀪न',\n '𑀠च𑀟 न𑀟𑀢प𑁦𑀣 पच ढनबच 𑀞न𑀪𑀠च𑀟 𑀣न𑀪𑁣 𑀞च𑀪𑁣𑀟 𑀲च𑀪𑀞𑁣 𑀞च𑀪𑀞च𑀳𑀫𑀢𑀟 प𑁦𑀟 𑀫चब',\n '𑀲𑁦𑀪𑀟च𑀟𑀣𑀢𑀟𑀫𑁣 णच 𑀞𑁣𑀠च चप𑀫ल𑁦प𑀢त𑁣 धच𑀪च𑀟च𑁦𑀟𑀳𑁦',\n 'ढ𑀛च 𑀤च𑀟 𑀣च𑀢𑀟च 𑀱च𑀳च 𑀣च 𑀞𑀱चलल𑁣 ढच𑀦 𑀣च 𑀳𑀫𑀢 च𑀞च 𑀳च𑀟𑀟𑀢 𑀢𑀟 𑀘𑀢 च𑀟प𑁣𑀟ण',\n 'च𑀛𑀟 𑀳च𑀟णच 𑀱च𑀙च𑀣𑀢𑀟 𑀞च𑀪𑀳𑀫𑁦 𑀟च 𑀞च𑀳च𑀳𑀫𑁦𑀟 𑀣च 𑀞𑁦 𑀳𑁣𑀟 𑀣चन𑀞च𑀪 𑀟चनण𑀢𑀟 बच𑀳च𑀪 𑀟चप𑀢𑁣𑀟𑀳 तनध ठ𑀧ठ𑀖',\n 'त𑀫𑁦ल𑀳𑁦च 𑀟च ढ𑀢𑀟त𑀢𑀞𑁦 𑀞च𑀟 𑀟न𑀟च 𑀱च𑀪𑀢णच𑀪 𑀣च च𑀞च ण𑀢 𑀱च 𑀳𑁣𑀟',\n '𑀠𑀛𑁣𑀣𑀪𑀢त 𑀤च𑀢 ढनबच 𑀱च त𑀪𑁣चप𑀢च बच𑀳च𑀪 त𑀢𑀟 𑀞𑁣𑀲𑀢𑀟 𑀣न𑀟𑀢णच 𑀞च𑀪𑁣 𑀟च 𑀫न𑀣न',\n '𑀪𑁦चल 𑀠च𑀣𑀪𑀢𑀣 त𑁦 𑀠च𑀲𑀢 𑀣च𑀪च𑀘च च 𑀞𑀱चलल𑁣𑀟 𑀞च𑀲च च 𑀣न𑀟𑀢णच',\n '𑀜चपच𑀪 ठ𑀧ठठ𑀕 𑀠𑁣𑀪𑁣तत𑁣 पच प𑀳चललच𑀞𑁦 च𑀠𑀠च च𑀟 𑀲𑀢पच𑀪 𑀣च ढ𑁦लब𑀢न𑀠',\n '𑀱चपच𑀞𑀢लच च 𑀳चणच𑀪 𑀣च त𑀫𑁦ल𑀳𑁦च 𑀟च𑀟 𑀣च 𑀞च𑀪𑀳𑀫𑁦𑀟 𑀱चपच𑀟 𑀟च𑀟',\n '𑁦𑀪ल𑀢𑀟ब ढ𑀪चनप 𑀫चचलच𑀟𑀣𑀕 𑀠च𑀟 त𑀢पण 𑀣च 𑀪𑁦चल 𑀣च ढच𑀪तच 𑀟च 𑀘𑀢𑀪च𑀟 च𑀠𑀳च 𑀣चबच 𑀣च𑀟 𑀱च𑀳च𑀟 𑀣𑁣𑀪प𑀠न𑀟𑀣',\n '𑁦𑀠𑀢ल𑁦 𑀳𑀠𑀢प𑀫 𑀪𑁣𑀱𑁦  𑀤च𑀢 ण𑀢 𑀘𑀢𑀟णच𑀪 𑀱चपच𑀟𑀟𑀢 ढचणच𑀟 ण𑀢 𑀠च𑀳च प𑀢णचपच',\n '𑀟च𑀘𑁦𑀪𑀢णच 𑀤च पच 𑀲च𑀲चपच 𑀣च चलब𑁦𑀪𑀢च च 𑀱च𑀳च𑀟 𑀳च𑀣च 𑀤न𑀠न𑀟पच',\n '𑀠च𑀟णचष𑀠च𑀟णच𑀟 च𑀳च𑀪च𑀪 𑀪चणन𑀞च 𑀣च च𑀞च पच𑁥च ण𑀢 च 𑀱च𑀘𑁦𑀟 𑀞चलल𑁣𑀟 𑀞𑀱चलल𑁣𑀟 𑀞च𑀲च च 𑀣न𑀟𑀢णच',\n '𑀠ढचधध𑁦 𑀟𑁦 𑀞च𑀟 बचढच च पच𑀞च𑀪च𑀪 पच𑀞चल𑀠𑀢𑀟 𑀤𑀢𑀟च𑀪𑁦 च बच𑀳च𑀪 𑀞𑁣𑀲𑀢𑀟 𑀣न𑀟𑀢णच',\n '𑀠𑀢ल𑀞ण𑀱चण बचलच𑀩ण𑀕 च𑀟 बच𑀟𑁣 𑀱च𑀟𑀢 चढन 𑀠च𑀢 𑀘न𑀘𑀘नणच𑀱च 𑀣च ढच च 𑀳च𑀟 𑀳𑀫𑀢 ढच च तच𑀟 𑀳च𑀪च𑀪𑀢𑀟 𑀳च𑀠च𑀟𑀢णच',\n 'णच𑀣𑀣च त𑀢च 𑀞𑁦 च𑀠𑀲च𑀟𑀢 𑀣च 𑀪न𑀱च 𑀱च𑀘𑁦𑀟 ण𑀢 𑀱च 𑀘च𑀠चभच पच𑀠ढचण𑁣ण𑀢',\n 'णच𑀣𑀣च ञच𑀟 च𑀣च𑀠 𑀞𑁦 𑀪न𑀳च 𑀱न𑀪च𑀪𑁦𑀟 पच𑀪𑀢𑀫𑀢 𑀟च 𑀣न𑀟𑀢णच च णचण𑀢𑀟 𑀫चझच𑀪 𑀠च𑀙च𑀣च𑀟च𑀢',\n 'त𑁣ख𑀢𑀣ष𑀭थ𑀕 ढढत च𑀲𑀪𑀢तच पच झच𑀣𑀣च𑀠च𑀪 𑀣च 𑀳𑀫𑀢𑀪𑀢𑀟 णचझ𑀢 𑀣च लचढच𑀪च𑀟 झच𑀪णच',\n 'त𑀫च𑀟ब𑀙𑁦ष𑀖𑀕 णच𑀣𑀣च त𑀫𑀢𑀟च पच ण𑀢 च𑀢𑀞𑁦 𑀣न𑀟𑀢णच𑀪 𑀱चपच 𑀣𑁣𑀟 च 𑀞𑀱च𑀳𑁣 𑀠चपच 𑀣न𑀱चप𑀳न',\n '𑀣च𑀞𑀢𑀟 च𑀘𑀢ण𑁦 𑀞चणच𑀟 पच𑀪𑀢𑀫𑀢𑀟 त𑀢च 𑀠च𑀲𑀢 𑀳𑀢𑀪𑀪𑀢 च 𑀣न𑀟𑀢णच',\n 'प𑁦ल𑁣ल𑀢 𑀟च 𑀳𑁣𑀟 𑀠चणच𑀪 𑀣च  𑀟च𑀢𑀪𑁣ढ𑀢 𑀱च𑀟𑀢 𑀳चढ𑁣𑀟 ढ𑀢𑀪𑀟𑀢 𑀟च 𑀤च𑀠च𑀟𑀢 पच 𑀲न𑀳𑀞च𑀪 पन𑀲च𑀲𑀢',\n '𑀠चपच𑀳𑀫𑀢𑀟 𑀣च 𑀞𑁦 ञचन𑀞च𑀪 𑀟चनण𑀢𑀟 𑀞च𑀪चपन𑀟𑀳च पच 𑀫च𑀟णच𑀪 𑀤च𑀟च 𑀫𑁣पन𑀟च𑀟 𑀠नपच𑀟𑁦 च 𑀞च𑀟𑁣',\n 'च𑀤च𑀪𑁦𑀕 𑀱𑀫चप𑀳चधध णच पच𑀢𑀠च𑀞च च𑀟 बच𑀟𑁣 णच𑀪𑀢𑀟णच𑀪 𑀣च पच ढचत𑁦 प𑀳च𑀱𑁣𑀟 𑀳𑀫𑁦𑀞च𑀪च 𑀫न𑀣न',\n 'च𑀟 च𑀠𑀢𑀟त𑁦 𑁦ल𑁣𑀟 𑀠न𑀳𑀞 णच 𑀳चण𑀢 प𑀱𑀢पप𑁦𑀪 च 𑀞च𑀟 घररढ𑀟',\n '𑀲चत𑁦ढ𑁣𑁣𑀞 णच 𑀫च𑀟च 𑀣नढढच𑀟 𑀠च𑀟𑀫च𑀘𑁣𑀘𑀢 च𑀢𑀞𑀢 च 𑀳𑀫च𑀲𑀢𑀟𑀳च',\n '𑀠च𑀳च𑀟च 𑀞𑀢𑀠𑀢णणच 𑀳न𑀟 ण𑀢 𑀞चपच𑀪𑀢𑀟 𑀳च𑀠न𑀟 𑀫𑁣प𑁣𑀟 𑀣न𑀟𑀢णच𑀪 𑀘नध𑀢प𑁦𑀪',\n '𑀘𑀛च𑀟च𑀪 𑀣𑀢𑀟 नबच𑀟𑀣च 𑀠च𑀢 𑀟न𑀟च त𑀢 𑀣च 𑀤नत𑀢 च प𑀱𑀢पप𑁦𑀪',\n 'णच𑀣𑀣च 𑀠च𑀙च𑀢𑀞चपच𑀟 ढढत 𑀞𑁦 𑀱चललच𑀲च 𑀠च𑀞न लचढच𑀪च𑀢 च 𑀢𑀟पच𑀟𑁦प',\n 'ढ𑀢𑀣𑀢ण𑁣𑀕 𑀠च𑀞च𑀫𑁣𑀟 𑀣च णच ण𑀢 𑀲𑀢त𑁦 च बणच𑀪च𑀟 𑀢𑀟𑀘𑀢𑀟 𑀘च𑀟च𑀪𑁦पच',\n 'णच𑀣𑀣च 𑀤च 𑀞न 𑀞नलच 𑀣च पच𑀪ढ𑀢णणच𑀪 णच𑀪च𑀟𑀞न णचण𑀢𑀟 𑀞च𑀪चपन च 𑀢𑀟पच𑀟𑁦प',\n 'ढ𑁣𑁦𑀢𑀟ब णच 𑀣च𑀞चपच𑀪 𑀣च 𑀞𑁦𑀪च 𑀘𑀢𑀪ब𑀢𑀟 𑀬𑀰𑀬 𑀠च𑀩',\n '𑀫𑁣पन𑀟च𑀟 पचन𑀪च𑀪𑀢 𑀠च𑀳न ढच𑀟 𑀳𑀫च𑀙च𑀱च 𑀣च 𑀱च𑀟𑀢 𑀣चल𑀢ढ𑀢 णच 𑀣चन𑀞च',\n 'च𑀠न𑀪𑀞च 𑀣च 𑀳चन𑀣𑀢णणच 𑀤च 𑀳न त𑀢 बचढच 𑀣च त𑀢𑀟𑀢𑀞𑀢𑀟 𑀠च𑀞च𑀠च𑀢',\n '𑁦𑀲तत पच 𑀞च𑀠च 𑀱च𑀟𑀣च च𑀠न𑀪𑀞च 𑀞𑁦 𑀤च𑀪ब𑀢 𑀣च 𑀤च𑀠ढच𑀪 𑀢𑀟पच𑀟𑁦प',\n 'च𑀟 त𑀢𑀠𑀠च 𑀠चप𑀳चणच प𑀳च𑀞च𑀟𑀢𑀟 त𑀫𑀢𑀟च 𑀣च च𑀠न𑀪𑀞च 𑀞च𑀟 𑀫नच𑀱𑁦𑀢',\n '𑀫च𑀟ण𑁣ण𑀢 न𑀞न 𑀣च च𑀞च 𑀲𑀢 ण𑀢 𑀱च 𑀠नपच𑀟𑁦 𑀞नप𑀳𑁦 च 𑀢𑀟पच𑀟𑁦प',\n '𑀲च𑀳च𑀫च𑀪 𑀖ब𑀕 चढ𑀢𑀟 𑀣च 𑀤च𑀢 𑀳च 𑀳चढढ𑀢𑀟 𑀱चण𑁣ण𑀢𑀟𑀞न 𑀳न 𑀢णच 𑀞च𑀱𑁣 त𑀢𑀞च𑀳 बच 𑀘𑀢𑀪चब𑁦𑀟 𑀳च𑀠च',\n '𑀠𑁦𑀟𑁦 𑀟𑁦 ढचढढच𑀟 लचण𑀢𑀟 𑀱नपच𑀪 लच𑀟पच𑀪𑀞𑀢 𑀟च 𑀟च𑀘𑁦𑀪𑀢णच 𑀞न𑀠च णचणच णच𑀞𑁦 च𑀢𑀞𑀢𑀡',\n '𑀠𑁦 णच 𑀳च 𑀞𑀱च𑀟च𑀞𑀢 𑀫न𑀣न ढचढन 𑀢𑀟प𑁦𑀪𑀟𑁦प च 𑀢𑀪च𑀟𑀡',\n 'च𑀣च𑀠 𑀞𑀢𑀟ब𑀕 णच𑀪𑁣 ञच𑀟 𑀳𑀫𑁦𑀞च𑀪च 𑀳𑀫𑀢𑀣च 𑀣च 𑀞𑁦 𑀣च ढन𑀪𑀢𑀟 च𑀢𑀞𑀢 𑀣च 𑀙णच𑀟 𑀳च𑀠च 𑀘च𑀟𑀟चप𑀢 णच ढन𑀪ब𑁦 𑀟च𑀳च',\n '𑀙णच𑀪𑀢𑀠च𑀟 𑀳चन𑀣𑀢णणच ढच𑀢 ण𑀢 𑀱च 𑀘𑁦𑀲𑀲 ढ𑁦𑀤𑁣𑀳 𑀞नप𑀳𑁦 च 𑀱𑀫चप𑀳चधध ढच𑀙',\n '𑀠च𑀞च𑀠च𑀟 𑀣च त𑀫𑀢𑀟च पच ण𑀢 च𑀠𑀲च𑀟𑀢 𑀣च 𑀳न 𑀱न𑀪𑀢𑀟 𑀞च𑀢 𑀫च𑀪𑀢 𑀞च𑀟 𑀢𑀟𑀣𑀢च',\n 'च𑀠न𑀪𑀞च 𑀟च ढ𑀢𑀟त𑀢𑀞𑁦 𑀞च𑀟 𑀱च𑀳न चढनढन𑀱च 𑀣च 𑀞𑁦 णच𑀱𑁣 च 𑀳च𑀪च𑀪𑀢𑀟 𑀳च𑀠च𑀟𑀢णच𑀪पच',\n 'णच𑀣𑀣च त𑀫𑀢𑀟च 𑀞𑁦 𑀳𑀫𑀢𑀪𑀢𑀟 𑀤च𑀠च बचबच𑀪न𑀠च𑀪 छच𑀪 𑀳च𑀠च 𑀘च𑀟𑀟चप𑀢 𑀲𑀢ण𑁦 𑀣च 𑀞𑁣𑀱च',\n '𑀣च बच𑀳𑀞𑁦 𑀫न𑀞न𑀠च𑀪 𑀲ढ𑀢 पच 𑀱च𑀟𑀞𑁦 चढढच 𑀞णच𑀪𑀢 𑀣चबच 𑀤च𑀪ब𑀢𑀟𑀳च च 𑀞च𑀟 𑀫न𑀳𑀫धनधध𑀢𑀡',\n '𑀳𑀫𑀢𑀟 𑀱च𑀟𑁦 𑀫चल𑀢 𑀠चपच𑀳च 𑀞च𑀟 𑀳𑀫𑀢बच 𑀢𑀣च𑀟 ढच 𑀳न पच𑀪𑁦 𑀣च 𑀱चण𑁣ण𑀢𑀟𑀳न𑀡',\n '𑀞न𑀣𑀢𑀟 𑀣च 𑀠च𑀳न 𑀲च𑀣च च 𑀘𑀢 च 𑀳𑀫च𑀲न𑀞च𑀟 𑀳च𑀣च 𑀤न𑀠न𑀟पच 𑀞𑁦 𑀳च𑀠न 𑀟च 𑀞च𑀪न𑀱च',\n '𑀠चपच𑀳𑀫𑀢णच𑀪 𑀣च पच 𑀳च𑀟 𑀞च𑀟 𑀠𑁣प𑁣त𑀢𑀟 𑀤च𑀠च𑀟𑀢 च चढन𑀘च',\n '𑀠नपन𑀠𑀢𑀟 𑀣च 𑀤च𑀢 𑀞च𑀳𑀫𑁦 𑀣चलच 𑀠𑀢ल𑀢णच𑀟 𑀣चणच 𑀣𑁣𑀟 प𑁣𑀟𑁣 ढ𑀢पत𑁣𑀢𑀟 𑀣𑀢𑀟𑀳च च 𑀱चपच ढचढढच𑀪 ढ𑁣लच',\n 'णच𑀣𑀣च 𑀱चपच बनबन𑀱च पच लचलचपच 𑀱नपच𑀪 लच𑀟पच𑀪𑀞𑀢 च च𑀠न𑀪𑀞च',\n 'झ𑀱चण𑁣ण𑀢𑀟 झ𑀱चझ𑀱चल𑀱च𑀪 𑀣च च𑀞च झ𑀢𑀪झ𑀢𑀪𑁣 𑀳न𑀟 ण𑀢 𑀱च𑀳च𑀟 𑀞𑀱च𑀠𑀲नपच',\n 'च𑀟च झ𑀢𑀣चणच𑀪 ब𑀢𑀱चण𑁦 𑀣चबच 𑀳च𑀪च𑀪𑀢𑀟 𑀳च𑀠च𑀟𑀢णच',\n '𑀞𑁣पन पच प𑀢लच𑀳पच 𑀱च 𑀞च𑀞च ब𑁣ब𑁦 𑀫𑁣पन𑀟च𑀟 𑀘𑀢𑀞𑁣𑀞𑀢𑀟पच 𑀣च पच 𑀳च च 𑀲चत𑁦ढ𑁣𑁣𑀞',\n '𑀠च𑀟𑀣लच 𑀠च𑀳𑁦𑀞𑁣𑀕 𑀣च𑀟 च𑀲𑀢𑀪𑀞च 𑀟च 𑀲च𑀪𑀞𑁣 𑀣च 𑀤च 𑀳𑀫𑀢 𑀳च𑀪च𑀪𑀢𑀟 𑀳च𑀠च𑀟𑀢णच णच 𑀠नपन',\n '𑀲च𑀲नपन𑀞च𑀪 𑀥𑀠𑁦प𑁣𑁣 ढचणच𑀟 𑀳𑀫𑁦𑀞च𑀪च ढ𑀢णच𑀪𑀕 च𑀞𑀱च𑀢 चढ𑀢𑀟 𑀣च णच 𑀳चनणच𑀡',\n 'पच𑀳𑀱𑀢𑀪च𑀪 ब𑁣𑁣बल𑁦 पच बच𑀟𑁣 बच𑀱च𑀪 𑀱च𑀟𑀢 च त𑀢𑀞𑀢𑀟 𑀠𑁣पच',\n 'णच𑀣𑀣च 𑀤च 𑀞न 𑀢णच 𑀫च𑀣च 𑀱चण𑁣ण𑀢𑀟 छचछच𑀟𑀞न 𑀣च 𑀟च𑀞न',\n 'णच𑀣𑀣च 𑀲च𑀳च𑀫च𑀪 𑀟च𑀙न𑀪च 𑀤च पच 𑀢णच 𑀱च𑀪बच𑀤च ढ𑀢ल𑀙च𑀣च𑀠च',\n 'णच 𑀤च 𑀞न ण𑀢 𑀢𑀣च𑀟 𑀞न𑀞च बच 𑀣च𑀠𑀢𑀳च𑀪 ब𑁣𑁣बल𑁦 च 𑀣च𑀞𑀢𑀟𑀞न𑀡',\n '𑀱𑀫चप𑀳चधध𑀕 𑀳चढ𑁣𑀟 𑀳चनण𑀢𑀟 𑀞च𑀲च𑀪 𑀳च𑀣च𑀪𑀱च𑀪 णच ढच𑀪 ढचणच 𑀣च झन𑀪च',\n '𑀱च𑀟𑀢 णच 𑀞च𑀲च 𑀞च𑀠𑀲च𑀟𑀢𑀟 ढ𑀢𑀟त𑀢𑀞𑁦𑀟 बच𑀟𑁣 𑀞𑁣 𑀠च𑀙चन𑀪चपच 𑀟च 𑀫च𑀙𑀢𑀟पच𑀪 𑀘न𑀟च च 𑀞च𑀟𑁣',\n '𑀠𑀢त𑀪𑁣𑀳𑁣𑀲प णच ढच𑀟𑀞च𑀣𑁣 णन𑀟𑀞न𑀪𑀢𑀟 ण𑀢𑀟 𑀞चप𑀳चलच𑀟𑀣च𑀟 च 𑀤चढ𑁦𑀟 च𑀠न𑀪𑀞च',\n '𑀳चनण𑀢𑀟 णच𑀟चण𑀢𑀕 𑀠𑁦 च𑀞𑁦 𑀟न𑀲𑀢 𑀣च च𑀣च𑀟च 𑀢𑀳𑀞च𑀪 तच𑀪ढ𑁣𑀟𑀦 𑀳𑀫𑀢𑀟 𑀤च𑀢 𑀢णच त𑁦प𑁣 𑀣न𑀟𑀢णच𑀡',\n 'गढच𑀟 ण𑀢 𑀠च𑀞च𑀪च𑀟पच ढच च𑀠𑀠च 𑀟च 𑀞𑀢𑀪𑀞𑀢𑀪𑀢 𑀢𑀟𑀘𑀢𑀟 ढच𑀟ष𑀪न𑀱च 𑀠च𑀢 च𑀠𑀲च𑀟𑀢 𑀣च 𑀫च𑀳𑀞𑁦𑀟 𑀪च𑀟च च 𑀳𑁣𑀞𑁣प𑁣भ',\n '𑀤च च 𑀣चन𑀞𑀢 𑀫𑁣प𑁣𑀟 𑀪च𑀠𑀢𑀟 ढलचत𑀞 𑀫𑁣ल𑁦 𑀣चबच 𑀣न𑀟𑀢णच𑀪 पचन𑀪च𑀪𑀢',\n 'च𑀟च 𑀳च𑀠न𑀟 𑀞च𑀪न𑀱च𑀪 ढ𑀢𑀣𑀢ण𑁣𑀟 ढ𑁣ब𑀢 𑀠च𑀢 𑀞च𑀠च𑀪 बच𑀳𑀞𑁦',\n '𑀬𑀰𑀬 𑀠च𑀩𑀕 च𑀟 च𑀠𑀢𑀟त𑁦 𑀱च ढ𑁣𑁦𑀢𑀟ब 𑀣च𑀱𑁣𑀱च 𑀳च𑀪च𑀪𑀢𑀟 𑀳च𑀠च𑀟𑀢णच𑀪 पन𑀪च𑀢',\n 'च𑀣च𑀟 𑀫न𑀳𑀳𑁦𑀢𑀟 𑀣𑀢𑀣च𑀕 𑀣चल𑀢ढ𑀢𑀟 𑀲𑀢𑀪च𑀠च𑀪𑁦𑀟 𑀣च णच 𑀳च𑀠च𑀪 𑀱च झचनण𑁦𑀟𑀳न 𑀱नपच𑀪 लच𑀟पच𑀪𑀞𑀢 च 𑀫चढच𑀳𑀫च',\n 'ब𑁣𑁣बल𑁦 णच त𑀢𑀞च 𑀳𑀫𑁦𑀞च𑀪च ठ𑀰𑀕 बच चढनढन𑀱च ठ𑀭 𑀣च ढच 𑀞न 𑀳च𑀟𑀢 ढच बच𑀠𑁦 𑀣च ब𑁣𑁣बल𑁦',\n 'च𑀟 ढच𑀢 𑀱च 𑁥𑁦𑀪च लच𑀠ढच𑀪 णचढ𑁣 𑀳चढ𑁣𑀣च ढच𑀘𑀢𑀟पच𑀪𑀳च पच बच𑀟𑁣 ढच𑀠चषढच𑀠च𑀢',\n '𑀱𑀫चप𑀳चधध णच 𑀳च𑀟णच 𑀫च𑀪𑀳𑀫𑁦𑀟 𑀫चन𑀳च च प𑀳च𑀪𑀢𑀟𑀳च',\n 'ढ𑀢𑀣𑀢ण𑁣𑀕 𑀠च𑀞च𑀫𑁣𑀟 𑀣च णच 𑀞𑁦𑀪च 𑀠𑁣पच च 𑀟च𑀘𑁦𑀪𑀢णच',\n 'चढ𑀢𑀟 𑀣च णच 𑀳च 𑀱चण𑁣ण𑀢𑀟𑀞न 𑀞𑁦 𑀤च𑀲𑀢 𑀣च णच𑀣𑀣च 𑀤च 𑀞न 𑀠चबच𑀟त𑁦 𑀠चप𑀳चलच𑀪']"},"metadata":{}}],"execution_count":208},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
