{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"13KoS5TcPLNrCujtp9KhfCZGXLnL7w7Qm","timestamp":1719644856617}],"authorship_tag":"ABX9TyPEQiyVIiQwqa9EXoh8Vl3n"},"widgets":{"application/vnd.jupyter.widget-state+json":{"21671848a51a401891400b8ed277cc73":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d1f5be69dfb4366b1f33bce9cd4582f","IPY_MODEL_c70e2c3d1d5e44fa95d134722a32f2c7","IPY_MODEL_b70399895f5040b8b9034ec4b0f05cf8"],"layout":"IPY_MODEL_aadde11616ab4b5381b23ec4166bc953"}},"0d1f5be69dfb4366b1f33bce9cd4582f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a55225dd650645fb8661defeac536d99","placeholder":"​","style":"IPY_MODEL_0f93517585404413873d52c2aa14d1bc","value":"model.safetensors: 100%"}},"c70e2c3d1d5e44fa95d134722a32f2c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f32d5d3c0d4044738f77dd523967d394","max":672247920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4841d2cf0964659816b2ef351beb9b7","value":672247920}},"b70399895f5040b8b9034ec4b0f05cf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a472485af7240cca9401f78cb75ac1d","placeholder":"​","style":"IPY_MODEL_c10e1a21691e4c01affe5987ac606b68","value":" 672M/672M [00:04&lt;00:00, 175MB/s]"}},"aadde11616ab4b5381b23ec4166bc953":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a55225dd650645fb8661defeac536d99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f93517585404413873d52c2aa14d1bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f32d5d3c0d4044738f77dd523967d394":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4841d2cf0964659816b2ef351beb9b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a472485af7240cca9401f78cb75ac1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c10e1a21691e4c01affe5987ac606b68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c11cd03f8b474c2ea7a46d193d43ac69":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4162b9c045cb4139894a6ca639f6ecf3","IPY_MODEL_b959a8d953c145568f175586c3607a19","IPY_MODEL_1b24362ffdbe4f5ca8feccfaa7008453"],"layout":"IPY_MODEL_bd76fccba04b4d86a2776a3cf0671714"}},"4162b9c045cb4139894a6ca639f6ecf3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57391a3802c445e18ba3f20ff6255bb6","placeholder":"​","style":"IPY_MODEL_c38538a02cdc4b71addaa31a8f49c2d5","value":"Map: 100%"}},"b959a8d953c145568f175586c3607a19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdefbe6b851f49d88b6171bb59a6772b","max":218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c52a681036654cc396dcdde4df62d4bc","value":218}},"1b24362ffdbe4f5ca8feccfaa7008453":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f66105aca2e4df392180f17802f036b","placeholder":"​","style":"IPY_MODEL_0c5c741374eb4eb18d8d623751dc410e","value":" 218/218 [00:00&lt;00:00, 1507.79 examples/s]"}},"bd76fccba04b4d86a2776a3cf0671714":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57391a3802c445e18ba3f20ff6255bb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c38538a02cdc4b71addaa31a8f49c2d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdefbe6b851f49d88b6171bb59a6772b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c52a681036654cc396dcdde4df62d4bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f66105aca2e4df392180f17802f036b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c5c741374eb4eb18d8d623751dc410e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Help BOBAI: Classify an unknown language","metadata":{"id":"bqAFpqJlnt77"}},{"cell_type":"markdown","source":"<img src=\"https://drive.google.com/uc?id=1Hvgrrah-T7yFTzDP002XuRodhyfY1Hju\" width=\"750\">\n\n## Background\nBob's AI start-up, Bobai, builds AI solutions for other companies which have to process large volumes of text in their daily tasks. Bobai serve companies from all over the world, and they pride themselves on their ability to handle a variety of languages, from English, through Arabic to Mandarin. The secret to Bobai's success is that all of their products are based on a strong multilingual language encoder, mBERT. Bobai's infrastructure is actually highly optimized for this specific language encoder, which makes their products super fast and efficient, i.e. very attractive to clients.\n\n## Task\n\nBut mBERT is trained on just 101 languages. So what happens when one of Bobai's biggest clients, Amoira, requests support for a new language X that is not among those 101 languages? Bob and his team have to find a way to meet this request, as they cannot risk losing the client.\n\nThe data Amoira has provided consists of a small labeled dataset for text classification and a larger corpus or raw text in the language.\n\nTo make things even more complicated, Amoira has encrypted the data, as they don't want to risk competitors finding out which new market they are targetting.\n\nBob has found out that at this time his team has no bandwidth to develop this product, so he is asking for your help. He has shared the baseline solution he uses for languages that mBERT already has support for, so you can start by checking how well this solution does and modify it to obtain better results. You should not waste any efforts on trying to decrypt the data - this will not help you build a better classifier and it will get you in trouble with Bob!\n\nYour task is to build the best text classifier for language X that you can, while operating within the constraints of Bobai:\n\n*   The classifier has to be based on mBERT (and cannot use any additional pre-trained language encoder).\n*   The classifier has to train in under 8 hours using an L4 GPU as the compute resources of the company are limited.\n*   The classifier has to perform inference on any random 500 data samples in under 5 minutes (Bobai will then apply their optimization tricks to bring this time even further down).\n\n## Deliverables\n\nYou need to submit:\n\n\n*   Your model predictions on the test inputs that we will provide 48 hours before the deadline.\n  * saved as a text file in the format shown at the bottom of the notebook\n*   Your best trained model.\n  * as a link to the Huggingface Hub (read up on `push_to_hub` [here](push_to_hub)).\n*   Working code that can be used to reproduce your best trained model.\n  * In this Colab notebook.\n","metadata":{"id":"vu6E-5QWIjlv"}},{"cell_type":"markdown","source":"## Prerequisites\n","metadata":{"id":"J6BNTewtA-Ku"}},{"cell_type":"markdown","source":"### HuggingFace configuration\n\nThe steps below need to be completed by the team leader:\n\n1. Create a team account on [HuggingFace](https://huggingface.co/) using the Gmail account provided by the IOAI organizers.\n\n2. Go to the [IOAI HuggingFace repo](https://huggingface.co/InternationalOlympiadAI) and request access to all datasets.\n\n3. In settings, create two Access Tokens, one with read rights, one with write rights, and store those in [Colab Secrets](https://www.youtube.com/watch?v=q87i2LZbbPc) as `hf_read` and `hf_write`, respectively.","metadata":{"id":"0Tg2sPb2ELb9"}},{"cell_type":"code","source":"import importlib\nimport torch, transformers\n\nif '2.3.0' not in torch.__version__:\n  !pip install torch==2.3.0\nif transformers.__version__!='4.41.2':\n  !pip install transformers==4.41.2\n\nif importlib.util.find_spec('datasets') is None:\n  !pip install datasets==2.18.0s\n  !pip install evaluate==0.4.2\n  !pip install accelerate -U\n","metadata":{"id":"8VH0WJYuM_4Z","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you've just installed `accelerate`, execute `Runtime > Restart session and run all` in the Colab UI menu above.","metadata":{"id":"zridt_PWpd9d"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Data","metadata":{"id":"EFTIQ9tDMqsE"}},{"cell_type":"code","source":"# load the data\n\nfrom datasets import load_dataset, Dataset, DatasetDict\n\nclassification_dataset = load_dataset('InternationalOlympiadAI/NLP_problem',token='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')\nraw_text = load_dataset('InternationalOlympiadAI/NLP_problem_raw',token='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')","metadata":{"id":"CpgcmI2NMLyF","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:44:03.706090Z","iopub.execute_input":"2025-05-02T23:44:03.706379Z","iopub.status.idle":"2025-05-02T23:44:06.353799Z","shell.execute_reply.started":"2025-05-02T23:44:03.706355Z","shell.execute_reply":"2025-05-02T23:44:06.353255Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"****","metadata":{}},{"cell_type":"markdown","source":"# Baseline","metadata":{"id":"cv9MBElmMs6G"}},{"cell_type":"code","source":"# load the pre-trained tokenizer and use it to process the data\n\nfrom transformers import AutoTokenizer\nfrom transformers import DataCollatorWithPadding\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")\n\n#Train new tokenizer\nfrom tokenizers import BertWordPieceTokenizer\nraw_text = raw_text['train']\ndef batch_iterator(batch_size=1000):\n    for i in range(0, len(raw_text), batch_size):\n        yield raw_text[i : i + batch_size][\"text\"]\n        \n#tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")\n#new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=50_000)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/mlm_bert_model/checkpoint-1000\")\nnew_tokenizer = tokenizer\n\ndef preprocess_function(examples):\n    return new_tokenizer(examples[\"text\"], truncation=True)\n\ntokenized_data = classification_dataset.map(preprocess_function, batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=new_tokenizer)","metadata":{"id":"tSOWNJRKNoWM","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c11cd03f8b474c2ea7a46d193d43ac69","4162b9c045cb4139894a6ca639f6ecf3","b959a8d953c145568f175586c3607a19","1b24362ffdbe4f5ca8feccfaa7008453","bd76fccba04b4d86a2776a3cf0671714","57391a3802c445e18ba3f20ff6255bb6","c38538a02cdc4b71addaa31a8f49c2d5","cdefbe6b851f49d88b6171bb59a6772b","c52a681036654cc396dcdde4df62d4bc","7f66105aca2e4df392180f17802f036b","0c5c741374eb4eb18d8d623751dc410e"]},"executionInfo":{"status":"ok","timestamp":1722521267341,"user_tz":-120,"elapsed":1145,"user":{"displayName":"Yova Kementchedjhieva","userId":"12503867628805318878"}},"outputId":"33155489-302d-4be9-d9a8-f5bd1be2a2fd","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:44:06.354813Z","iopub.execute_input":"2025-05-02T23:44:06.355097Z","iopub.status.idle":"2025-05-02T23:44:06.941510Z","shell.execute_reply.started":"2025-05-02T23:44:06.355052Z","shell.execute_reply":"2025-05-02T23:44:06.940743Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1524 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"052e37e7ea5d45b19886751752d5bd18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/218 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d81909d33aca4f47a3fefec84fb70f71"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from gensim.models import Word2Vec, FastText\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom gensim.models.callbacks import CallbackAny2Vec\n\nclass TQDMProgressBar(CallbackAny2Vec):\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.pbar = None\n\n    def on_train_begin(self, model):\n        self.pbar = tqdm(total=self.epochs, desc=\"Word2Vec Training\", unit=\"epoch\")\n\n    def on_epoch_end(self, model):\n        self.pbar.update(1)\n\n    def on_train_end(self, model):\n        self.pbar.close()\n\ndef train_word2vec_with_transformers_tokenizer(texts, tokenizer, vector_size=768, window=5, min_count=1, epochs=5):\n    tokenized_texts = [\n        [tokenizer.convert_ids_to_tokens(tok_id) for tok_id in tokenizer.encode(text, add_special_tokens=False)]\n        for text in tqdm(texts)\n    ]\n    \n    w2v = Word2Vec(\n        sentences=tokenized_texts,\n        vector_size=vector_size,\n        window=window,\n        min_count=min_count,\n        sg=1,  # skip-gram\n        workers=4,\n        seed=56,\n        #alpha=0.03,\n        epochs=epochs,\n        callbacks=[TQDMProgressBar(epochs)]\n    )\n    \n    token_embeddings = {token: w2v.wv[token] for token in w2v.wv.index_to_key}\n    \n    return token_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:44:10.254411Z","iopub.execute_input":"2025-05-02T23:44:10.254729Z","iopub.status.idle":"2025-05-02T23:45:04.760911Z","shell.execute_reply.started":"2025-05-02T23:44:10.254705Z","shell.execute_reply":"2025-05-02T23:45:04.760317Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"token_w2v_embeds = train_word2vec_with_transformers_tokenizer(raw_text['text'], new_tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom transformers import BertModel, BertTokenizer\n\ndef replace_bert_embeddings_with_word2vec(model, tokenizer, w2v_embeddings):\n    # Получаем embedding слой\n    embeddings = model.get_input_embeddings()\n    vocab_size, emb_dim = embeddings.weight.shape\n\n    # Создаем новую матрицу эмбеддингов\n    new_weight = embeddings.weight.data.clone()\n    \n    replaced = 0\n    for idx in range(vocab_size):\n        token = tokenizer.convert_ids_to_tokens(idx)\n        if token in w2v_embeddings:\n            vec = w2v_embeddings[token]\n            # Если размерность совпадает\n            if vec.shape[0] == emb_dim:\n                new_weight[idx] = torch.tensor(vec, dtype=new_weight.dtype)\n                replaced += 1\n            else:\n                print('hui')\n                pass\n    print(f\"Заменено эмбеддингов: {replaced}/{vocab_size}\")\n\n    # Заменяем веса\n    embeddings.weight.data = new_weight\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define the evaluation metric\n#!pip install evaluate -q\nimport evaluate\nimport numpy as np\n\nf1 = evaluate.load(\"f1\")\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return f1.compute(predictions=predictions, references=labels, average='macro')","metadata":{"id":"nN64VrhSNuYA","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:45:06.727215Z","iopub.execute_input":"2025-05-02T23:45:06.727896Z","iopub.status.idle":"2025-05-02T23:45:07.343271Z","shell.execute_reply.started":"2025-05-02T23:45:06.727873Z","shell.execute_reply":"2025-05-02T23:45:07.342589Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"/kaggle/working/mlm_bert_model/checkpoint-1000\", num_labels=5\n)\n\n#model.resize_token_embeddings(len(new_tokenizer))\n#model = replace_bert_embeddings_with_word2vec(model,new_tokenizer,token_w2v_embeds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:45:10.286627Z","iopub.execute_input":"2025-05-02T23:45:10.287275Z","iopub.status.idle":"2025-05-02T23:45:10.394988Z","shell.execute_reply.started":"2025-05-02T23:45:10.287250Z","shell.execute_reply":"2025-05-02T23:45:10.394412Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/mlm_bert_model/checkpoint-1000 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"basiline_bobai\",\n    learning_rate=5e-6,\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=64,\n    num_train_epochs=20,#20,\n    #lr_scheduler_type='linear',\n    #warmup_ratio=0.05,\n    report_to='none',\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=5,\n    metric_for_best_model='f1',\n    load_best_model_at_end=True,\n    #push_to_hub=True,\n    #hub_strategy=\"checkpoint\",\n    #hub_token=write_access_token,\n    #hub_private_repo=True,\n    #hub_model_id='baseline_bobai'\n\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_data[\"train\"],\n    eval_dataset=tokenized_data[\"dev\"],\n    tokenizer=new_tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"id":"hCojWe8hOgRv","colab":{"base_uri":"https://localhost:8080/","height":103,"referenced_widgets":["21671848a51a401891400b8ed277cc73","0d1f5be69dfb4366b1f33bce9cd4582f","c70e2c3d1d5e44fa95d134722a32f2c7","b70399895f5040b8b9034ec4b0f05cf8","aadde11616ab4b5381b23ec4166bc953","a55225dd650645fb8661defeac536d99","0f93517585404413873d52c2aa14d1bc","f32d5d3c0d4044738f77dd523967d394","b4841d2cf0964659816b2ef351beb9b7","7a472485af7240cca9401f78cb75ac1d","c10e1a21691e4c01affe5987ac606b68"]},"executionInfo":{"status":"ok","timestamp":1722519670357,"user_tz":-120,"elapsed":7492,"user":{"displayName":"Yova Kementchedjhieva","userId":"12503867628805318878"}},"outputId":"69375388-0bb5-4b7e-bdea-6b2023703ae7","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:49:22.659950Z","iopub.execute_input":"2025-05-02T23:49:22.660698Z","iopub.status.idle":"2025-05-02T23:49:22.697869Z","shell.execute_reply.started":"2025-05-02T23:49:22.660670Z","shell.execute_reply":"2025-05-02T23:49:22.697068Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_1224/3326983729.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# execute the model training\ntrainer.train()","metadata":{"id":"VTJ-w6BnosYy","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:49:24.078044Z","iopub.execute_input":"2025-05-02T23:49:24.078334Z","iopub.status.idle":"2025-05-02T23:51:44.818729Z","shell.execute_reply.started":"2025-05-02T23:49:24.078313Z","shell.execute_reply":"2025-05-02T23:51:44.817994Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [480/480 02:19, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.772951</td>\n      <td>0.889128</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.778845</td>\n      <td>0.888468</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.838217</td>\n      <td>0.887962</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.824149</td>\n      <td>0.882995</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.821522</td>\n      <td>0.882995</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>0.818996</td>\n      <td>0.888081</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>0.823724</td>\n      <td>0.888081</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>0.831078</td>\n      <td>0.888081</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>0.840837</td>\n      <td>0.888081</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>0.847804</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>No log</td>\n      <td>0.849786</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>No log</td>\n      <td>0.853234</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>No log</td>\n      <td>0.856690</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>No log</td>\n      <td>0.864152</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>No log</td>\n      <td>0.864274</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>No log</td>\n      <td>0.864285</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>No log</td>\n      <td>0.863534</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>No log</td>\n      <td>0.864671</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>No log</td>\n      <td>0.863225</td>\n      <td>0.893829</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>No log</td>\n      <td>0.866156</td>\n      <td>0.893829</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=480, training_loss=0.00026071093355615934, metrics={'train_runtime': 140.0402, 'train_samples_per_second': 217.652, 'train_steps_per_second': 3.428, 'total_flos': 379885040624352.0, 'train_loss': 0.00026071093355615934, 'epoch': 20.0})"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# MLM Train","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import (AutoTokenizer, AutoModelForMaskedLM,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)\nfrom datasets import load_dataset\n\nmodel_name = \"google-bert/bert-base-multilingual-uncased\" \ntokenizer = new_tokenizer#AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n\nmodel.resize_token_embeddings(len(new_tokenizer))\nmodel = replace_bert_embeddings_with_word2vec(model,new_tokenizer,token_w2v_embeds)\n\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n        return_special_tokens_mask=True,\n    )\n\ntokenized_dataset = raw_text.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True,\n    mlm_probability=0.15  # вероятность маскирования токена\n)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./mlm_bert_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=64,\n    save_steps=500,\n    save_total_limit=2,\n    logging_steps=100,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    prediction_loss_only=True,\n    report_to='none',\n    \n)\n\n# 6. Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)\n\n# 7. Обучение\ntrainer.train()\n\n# 8. Сохранение модели\ntrainer.save_model(\"./mlm_bert_model\")\ntokenizer.save_pretrained(\"./mlm_bert_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"oSYydzx9NAGU"}},{"cell_type":"code","source":"# run the trained model on a dev/test split\ndata_split = \"dev\"\neval_out = trainer.predict(tokenized_data[data_split])\npredictions = eval_out.predictions.argmax(1)\nlabels = eval_out.label_ids\ndev_f1 = f1.compute(predictions=predictions, references=labels, average='macro')","metadata":{"id":"jaa80VhiNBG_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dev_f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing","metadata":{"id":"dwT-GexR956j"}},{"cell_type":"code","source":"# UPDATE THIS CELL ACCORDINGLY\n\n# define a funciton to load your tokenizer and model from a HF path\n# the path variables can be strings or lists of strings (for ensemble solutions)\ndef load_model(path_to_tokenizer, path_to_model, token):\n  # Example:\n  tokenizer = AutoTokenizer.from_pretrained(path_to_tokenizer, token=token)\n  model = AutoModelForSequenceClassification.from_pretrained(path_to_model, token=token)\n  model.eval()\n\n  return tokenizer, model\n\n# define a \"predict\" function that takes the model and a list of input strings\n# and returns the outputs as a list of integer classes\ndef predict(tokenizer, model, input_texts):\n  #Example:\n  predictions = []\n  for input_text in input_texts:\n\n    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n\n    with torch.no_grad():\n      logits = model(**input_ids).logits\n\n    predictions.append(logits.argmax().item())\n\n  return predictions\n\n# set variables\npath_to_model = \"path/to/your/best/model/on/hf\" # can be a list instead\npath_to_tokenizer = \"path/to/your/best/tokenizer/on/hf\" # can be a list instead\nmodel_access_token = \"access token\" # a fine-grained token with read rights for your model repository\n","metadata":{"id":"oZkqwv229-PM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DO NOT CHANGE THIS CELL!!!\n\ntokenizer, model = load_model(path_to_tokenizer, path_to_model, token=model_access_token)\n\ntest_data = load_dataset(\"InternationalOlympiadAI/NLP_problem_test\")['test']['text']\n\npredictions = predict(tokenizer, model, test_data)\n\nwith open('test_predictions.txt', 'w') as outfile:\n  outfile.write('\\n'.join([str(p) for p in predictions]))","metadata":{"id":"68SDwUjRLBYC","trusted":true},"outputs":[],"execution_count":null}]}